{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31bd70f1",
   "metadata": {},
   "source": [
    "# Métodos de Suavización Exponencial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05239a64",
   "metadata": {},
   "source": [
    "- El presente capítulo trata del `suavizado de datos en la señal de series temporales`. El capítulo está organizado como sigue:\n",
    "\n",
    "    - `Introducción al suavizado de series temporales`\n",
    "\n",
    "    - `Suavizado exponencial de primer orden`\n",
    "\n",
    "    - `Suavizado exponencial de segundo orden`\n",
    "\n",
    "    - `Suavizado exponencial de orden superior`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d2444",
   "metadata": {},
   "source": [
    "# Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f51dfc",
   "metadata": {},
   "source": [
    "## Introducción al suavizado de series temporales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "060995b1",
   "metadata": {},
   "source": [
    "- Los datos de las series temporales se componen de `señal y ruido`, donde la `señal captura la dinámica intrínseca del proceso`; sin embargo, el `ruido representa el componente no modelado de la señal o error aleatorio no correlacionado, el cual en diferentes tiempos es estadísticamente independiente`. \n",
    "\n",
    "- La dinámica intrínseca de una señal de serie temporal puede ser tan simple como `la media del proceso` o puede ser una `forma funcional compleja` dentro de las observaciones, como se representa aquí:\n",
    "\n",
    "    $$\n",
    "    \\\\[1mm]\n",
    "    y_{t}=f(t, \\boldsymbol{\\beta})+\\varepsilon_{t},\\quad\\text{para}\\quad i=1,2,3,\\dots,T,\n",
    "$$\n",
    "    \n",
    "    donde $y_{t}$ representa las observaciones, $\\boldsymbol{\\beta}$ es un vector de parámetros desconocidos, y $\\varepsilon_{t}$ es el `error aleatorio no correlacionado`, usualmente con media 0 y desviación 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe79a5d",
   "metadata": {},
   "source": [
    "``````{figure} ./figures/signal_noise.png\n",
    ":align: center\n",
    ":name: signal_noise_fig\n",
    ":scale: 80\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657622a",
   "metadata": {},
   "source": [
    "- $f(t, \\boldsymbol{\\beta})$ `denota la forma funcional de la señal`. Un ejemplo puede ser una constante (`media`) como forma funcional:\n",
    "\n",
    "$$\n",
    "    y_{t}=\\mu+\\varepsilon_{t}\n",
    "$$\n",
    "\n",
    "- Dado que $\\varepsilon_{t}$ es `ruido blanco`, este enfoque basado en el suavizado ayuda a `separar la forma funcional intrínseca del ruido aleatorio, cancelándolo`. \n",
    "\n",
    "- Los métodos predictivos de suavización pueden considerarse como `filtros` que toman datos de entrada y `separan los componentes de tendencia y de ruido`, como se muestra en la {numref}`fig1_chapter2`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd5eaf40",
   "metadata": {},
   "source": [
    "```{figure} ./figures/fig1_chapter2.png\n",
    ":name: fig1_chapter2\n",
    ":align: center\n",
    "\n",
    "Separación de componentes de tendencia y ruido.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899e6f8",
   "metadata": {},
   "source": [
    "- Un `proceso constante` puede ser suavizado reemplazando la observación actual con el mejor estimador para $\\mu$. Usando el criterio de `mínimos cuadrados`, definimos la `suma de cuadrados de los errores` $SS_{E}$, para el proceso constante como \n",
    "\n",
    "$$\n",
    "SS_{E}=\\sum_{t=1}^{T}(y_{t}-\\mu)^{2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5edb9f",
   "metadata": {},
   "source": [
    "- La estimación por mínimos cuadrados de $\\mu$ puede ser calculada derivando $SS_{E}$ con respecto a $\\mu$ el igualar a cero (`verifíquelo`). Esto entrega\n",
    "\n",
    "$$\n",
    "    \\hat{\\mu}=\\frac{1}{T}\\sum_{t=1}^{T}y_{t},\n",
    "$$(constant_process_ols)\n",
    "\n",
    "    donde $\\hat{\\mu}$ es la estimación por mínimos cuadrados de $\\mu$.\n",
    "\n",
    "- La Ecuación {eq}`constant_process_ols` muestra que la estimación por mínimos cuadrado de $\\mu$, es en efecto, el promedio de las observaciones hasta el tiempo $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c1533",
   "metadata": {},
   "source": [
    "``````{admonition} Observación\n",
    "- El principal problema con el suavizador de la Ecuación {eq}`constant_process_ols` es que `reacciona lentamente a procesos de cambios`, debido a que `acumula mas y mas puntos de datos ganando algún tipo de inercia`. La solución mas obvia es de alguna forma descontar datos mas antiguos. \n",
    "- Una solución común es usar la `media móvil simple` definida por\n",
    "\n",
    "$$\n",
    "M_{T}= \\frac{y_{T}+y_{T-1}+\\cdots+y_{T-N+1}}{N}=\\frac{1}{N}\\sum_{t=T-N+1}^{N}y_{t}.\n",
    "$$(simple_moving_avg)\n",
    "\n",
    "- Una media reaccionará mas rápido a cambios si $N$ es pequeño. Dado que para observaciones no correlacionadas con varianza constante $\\sigma^{2}$ se tiene que (`verifíquelo`)\n",
    "\n",
    "    $$\n",
    "    \\text{Var}(M_{T})=\\frac{\\sigma^{2}}{N}.\n",
    "    $$(variance_sma)\n",
    "\n",
    "- Dada la Ecuación {eq}`variance_sma`. Si se espera que el proceso es constante, un valor grande de $N$ puede ser usado, mientras que, un valor pequeño de $N$ es preferido si el proceso es cambiante.    \n",
    "- Queda como ejercicio para el lector verificar que la `función de autocorrelación (ACF)` de la media móvil con $k$-lags está dada por\n",
    "\n",
    "$$\n",
    "\\rho_{k}=\n",
    "\\begin{cases}\n",
    "\\displaystyle{1-\\frac{|k|}{N}}, & k<N\\\\\n",
    "0, & k\\geq N\n",
    "\\end{cases}.\n",
    "$$(acf_moving_average)\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69182a8c",
   "metadata": {},
   "source": [
    "## Suavización exponencial de primer orden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a5886",
   "metadata": {},
   "source": [
    "````{prf:theorem} Suavización exponencial simple\n",
    ":label: th-simple_exp_smoothing\n",
    "\n",
    "Dada una serie de tiempo $(y_{t})_{t=1}^{T}$. Una `suavización exponencial ponderada` está definida por:\n",
    "\n",
    "$$\n",
    "\\sum_{t=1}^{T-1}\\theta^{t}y_{T-t}=y_{T}+\\theta y_{T-1}+\\theta^{2}y_{t-2}+\\cdots+\\theta^{T-1}y_{1}\n",
    "$$ (eq:simple_exp_smoothing)\n",
    "\n",
    "donde $\\theta$ satisface $|\\theta|<1$. Esto es, observaciones pasadas son descontadas de manera geométricamente descendiente. Entonces, la `suavización exponencial simple` está representada por:\n",
    "\n",
    "$$\n",
    "\\tilde{y}_{T}=\\lambda y_{T}+(1-\\lambda)\\tilde{y}_{T-1},\\quad\\text{donde}\\quad\\lambda=1-\\theta.\n",
    "$$\n",
    "\n",
    "$\\lambda$ es conocido como el `factor de descuento`. Nótese que el factor de descuento $\\lambda$, representa el peso colocado sobre la última observación $y_{T}$ y $1-\\lambda$ el peso asignado al valor suavizado de la previa observación $\\tilde{y}_{T-1}$.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b63b5",
   "metadata": {},
   "source": [
    "**`Demostración`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853494f3",
   "metadata": {},
   "source": [
    "- Por hipótesis, pasadas observaciones son descontadas en forma geométricamente descendente, con $|\\theta|<1$. Nótese que el suavizador {eq}`eq:simple_exp_smoothing` no es una media movil simple. En efecto:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac5431",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{t=0}^{T-1}\\theta^{t}&=1+\\theta+\\theta^2+\\cdots+\\theta^{T-1},\\quad\\text{multiplicando por}~(1-\\theta)\\\\\n",
    "(1-\\theta)\\sum_{t=0}^{T-1}\\theta^{t}&=(1-\\theta)(1+\\theta+\\theta^2+\\cdots+\\theta^{T-1})\\\\\n",
    "&=(1+\\theta+\\theta^2+\\cdots+\\theta^{T-1})-(\\theta+\\theta^2+\\theta^{3}+\\cdots+\\theta^{T-1}+\\theta^{T})\\\\\n",
    "&=1-\\theta^{T}\\Longrightarrow\\\\\n",
    "\\sum_{t=0}^{T-1}\\theta^{t}&=\\frac{1-\\theta^{T}}{1-\\theta}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e553f5",
   "metadata": {},
   "source": [
    "- Nótese que la sumatoria:\n",
    "    \n",
    "    $$\n",
    "    \\sum_{t=0}^{T-1}\\theta^{t}=\\frac{1-\\theta^{T}}{1-\\theta},\n",
    "    $$\n",
    "\n",
    "    no necesariamente es igual a 1. Esto puede ser ajustado multiplicando por $(1-\\theta)/(1-\\theta^{T})$ la ecuación {eq}`eq:simple_exp_smoothing`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b90a6",
   "metadata": {},
   "source": [
    "- Además, dado que $(1-\\theta)/(1-\\theta^{T})\\rightarrow 1-\\theta$ cuando $T\\rightarrow\\infty,~ |\\theta|<1$. Entonces {eq}`eq:simple_exp_smoothing` puede reescribirse como:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed615d8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{y}_{T}&=\\frac{1-\\theta}{1-\\theta^{T}}\\sum_{t=0}^{T-1}\\theta^{t}y_{T-t}\\overset{T\\rightarrow\\infty}{=}(1-\\theta)\\sum_{t=0}^{T-1}\\theta^{t}y_{T-t}\\\\\n",
    "&=(1-\\theta)(y_{T}+\\theta y_{T-1}+\\theta^{2}y_{T-2}+\\cdots+\\theta^{T-1}y_{1})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841db35c",
   "metadata": {},
   "source": [
    "- Entonces la `suavización exponencial simple` está dada por\n",
    "\n",
    "$$\n",
    "\\tilde{y}_{T}=(1-\\theta)(y_{T}+\\theta y_{T-1}+\\theta^{2}y_{T-2}+\\cdots+\\theta^{T-1}y_{1})\n",
    "$$ (eq:simple_exp_smoothing2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1518c",
   "metadata": {},
   "source": [
    "- Una expresión alternativa para la suavización exponencial simple es:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c07365",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{y}_{T}&=(1-\\theta)(y_{T}+\\theta y_{T-1}+\\theta^{2}y_{T-2}+\\cdots+\\theta^{T-1}y_{1}\\\\\n",
    "&=(1-\\theta)y_{T}+(1-\\theta)(\\theta y_{T-1}+\\theta^{2}y_{T-2}+\\cdots+\\theta^{T-1}y_{1})\\\\\n",
    "&=(1-\\theta)y_{T}+\\theta(1-\\theta)(y_{T-1}+\\theta y_{T-2}+\\cdots+\\theta^{T-2}y_{1})\\\\\n",
    "&=(1-\\theta)y_{T}+\\theta\\tilde{y}_{T-1}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015f9fc9",
   "metadata": {},
   "source": [
    "- Esto es, la suavización exponencial de primer orden, puede verse como combianción lineal de la observación actual y la observación suavizada en un tiempo previo. Definiendo $\\lambda=1-\\theta$, (`factor de corrección`) se tiene que:\n",
    "\n",
    "$$\n",
    "\\tilde{y}_{T}=\\lambda y_{T}+(1-\\lambda)\\tilde{y}_{T-1}.\n",
    "$$ (eq:simple_exp_smoothing3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c56a3a",
   "metadata": {},
   "source": [
    "```{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Análogamente a la longitud del periodo en la media movil, `un asunto importante para la suavización exponencial es la selección del factor de descuento` $\\lambda$. Además, a partir de la ecuación {eq}`eq:simple_exp_smoothing3` podemos observa que el cálculo de $\\tilde{y}_{1}$ va a requerir conocer $\\tilde{y}_{0}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865f5ac",
   "metadata": {},
   "source": [
    "````{prf:theorem} Selección de $\\tilde{y}_{0}$\n",
    ":label: th-initial-value-simple-exp-smoothing\n",
    "\n",
    "Dada una serie de tiempo $(y_{t})_{t=1}^{T}$, donde $T$ es grande, la contribución de $\\tilde{y}_{0}$ a $\\tilde{y}_{T}$ es mínima. Por lo tanto, `la estimación de` $\\tilde{y}_{0}$ `tiene poca influencia sobre la suavización`. Sin embargo, dos estimaciones de $\\tilde{y}_{0}$ comunmente usadas en las aplicaciones son las siguientes:\n",
    "\n",
    "1. $\\tilde{y}_{0}=y_{1}$: Si los cambios esperados del proceso ocurren temprano y rápido, esta selección para el valor inicial de $\\tilde{y}_{T}$ es razonable.\n",
    "2. $\\tilde{y}_{0}=\\overline{y}$: Tomar el promedio de los datos disponibles. Si el proceso es constante, al menos al inicio, este sería el valor mas adecuado.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9980d2",
   "metadata": {},
   "source": [
    "**`Demostración`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047bfa3",
   "metadata": {},
   "source": [
    "- Dado que $\\tilde{y}_{0}$ es necesario en el calculo recursivo que inicia con: $\\tilde{y}_{1}=\\lambda y_{1}+(1-\\lambda)\\tilde{y}_{0}$, estimamos su valor a partir de la Ecuación {eq}`eq:simple_exp_smoothing3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a12f0b",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{y}_{1}&=\\lambda y_{1}+(1-\\lambda)\\tilde{y}_{0}\\\\\n",
    "\\tilde{y}_{2}&=\\lambda y_{2}+(1-\\lambda)\\tilde{y}_{1}=\\lambda y_{2}+(1-\\lambda)(\\lambda y_{1}+(1-\\lambda)\\tilde{y}_{0})=\\lambda(y_{2}+(1-\\lambda)y_{1})+(1-\\lambda)^{2}\\tilde{y}_{0}\\\\\n",
    "\\tilde{y}_{3}&=\\lambda y_{3}+(1-\\lambda)\\tilde{y}_{2}=\\lambda y_{3}+(1-\\lambda)(\\lambda(y_{2}+(1-\\lambda)\\tilde{y}_{1})+(1-\\lambda)^{2}\\tilde{y}_{0})\\\\\n",
    "&=\\lambda(y_{3}+(1-\\lambda)y_{2}+(1-\\lambda)^{2}y_{1})+(1-\\lambda)^{3}\\tilde{y}_{0}\\\\\n",
    "&\\vdots\\\\\n",
    "\\tilde{y}_{T}&=\\lambda(y_{T}+(1-\\lambda)y_{T-1}+(1-\\lambda)^{2}y_{T-2}+\\cdots+(1-\\lambda)^{T-1}y_{1})+(1-\\lambda)^{T}\\tilde{y}_{0}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8ceed",
   "metadata": {},
   "source": [
    "- Por lo tanto, cuando $T\\rightarrow\\infty,~ (1-\\lambda)^{T}\\rightarrow 0$, esto es, la contribución de $\\tilde{y}_{0}$ a $\\tilde{y}_{T}$ es mínima. Esto es, la estimación de $\\tilde{y}_{0}$ no es relevante para un conjunto grande de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a44dd5",
   "metadata": {},
   "source": [
    "## El valor de $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33351f95",
   "metadata": {},
   "source": [
    "``````{admonition} Observación\n",
    "- En general, cuando $\\lambda$ tiende a 1, y se hace mas hincapié en la ultima observación, los `valores suavizados se aproximarán a las observaciones originales`. \n",
    "- Si $\\lambda=0$, los valores suavizados serán todos igual a una constante, a saber $\\tilde{y}_{0}$. Esta línea constantes sería la `versión mas suavizada` de cualquier patron que siga la serie de tiempo original.\n",
    "- Para $\\lambda=1$, tenemos $\\tilde{y}_{T}=y_{T}$ y este representará la `versión menos suavizada` de la serie de tiempo original.\n",
    "- Con base en la selección de $\\lambda$, esperamos acordemente que la varianza varíe entre 0 y la varianza de la serie de tiempo original. \n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5cd284",
   "metadata": {},
   "source": [
    "- Nótese que bajo el supuesto de independencia y varianza constante tenemos que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}(\\tilde{y}_{T}) &= \\text{Var}\\left(\\lambda\\sum_{t=0}^{\\infty}(1-\\lambda)^{t}y_{T-t}\\right)\\\\\n",
    "&= \\lambda^{2}\\sum_{t=0}^{\\infty}(1-\\lambda)^{2t}\\text{Var}(y_{T-t})\\\\\n",
    "&= \\lambda^{2}\\sum_{t=0}^{\\infty}(1-\\lambda)^{2t}\\text{Var}(y_{T})\\\\\n",
    "&= \\text{Var}(y_{T})\\lambda^{2}\\sum_{t=0}^{\\infty}(1-\\lambda)^{2t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- Observe que\n",
    "\n",
    "$$\n",
    "\\sum_{t=0}^{T-1}(1-\\lambda)^{2t}=1+(1-\\lambda)^{2}+(1-\\lambda)^{4}+\\cdots+(1-\\lambda)^{2(T-1)}\n",
    "$$\n",
    "\n",
    "- Multiplicando por $1-(1-\\lambda)^{2}$ se tiene que:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(1-(1-\\lambda)^{2})\\sum_{t=0}^{T-1}(1-\\lambda)^{2t} &= 1+(1-\\lambda)^{2}+(1-\\lambda)^{4}+\\cdots+(1-\\lambda)^{2(T-1)}\\\\\n",
    "&- ((1-\\lambda)^{2}+(1-\\lambda)^{4}+\\cdots+(1-\\lambda)^{2T}\\\\[3mm]\n",
    "&= 1-(1-\\lambda)^{2T}\\xrightarrow[T\\rightarrow\\infty]{}1,\\quad |1-\\lambda|<1\\Rightarrow\\\\\n",
    "\\sum_{t=0}^{T-1}(1-\\lambda)^{2t} &= \\frac{1}{\\lambda(2-\\lambda)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Por lo tanto,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}(\\tilde{y}_{T}) = \\text{Var}(y_{T})\\lambda^{2}\\sum_{t=0}^{\\infty}(1-\\lambda)^{2t} = \\text{Var}(y_{T})\\lambda^{2}\\frac{1}{\\lambda(2-\\lambda)} = \\frac{\\lambda}{2-\\lambda}\\text{Var}(y_{T})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1156fa0",
   "metadata": {},
   "source": [
    "- Nótese que si $\\lambda\\rightarrow1$, $\\text{Var}(\\tilde{y}_{T})\\rightarrow\\text{Var}(y_{T})$, y si $\\lambda\\rightarrow0$, $\\text{Var}(\\tilde{y}_{T}\\rightarrow0)$. La pregunta ahora es, ¿cuánto suavizado se necesita?. En la literatura se recomienda $\\lambda$ entre 0.1 y 0.4 y, de hecho, funcionan bien en la práctica. Un método más riguroso para encontrar el valor $\\lambda$ se discutirá mas adelante en este capítulo.\n",
    "\n",
    "- Como medidas de precisión usaremos las siguientes métricas que vienen ya implementadas en `Python`; `MAPE, MAD` y `MSD`. \n",
    "\n",
    "- El `error porcentual medio absoluto (MAPE)` es el cambio porcentual medio absoluto entre el valor predicho, esto es $\\tilde{y}_{t-1}:=\\tilde{y}_{t}(t-1)$, para una predicción `one-step-ahead` y el valor real dado por\n",
    "\n",
    "$$\n",
    "\\text{MAPE}=\\frac{\\displaystyle{\\sum_{t=1}^{T}|(y_{t}-\\tilde{y}_{t-1})/y_{t}|}}{T}\\times 100,\\quad (y_{t}\\neq 0).\n",
    "$$\n",
    "\n",
    "- La `desviación media absoluta (MAD)` es la diferencia absoluta media entre los valores predichos y los verdaderos, dada por\n",
    "\n",
    "$$\n",
    "\\text{MAD}=\\frac{\\displaystyle{\\sum_{t=1}^{T}|(y_{t}-\\tilde{y}_{t-1})|}}{T}.\n",
    "$$\n",
    "\n",
    "- La `desviación cuadrática media (MSD)` es la diferencia cuadrática media entre los valores predichos y los verdaderos, dada por\n",
    "\n",
    "$$\n",
    "\\text{MSD}=\\frac{\\displaystyle{\\sum_{t=1}^{T}(y_{t}-\\tilde{y}_{t-1})^{2}}}{T}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704fd10",
   "metadata": {},
   "source": [
    "## Modelado de datos de series temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0e461b",
   "metadata": {},
   "source": [
    "````{prf:theorem}\n",
    "\n",
    "- Considere un `proceso constante`, esto es, un proceso donde los datos de series de tiempo se espera que varíen en torno a un nivel constante, con fluctuaciones aleatorias, las cuales son caracterizadas usualmente por `errores no correlacionados con media 0 y varianza constante`. \n",
    "\n",
    "- La clase general de modelos puede expresarse como\n",
    "\n",
    "    $$\n",
    "    y_{t}=f(t, \\boldsymbol{\\beta})+\\varepsilon_{t},\n",
    "    $$\n",
    "\n",
    "    donde $\\boldsymbol{\\beta}$ es un vector de parámetros desconocidos y $\\varepsilon_{t}$ representa errores no correlacionados. \n",
    "\n",
    "- El `proceso constante` es miembro de esta clase general:\n",
    "\n",
    "    $$\n",
    "    y_{t}=\\beta_{0}+\\varepsilon_{t},\n",
    "    $$\n",
    "\n",
    "    donde $\\beta_{0}$ es igual a $\\mu$.\n",
    "\n",
    "- Entonces, la `estimación de mínimos cuadrados ponderada` de $\\beta_{0}$ en el proceso constante, con `pesos exponencialmente decrecientes en el tiempo` es\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{0}=(1-\\theta)\\sum_{t=0}^{T-1}\\theta^{t}y_{T-t}.\n",
    "$$\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ccb17",
   "metadata": {},
   "source": [
    "**`Demostración`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588bea16",
   "metadata": {},
   "source": [
    "- Consideremos la `suma de errores cuadráticos` para el proceso constante, dada por\n",
    "\n",
    "$$\n",
    "SS_{E}=\\sum_{t=1}^{T}(y_{t}-\\mu)^{2}\n",
    "$$\n",
    "\n",
    "- Si afirmamos que no todas las observaciones deberían tener igual influencia sobre la suma, introducimos una cadena de `pesos geométricamente decrecientes en tiempo`\n",
    "\n",
    "$$\n",
    "SS_{E}^{\\star}=\\sum_{t=0}^{T-1}\\theta^{t}(y_{T-t}-\\beta_{0})^{2},~\\text{donde}~|\\theta|<1.\n",
    "$$(error_sq_sum1)\n",
    "\n",
    "- Derivando {eq}`error_sq_sum1` con respecto a $\\beta_{0}$ e igualando a cero obtenemos el estimador por mínimos cuadrado para $\\beta_{0}$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial SS_{E}^{\\star}}{\\partial\\beta_{0}} &= \\frac{\\partial}{\\partial\\beta_{0}}\\left(\\sum_{t=0}^{T-1}\\theta^{t}(y_{T-t}-\\hat{\\beta}_{0})\\right)=-2\\sum_{t=0}^{T-1}\\theta^{t}(y_{T-t}-\\hat{\\beta}_{0})=0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Dado que $\\sum_{t=0}^{T-1}\\theta^{t}=(1-\\theta^{T})/(1-\\theta)$, entonces\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{0}=\\frac{1-\\theta}{1-\\theta^{T}}\\sum_{t=0}^{T-1}\\theta^{t}y_{T-t}\\xrightarrow[T\\rightarrow\\infty]{}(1-\\theta)\\sum_{t=0}^{T-1}\\theta^{t}y_{T-t}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68ad9a",
   "metadata": {},
   "source": [
    "- Nótese que $\\hat{\\beta}_{0}=\\tilde{y}_{T}$. Esto es, el proceso de suavización exponencial simple en efecto provee una `estimación de mínimos cuadrados ponderada` de $\\beta_{0}$, para el proceso constante con pesos que son exponencialmente decrecientes en el tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d73fc2",
   "metadata": {},
   "source": [
    "``````{admonition} Observación\n",
    "- Retomando con nuestra clase de modelo general $y_{t}=f(t; \\boldsymbol{\\beta})+\\varepsilon_{t},~f(t, \\boldsymbol{\\beta})$ puede ser cualquier función de $t$.\n",
    "- Por ejemplo, para `series de tiempo no estacionarias`, es mas conveniente considerar una `familia polinómica`, donde el proceso constante es el modelos mas simple que podemos considerar.\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfd7ed",
   "metadata": {},
   "source": [
    "``````{prf:theorem}\n",
    "- Un modelo apropiado en tiempo para tendencias lineales está dado por:\n",
    "\n",
    "    $$\n",
    "    y_{t}=\\beta_{0}+\\beta_{1}t+\\varepsilon_{t},\n",
    "    $$\n",
    "\n",
    "    donde $\\varepsilon_{t}$ es no correlacionado con media 0 y varianza constante $\\sigma_{\\varepsilon}^{2}$.\n",
    "\n",
    "- Además, el suavizador exponencial simple es un estimador sesgado para el `modelo de tendencia lineal` y la cantidad de sesgo es: $-(1-\\lambda)\\beta_{1}/\\lambda$, donde\n",
    "\n",
    "$$\n",
    "\\text{E}(\\tilde{y}_{T})=\\beta_{0}+\\beta_{1}T-\\frac{1-\\lambda}{\\lambda}\\beta_{1}=\\text{E}(y_{T})-\\frac{1-\\lambda}{\\lambda}\\beta_{1}.\n",
    "$$\n",
    "\n",
    "- Dada la `suavización exponencial de segundo orden` definida por:\n",
    "\n",
    "    $$\n",
    "    \\tilde{y}_{T}^{(2)}=\\lambda\\tilde{y}_{T}^{(1)}+(1-\\lambda)\\tilde{y}_{T-1}^{(2)},\n",
    "    $$\n",
    "\n",
    "    donde $\\tilde{y}_{T}^{(1)}, \\tilde{y}_{T}^{(2)}$ denotan las suavizaciones exponenciales de primer y segundo orden respectivamente. Entonces el predictor de $\\tilde{y}_{T}$ está dado por\n",
    "\n",
    "    $$\n",
    "    \\tilde{y}_{T}=2\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)}.\n",
    "    $$ \n",
    "\n",
    "- Pude demostrarse que $\\tilde{y}_{T}$ es insesgado.\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1e9b52",
   "metadata": {},
   "source": [
    "**`Demostración`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e25e8",
   "metadata": {},
   "source": [
    "- Bajo el supuesto de independencia sobre $(y_{t})_{t=1}^{\\infty}$, calculamos suavización exponencial para datos de series de tiempo que exhiben tendencia lineal\n",
    "\n",
    "$$\n",
    "\\text{E}(\\tilde{y}_{T})=\\text{E}\\left(\\lambda\\sum_{t=0}^{T-1}(1-\\lambda)^{t}y_{T-t}\\right)=\\lambda\\sum_{t=0}^{T-1}(1-\\lambda)^{t}\\text{E}(y_{T-t}).\n",
    "$$\n",
    "\n",
    "- Dado que para la tendencia lineal: $\\text{E}(y_{t})=E(\\beta_{0})+\\beta_{1}t+\\varepsilon_{t})=\\beta_{0}+\\beta_{1}t$, entonces\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{E}(\\tilde{y}_{T}) &= \\lambda\\sum_{t=0}^{T-1}(1-\\lambda)^{t}(\\beta_{0}+\\beta_{1}(T-t))\\\\\n",
    "&= \\lambda\\sum_{t=0}^{T-1}(1-\\lambda)^{t}(\\beta_{0}+\\beta_{1}t)-\\lambda\\sum_{t=0}^{T-1}(1-\\lambda)^{t}(\\beta_{1}t)\\\\\n",
    "&= (\\beta_{0}+\\beta_{1}T)\\lambda\\sum_{t=0}^{T-1}(1-\\lambda)^{t}-\\lambda\\beta_{1}\\sum_{t=0}^{T-1}(1-\\lambda)^{t}t.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892e4e9",
   "metadata": {},
   "source": [
    "- Dado que $|1-\\lambda|<1$ se tiene que $\\sum_{t=0}^{T-1}(1-\\lambda)^{t}=1/(1-(1-\\lambda))=1/\\lambda$. Nótese además que\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{t=0}^{\\infty}(1-\\lambda)^{t}t&=(1-\\lambda)\\sum_{t=0}^{\\infty}t(1-\\lambda)^{t-1}\\\\\n",
    "&= -(1-\\lambda)\\sum_{t=0}^{\\infty}\\frac{\\partial}{\\partial\\lambda}(1-\\lambda)^{t}\\\\\n",
    "&= -(1-\\lambda)\\frac{\\partial}{\\partial\\lambda}\\left(\\sum_{t=0}^{\\infty}(1-\\lambda)^{t}\\right)\\\\\n",
    "&= -(1-\\lambda)\\frac{\\partial}{\\partial\\lambda}\\left(\\frac{1}{1-(1-\\lambda)}\\right)\\\\\n",
    "&= -(1-\\lambda)\\frac{\\partial}{\\partial\\lambda}\\left(\\frac{1}{\\lambda}\\right)=-(1-\\lambda)\\left(-\\frac{1}{\\lambda^{2}}\\right)=\\frac{1-\\lambda}{\\lambda^{2}}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Entonces \n",
    "\n",
    "$$\n",
    "\\text{E}(\\tilde{y}_{T})=(\\beta_{0}+\\beta_{1}T)\\lambda\\left(\\frac{1}{\\lambda}\\right)-\\lambda\\beta_{1}\\left(\\frac{1-\\lambda}{\\lambda^{2}}\\right)=(\\beta_{0}+\\beta_{1}T)-\\frac{1-\\lambda}{\\lambda}\\beta_{1}=\\text{E}(y_{T})-\\frac{1-\\lambda}{\\lambda}\\beta_{1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf188cb8",
   "metadata": {},
   "source": [
    "- Esto es, la suavización exponencial simple es `sesgada como estimador` y la cantidad de sesgo está dada por $-(1-\\lambda)\\beta_{1}/\\lambda$, la cual puede generar errores de subestimación. \n",
    "- Una solución puede ser usar $\\lambda$ grande ya que $(1-\\lambda)/\\lambda\\rightarrow0$ cuando $\\lambda\\rightarrow1$. Técnicas autoadaptativas para $\\lambda$ pueden ser estudiadas, las cuales sigan los cambios en el proceso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad1672",
   "metadata": {},
   "source": [
    "- Dado que la suavización de segundo orden está dada por\n",
    "\n",
    "    $$\n",
    "    \\tilde{y}_{T}^{(2)}=\\lambda\\tilde{y}_{T}^{(1)}+(1-\\lambda)y_{T-1}^{(2)},\n",
    "    $$\n",
    "\n",
    "    esto es, $\\tilde{y}_{T}^{(2)}$ es obtenida aplicando suavización exponencial simple a $\\tilde{y}_{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e99da",
   "metadata": {},
   "source": [
    "- Por motivos de derivación de la expresión, consideramos el mismo $\\lambda$ para $\\tilde{y}_{T}^{(1)}$ y $\\tilde{y}_{t}^{(2)}$. Dado que la suavización exponencial de primer orden es sesgada, su suavización (`suavización de segundo orden`) también es sesgada. Entonces\n",
    "\n",
    "$$\n",
    "\\text{E}(\\tilde{y}_{T}^{(2)})=\\text{E}(\\tilde{y}_{T}^{(1)})-\\frac{1-\\lambda}{\\lambda}\\beta_{1}. \n",
    "$$\n",
    "\n",
    "- Además\n",
    "\n",
    "$$\n",
    "\\frac{1-\\lambda}{\\lambda}\\beta_{1}=\\text{E}(\\tilde{y}_{T}^{(1)})-E(\\tilde{y}_{T}^{(2)})\\Rightarrow\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)}=\\frac{1-\\lambda}{\\lambda}\\beta_{1},~\\text{entonces}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textcolor{red}{\\hat{\\beta}_{1, T}=\\frac{\\lambda}{1-\\lambda}(\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)})}.\n",
    "$$(beta1_estimation_eq)\n",
    "\n",
    "- Usando la Ecuación {eq}`beta1_estimation_eq` tenemos que: $\\text{E}(\\tilde{y}_{T}^{(1)})=(\\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T}T)-(1-\\lambda)\\hat{\\beta}_{1, T}/\\lambda$, entonces, análogamente\n",
    "\n",
    "$$\n",
    "\\textcolor{red}{\\hat{\\beta}_{0, T}=\\tilde{y}_{T}^{(1)}-T\\hat{\\beta}_{1, T}+\\displaystyle{\\frac{1-\\lambda}{\\lambda}}\\hat{\\beta}_{1, T}.}\n",
    "$$(beta0_estimation_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba4094",
   "metadata": {},
   "source": [
    "- Reescribiendo $\\hat{\\beta}_{0, T}$ en términos de $\\tilde{y}_{T}^{(1)}$ y $\\tilde{y}_{T}^{(2)}$ obtenemos\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\beta}_{0, T} &= \\tilde{y}_{T}^{(1)}-T\\left(\\frac{\\lambda}{1-\\lambda}(\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)})\\right)+\\frac{1-\\lambda}{\\lambda}\\left(\\frac{\\lambda}{1-\\lambda}(\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)})\\right)\\\\\n",
    "&= \\tilde{y}_{T}^{(1)}-T\\frac{\\lambda}{1-\\lambda}(\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)})+(\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)})\\\\\n",
    "&= \\left(2-T\\frac{\\lambda}{1-\\lambda}\\right)\\tilde{y}_{T}^{(1)}-\\left(1-T\\frac{\\lambda}{1-\\lambda}\\right)y_{T}^{(2)},\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Entonces\n",
    "\n",
    "$$\n",
    "\\textcolor{red}{\\hat{\\beta}_{0, T} = \\left(2-T\\frac{\\lambda}{1-\\lambda}\\right)\\tilde{y}_{T}^{(1)}-\\left(1-T\\frac{\\lambda}{1-\\lambda}\\right)y_{T}^{(2)}},\n",
    "$$(beta0_estimation_eq_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bcebc",
   "metadata": {},
   "source": [
    "- Combinando las Ecuaciones {eq}`beta1_estimation_eq`, {eq}`beta0_estimation_eq_yhat` tenemos que\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{y}_{T} &= \\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T}T=\\left(2-T\\frac{\\lambda}{1-\\lambda}\\right)\\tilde{y}_{T}^{(1)}-\\left(1-T\\frac{\\lambda}{1-\\lambda}\\right)\\tilde{y}_{T}^{(2)}+\\frac{\\lambda}{1-\\lambda}(\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)})T\\\\\n",
    "&= \\left(2-T\\frac{\\lambda}{1-\\lambda}+T\\frac{\\lambda}{1-\\lambda}\\right)\\tilde{y}_{T}^{(1)}-\\left(1-T\\frac{\\lambda}{1-\\lambda}+T\\frac{\\lambda}{1-\\lambda}\\right)\\tilde{y}_{T}^{(2)}\\\\[2mm]\n",
    "&=2\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)}.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Entonces\n",
    "\n",
    "$$\n",
    "\\textcolor{red}{\\tilde{y}_{T}=2\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)}}\n",
    "$$(first_exp_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09074a19",
   "metadata": {},
   "source": [
    "- Nótese que como\n",
    "\n",
    "$$\n",
    "\\text{E}(\\tilde{y}_{T}^{(1)}) = (\\beta_{0}+\\beta_{1}T)-\\frac{1-\\lambda}{\\lambda}\\beta_{1}\\Rightarrow\\tilde{y}_{T}^{(1)}=(\\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T}T)-\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f790f6",
   "metadata": {},
   "source": [
    "- Además\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{E}(\\tilde{y}_{T}^{(2)}) &= \\text{E}(\\tilde{y}_{T}^{(1)})-\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, T}=\\text{E}\\left((\\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T}T)-\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, T}\\right)-\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, T}\\\\\n",
    "&= (\\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T})-\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, T}-\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, T}=(\\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T})-2\\left(\\frac{1-\\lambda}{\\lambda}\\right)\\hat{\\beta}_{1, T}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a68fab4",
   "metadata": {},
   "source": [
    "- Entonces\n",
    "\n",
    "$$\n",
    "\\textcolor{red}{\n",
    "\\begin{align*}\n",
    "\\tilde{y}_{T}^{(1)} &= (\\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T})-\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, T}\\\\\n",
    "\\tilde{y}_{T}^{(2)} &= (\\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T})-2\\left(\\frac{1-\\lambda}{\\lambda}\\right)\\hat{\\beta}_{1, T}\n",
    "\\end{align*}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a4340",
   "metadata": {},
   "source": [
    "- Con los respectivos valores iniciales\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{y}_{0}^{(1)} &= \\hat{\\beta}_{0, 0}-\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, 0}\\\\\n",
    "\\tilde{y}_{0}^{(2)} &= \\hat{\\beta}_{0, 0}-2\\left(\\frac{1-\\lambda}{\\lambda}\\right)\\hat{\\beta}_{1, 0}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc024a21",
   "metadata": {},
   "source": [
    "- La `estimación de los parámetros iniciales` es obtenida usualmente por medio del `método de mínimos cuadrado`. Por ejemplo, los parámetros $\\hat{\\beta}_{0, 0}, \\hat{\\beta}_{1, 0}$ asociados al `US Consumer Prices Index (CPI)` de Enero 1995 a Diciembre 2004, obtenidos por mínimos cuadrados están dados por $\\hat{\\beta}_{0, 0}=149.89$ y $\\hat{\\beta}_{1, 0}=0.3$. Usando $\\lambda=0.3$ se tiene que\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}_{0}^{(1)} &= \\hat{\\beta}_{0, 0}-\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, 0}=149.89-\\frac{1-0.3}{0.3}0.3=146.22\\\\\n",
    "\\hat{y}_{0}^{(2)} &= \\hat{\\beta}_{0, 0}-2\\frac{1-\\lambda}{\\lambda}\\hat{\\beta}_{1, 0}=149.89-2\\frac{1-0.3}{0.3}0.3=142.56\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa0f1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ff36a15",
   "metadata": {},
   "source": [
    "## Suavización exponencial de alto orden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3186c911",
   "metadata": {},
   "source": [
    "- Hasta ahora hemos discutido el uso de suavizadores exponenciales al estimar los `modelos de tendencia constante y lineal`. Para el primero empleamos el suavizador exponencial simple o de primer orden y para el segundo el suavizador exponencial de segundo orden. Además, se puede demostrar que para el `modelo polinómico general de` $n\\text{th}$-ésimo grado de la forma\n",
    "\n",
    "    $$\n",
    "    y_{t}=\\beta_{0}+\\beta_{1}t+\\frac{\\beta_{2}}{2!}t^{2}+\\cdots+\\frac{\\beta_{n}}{n!}t^{n}+\\varepsilon_{t},\n",
    "    $$(highorder_exponential_smoothing)\n",
    "\n",
    "    donde $\\varepsilon_{t}$ se supone independiente con media 0 y varianza constante $\\sigma_{\\varepsilon}^{2}$, empleamos suavizadores exponenciales de orden $n-1$\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\tilde{y}_{T}^{(1)} &= \\lambda y_{T}+(1-\\lambda)\\tilde{y}_{T-1}^{(1)}\\\\\n",
    "    \\tilde{y}_{T}^{(2)} &= \\lambda \\tilde{y}_{T}^{(1)}+(1-\\lambda)\\tilde{y}_{T-1}^{(2)}\\\\\n",
    "    &\\vdots\\\\\n",
    "    \\tilde{y}_{T}^{(n)} &= \\lambda \\tilde{y}_{T}^{(n-1)}+(1-\\lambda)\\tilde{y}_{T-1}^{(n)},\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    \n",
    "    para estimar los parámetros del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd77f9e",
   "metadata": {},
   "source": [
    "````{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "- Incluso para el modelo cuadrático, los cálculos se vuelven bastante complicados. Consulte {cite}`montgomery1990forecasting`, {cite}`brown2004smoothing` y {cite}`abraham2009statistical` para las `soluciones a problemas de suavizamiento exponencial de orden superior`.\n",
    "\n",
    "- Si un polinomio de alto orden parece ser necesario para la serie de tiempo, los `modelos autorregresivos de media móvil integrada (ARIMA) pueden considerarse en cambio`.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62b7dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "705dbdcd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a7c894e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02d60f00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d5fb4bb",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b7bc4a",
   "metadata": {},
   "source": [
    "``````{admonition} Forecasting\n",
    "- En el tiempo $T$, deseamos predecir la observación en la unidad de tiempo siguiente, $T+1$, u otro valor en el futuro. Denotaremos el pronóstico $\\tau$ pasos hacia adelante (*horizonte* $\\tau$) realizado en el tiempo $T$ como $\\hat{y}_{T+\\tau}(T)$.\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a0a3df",
   "metadata": {},
   "source": [
    "### Proceso constante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b311d",
   "metadata": {},
   "source": [
    "- La suavización exponencial de primer orden para el proceso constante está dada por \n",
    "\n",
    "$$\n",
    "\\tilde{y}_{T}=\\lambda y_{T}+(1-\\lambda)\\tilde{y}_{T-1}\n",
    "$$\n",
    "\n",
    "- Se demostró también que $f(t, \\boldsymbol{\\beta})=\\beta_{0}$ puede ser estimado con $\\tilde{y}_{T}$. Dado que hasta el momento, el error aleatorio no puede ser predicho\n",
    "\n",
    "$$\n",
    "\\hat{y}_{T+\\tau}(T)=\\tilde{y}_{T}=\\hat{\\beta}_{0}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af691a56",
   "metadata": {},
   "source": [
    "- Dado que pronósticos constantes pueden no ser correctos a medida que vamos acumulando observaciones, actualizamos nuestra predicción por medio de técnicas como el `rolling`.\n",
    "\n",
    "- Por ejemplo, si la observación en el tiempo $T+1$ está disponible, nuestro pronostico para futuras observaciones sería\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{y}_{T+1} &= \\lambda y_{T+1}+(1-\\lambda)\\tilde{y}_{T},\\quad\\text{o bien}\\\\\n",
    "\\tilde{y}_{T+1+\\tau}(T+1) &= \\lambda y_{T+1}+(1-\\lambda)\\tilde{y}_{T+\\tau}(T)\\\\[4mm]\n",
    "\\end{align*}\n",
    "$$(fc_in_tplustau_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc4026",
   "metadata": {},
   "source": [
    "``````{figure} ./figures/forecasttau_timeseries.png\n",
    ":align: center\n",
    ":name: forecasttau_timeseries_fig\n",
    ":scale: 18\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d2316",
   "metadata": {},
   "source": [
    "- Para $\\tau=1$ la Ecuación {eq}`fc_in_tplustau_eq` puede reescribirse como\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    \\tilde{y}_{T+2}(T+1) &= \\lambda y_{T+1}+(1-\\lambda)\\tilde{y}_{T+1}(T)\\\\\n",
    "    &= \\tilde{y}_{T+1}(T)+\\lambda(y_{T+1}-\\tilde{y}_{T+1}(T))\\\\\n",
    "    &= \\tilde{y}_{T+1}(T)+\\lambda e_{T+1}(1),\n",
    "    \\end{align*}\n",
    "    $$\n",
    "\n",
    "    donde $e_{T+1}(1):=y_{T+1}-\\tilde{y}_{T+1}(T)$ es llamado, predicción de un paso hacia adelante (`one-step-ahead`) o error de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8494256",
   "metadata": {},
   "source": [
    "``````{admonition} Observación\n",
    "- Pronostico para la observación siguiente es simplemente, el `pronostico actual mas una fracción del error cometido en el pronostico de la observación actual`\n",
    "- Nótese que la velocidad de reacción del pronostico al error de predicción depende del factor de descuento $\\lambda$ \n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e22526",
   "metadata": {},
   "source": [
    "## Selección de $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf24fd7",
   "metadata": {},
   "source": [
    "- Definimos la suma de cuadrado para los errores de predicción de horizonte 1 como\n",
    "\n",
    "$$\n",
    "SS_{E}(\\lambda)=\\sum_{t=1}^{T}e_{t}^{2}(1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5a78f",
   "metadata": {},
   "source": [
    "``````{admonition} Observación\n",
    ":class: tip\n",
    "- Para un conjunto histórico de datos, podemos en general calcular $SS_{E}(\\lambda)$ para varios valores de $\\lambda$ y `seleccionar aquel con la menor suma de errores de predicción al cuadrado`.\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5114d9",
   "metadata": {},
   "source": [
    "## Intervalo de predicción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0d6b9f",
   "metadata": {},
   "source": [
    "- El cálculo de intervalos de predicción requiere de la `estimación de la varianza del error de predicción`. Este tipo de técnicas serán estudiadas mas adelante en la presente sección"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375de92",
   "metadata": {},
   "source": [
    "``````{admonition} Proceso contante\n",
    "- Para un proceso constante, el intervalo de predicción $100(1-\\alpha/2)$ para cualquier horizonte $\\tau$ está dado por\n",
    "\n",
    "    $$\n",
    "    \\tilde{y}\\pm Z_{\\alpha/2}\\hat{\\sigma}_{e},\n",
    "    $$\n",
    "\n",
    "    donde $\\tilde{y}_{T}$ es la suavización exponencial de primer orden, $Z_{\\alpha/2}$ es el $100(1-\\alpha/2)$ percentíl de la distribución normal estándar, y $\\hat{\\sigma}_{e}$ es la estimación de la desviación estándar de los errores predichos.\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0654fe",
   "metadata": {},
   "source": [
    "- Encontraremos solución a problemas de intervalo constante para todos los horizontes de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37422c90",
   "metadata": {},
   "source": [
    "``````{admonition} Proceso de tendencia lineal\n",
    "- El pronóstico de horizonte $\\tau$ para el `modelo de tendencia lineal` está dado por\n",
    "\n",
    "$$\n",
    "\\hat{y}_{T+\\tau}=\\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T}(T+\\tau)=\\hat{\\beta}_{0, T}+\\hat{\\beta}_{1, T}T+\\hat{\\beta}_{1, T}\\tau=\\hat{y}_{T}+\\hat{\\beta}_{1, T}\\tau.\n",
    "$$\n",
    "\n",
    "- En términos de suavizadores exponenciales\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}_{T+\\tau}(T) &= (2\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)})+\\tau\\frac{\\lambda}{1-\\lambda}(\\tilde{y}_{T}^{(1)}-\\tilde{y}_{T}^{(2)})\\\\\n",
    "&= \\left(2+\\frac{\\lambda}{1-\\lambda}\\tau\\right)\\tilde{y}_{T}^{(1)}-\\left(1+\\frac{\\lambda}{1-\\lambda}\\tau\\right)\\tilde{y}_{T}^{(2)}.\n",
    "\\end{align*}\n",
    "$$\n",
    "``````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16778a",
   "metadata": {},
   "source": [
    "- Nótese que las predicciones dependerán ahora del horizonte de predicción $\\tau$. En este caso el $100(1-\\alpha/2)$ intervalo de predicción para cualquier horizonte de tiempo $\\tau$ está dado por\n",
    "\n",
    "$$\n",
    "\\left(2+\\frac{\\lambda}{1-\\lambda}\\tau\\right)\\tilde{y}_{T}^{(1)}-\\left(1+\\frac{\\lambda}{1-\\lambda}\\tau\\right)\\tilde{y}_{T}^{(2)}\\pm Z_{\\alpha/2}\\hat{\\sigma}_{e}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7d8127",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb76ece",
   "metadata": {},
   "source": [
    "## Estimación de $\\sigma_{e}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f642f2",
   "metadata": {},
   "source": [
    "- En el proceso de estimar la varianza de los errores de pronóstico, representada como $\\sigma_{e}^{2}$, frecuentemente se postula que el modelo subyacente (por ejemplo, constante, tendencia lineal) es `correcto e invariante en el tiempo`. Bajo estas condiciones, dos metodologías distintas están disponibles para estimar $\\sigma_{e}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0018b6c2",
   "metadata": {},
   "source": [
    "```{admonition} Assignment\n",
    ":class: tip\n",
    "\n",
    "- Encuentre la estimación de la varianza de los errores de pronóstico $\\sigma_{e}^{2}$ para el modelo de suavización exponencial. Utilice `Látex` para redactar la solución y además, utilice los siguientes documentos como guía: [Peter Wanke et. al](https://www.pomsmeetings.org/confproceedings/015/fullpapers/015-0271.pdf) y [Kuliah Genap](http://adjifern.lecture.ub.ac.id/files/2016/03/bahan-ADE-minggu-11b.pdf).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0370a8",
   "metadata": {},
   "source": [
    "1. Dado el pronostico `one-step-ahead` definido como $e_{T}(1)=y_{T}-\\hat{y}_{T}(T-1)$. El objetivo es aplicar el modelo a los datos históricos y obtener el error de pronostico a calcular \n",
    "\n",
    "$$\n",
    "\\sigma_{e}^{2}=\\frac{1}{T}\\sum_{t=1}^{T}e_{t}^{2}(1)=\\frac{1}{T}\\sum_{t=1}^{T}(y_{t}-\\hat{y}_{t}(t-1))^{2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d5a9d",
   "metadata": {},
   "source": [
    "- Nótese que en el calculo de la varianza, la media del ajuste no fue necesaria, ya que para el modelo correcto los pronósticos son insesgados; esto es, el valor esperado de los errores de pronóstico es 0. \n",
    "\n",
    "- A medida que se recopilan más datos, la varianza de los errores de pronóstico puede ser actualizada\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{eT+1}=\\frac{1}{T+1}\\left(T\\hat{\\sigma}_{e, T}^{2}+e_{T+1}^{2}(1)\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2a00bb",
   "metadata": {},
   "source": [
    "- Puede resultar contraintuitivo tener una varianza constante de errores de pronóstico para todos los horizontes de predicción. En su lugar, podemos definir $\\sigma_{e}^{2}(\\tau)$ como la `varianza del error de pronóstico a` $\\tau$ `pasos hacia adelante` y estimarla mediante\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{e}^{2}(\\tau)=\\frac{1}{T-\\tau+1}\\sum_{t=\\tau}^{T}e_{1}^{2}(\\tau).\n",
    "$$(error_variance_estimation_tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba38056",
   "metadata": {},
   "source": [
    "- Por lo tanto, la estimación en la Ecuación `eq`{error_variance_estimation_tau} puede ser utilizada en los cálculos del `intervalo de predicción para el pronóstico a` $\\tau$ `pasos hacia adelante`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81291f",
   "metadata": {},
   "source": [
    "2. Para el segundo método de estimación de $\\sigma_{e}^{2}$, primero definiremos la `desviación media absoluta` $\\Delta$ como\n",
    "\n",
    "    $$\n",
    "    \\Delta = E(|e-E(e)|)\n",
    "    $$\n",
    "    \n",
    "    y, asumiendo que el modelo es correcto, calcular su estimación mediante\n",
    "\n",
    "    $$\n",
    "    \\hat{\\Delta}_{T}=\\delta|e_{T}(1)|+(1-\\delta)\\hat{\\Delta}_{T-1}.\n",
    "    $$(mean_absolute_deviationT)\n",
    "\n",
    "    Entonces, el estimador de $\\sigma_{e}^{2}$ está dado por\n",
    "\n",
    "    $$\n",
    "    \\hat{\\sigma}_{e, T}=1.25\\hat{\\Delta}_{T}.\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea81ff",
   "metadata": {},
   "source": [
    "## Adaptación actualizada del factor de descuento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65043d45",
   "metadata": {},
   "source": [
    "- Cambios en el modelo subyacente de series temporales dificultarán que el suavizador exponencial con factor de descuento fijo siga estos cambios. Por lo tanto, surge la necesidad de `monitorear y, si es necesario, modificar el factor de descuento`. Al hacerlo, el factor de descuento se adaptará a los cambios en el modelo de series temporales. \n",
    "\n",
    "- Para ello, emplearemos el procedimiento descrito originalmente por {cite}`trigg1967exponential` para un único factor de descuento. Como ejemplo, consideraremos el `suavizador exponencial de primer orden` y lo modificaremos de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\hat{y}_{T}=\\lambda_{T}y_{T}+(1-\\lambda_{T})\\hat{y}_{T-1}.\n",
    "$$(adaptive_discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd1a2fc",
   "metadata": {},
   "source": [
    "- Nótese que en la Ecuación {eq}`adaptive_discount_factor`, el factor de descuento $\\lambda_{T}$ se introduce como una función del tiempo y, por lo tanto, `se le permite adaptarse a los cambios en el modelo de series temporales`. \n",
    "\n",
    "- También definimos el `error de suavizado` como\n",
    "\n",
    "    $$\n",
    "    Q_{T}=\\delta e_{T}(1)+(1-\\delta)Q_{T-1},\n",
    "    $$\n",
    "\n",
    "    donde $\\delta$ es un parámetro de suavización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9752c48",
   "metadata": {},
   "source": [
    "- Por último, definimos la señal de seguimiento como\n",
    "\n",
    "    $$\n",
    "    \\frac{Q_{T}}{\\hat{\\Delta}_{T}},\n",
    "    $$\n",
    "\n",
    "    donde $\\hat{\\Delta}_{T}$ está dado por la Ecuación {eq}`mean_absolute_deviationT`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbf0db8",
   "metadata": {},
   "source": [
    "- Se espera que este cociente sea cercano a 0 cuando el sistema de predicción funciona bien, y que se acerque a $\\pm1$ cuando empiece a fallar. De hecho, {cite}`trigg1967exponential` sugieren fijar el factor de descuento en\n",
    "\n",
    "    $$\n",
    "    \\lambda_{T}=\\left|\\frac{Q_{T}}{\\hat{\\Delta}_{T}}\\right|,\n",
    "    $$(adaptive_discount_factor_abs)\n",
    "\n",
    "    Donde $\\hat{\\Delta}_{T}$ está dado por la Ecuación {eq}`mean_absolute_deviationT`. La Ecuación {eq}`adaptive_discount_factor_abs` `permitirá actualizar automáticamente el factor de descuento`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da949cb",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc89c6a",
   "metadata": {},
   "source": [
    "- Si el modelo de pronóstico funciona como se esperaba, `los errores de pronóstico deberían no exhibir ningún patrón o estructura`; es decir, no deberían estar correlacionados. Por lo tanto, siempre es una buena idea verificar esto. Puede hacerlo calculando el `ACF muestral de los errores de pronóstico` de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce6bdf",
   "metadata": {},
   "source": [
    "$$\n",
    "r_{k}=\\frac{\\displaystyle{\\sum_{t=k}^{T-1}[e_{t}(1)-\\overline{e}][e_{t-k}(1)-\\overline{e}]}}{\\displaystyle{\\sum_{T=0}^{T-1}[e_{t}(1)-\\overline{e}]^{2}}},~\\text{donde}~\\overline{e}=\\frac{1}{n}\\sum_{t=1}^{T}e_{t}(1).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186386c",
   "metadata": {},
   "source": [
    "````{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "- Si los `errores de pronóstico` de un paso adelante en realidad `no están correlacionados`, las `autocorrelaciones muestrales para cualquier rezago` $k$ `deben estar alrededor de 0` con un error estándar $1/\\sqrt{T}$.\n",
    "\n",
    "- De ahí una autocorrelación muestral para cualquier rezago $k$ que se encuentre fuera de los límites $\\pm2/\\sqrt{T}$ requerirán una mayor investigación del modelo.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48ae8c3",
   "metadata": {},
   "source": [
    "## Suavización exponencial para datos estacionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a306b0e",
   "metadata": {},
   "source": [
    "- Algunos datos de series de tiempo exhiben `patrones cíclicos o estacionales` que no se pueden modelar efectivamente utilizando el modelo polinómico de la Ecuación {eq}`highorder_exponential_smoothing`. Hay varios enfoques disponibles para el análisis de dichos datos. \n",
    "\n",
    "- La metodología en la que nos centraremos fue introducida originalmente por `Holt (1957) and Winters (1960)` y generalmente se conoce como `método de Winters`, donde se realiza un ajuste estacional al modelo de tendencia lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd39f4e8",
   "metadata": {},
   "source": [
    "### Modelo estacional aditivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ce904",
   "metadata": {},
   "source": [
    "- Para el caso del `modelo estacional aditivo`, asumiremos que la serie temporal estacional se puede representar mediante el siguiente modelo:\n",
    "\n",
    "    $$\n",
    "    y_{t}=L_{t}+S_{t}+\\varepsilon_{t},\n",
    "    $$(additive_stational_model)\n",
    "\n",
    "    donde $L_{t}$ representa el nivel o `componente de tendencia lineal` y puede a su vez ser representado por $\\beta_{0}+\\beta_{1}t$; $S_{t}$ representa el `ajuste estacional` con $S_{t}=S_{t+s}=S_{t+2s}=\\cdots$ para $t=1, 2,\\cdots, s-1$ donde $s$ es la longitud de período (estación) de los ciclos; y suponemos que $\\varepsilon_{t}$ es `no correlacionado con media o y varianza constante` $\\sigma_{\\varepsilon}^{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432c2db",
   "metadata": {},
   "source": [
    "- Una `restricción habitual` de este modelo es que los ajustes estacionales suman cero durante un periodo,\n",
    "\n",
    "$$\n",
    "\\sum_{t=1}^{s}S_{t}=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a58915",
   "metadata": {},
   "source": [
    "- En el modelo dado en la Ecuación {eq}`additive_stational_model`, para pronosticar las observaciones futuras, emplearemos `suavizadores exponenciales de primer orden con diferentes factores de descuento`. El procedimiento para actualizar las estimaciones de los parámetros una vez que se obtiene la observación actual $y_{T}$ es el siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd2a6c",
   "metadata": {},
   "source": [
    "1. Actualizar la estimación de $L_{t}$ usando\n",
    "\n",
    "    $$\n",
    "    \\hat{L}_{t}=\\lambda_{1}(y_{T}-\\hat{S}_{T-s})+(1-\\lambda_{1})(\\hat{L}_{T-1}+\\hat{\\beta}_{1, T-1}),\n",
    "    $$(LT_estimation)\n",
    "\n",
    "    donde $0<\\lambda_{1}<1$. Cabe señalar que en la Ecuación {eq}`LT_estimation`, la primera parte puede verse como el valor \"actual\" de $L_{T}$ y la segunda parte como el pronóstico de $L_{T}$ basado en las estimaciones en $T-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5cf0b3",
   "metadata": {},
   "source": [
    "2. Actualizar la estimación de $\\beta_{1}$ usando\n",
    "\n",
    "   $$\n",
    "   \\hat{\\beta}_{1, T}=\\lambda_{2}(\\hat{L}_{T}-\\hat{L}_{T-1})+(1-\\lambda_{2})\\hat{\\beta}_{1, T-1},\n",
    "   $$(beta1T_additive_seasonal_model)\n",
    "\n",
    "   donde $0<\\lambda_{2}<1$. Como en el Paso 1, la estimación de $\\beta_{1}$ en la Ecuación {eq}`beta1T_additive_seasonal_model` puede verse como la combinación lineal del actual valor de $\\beta_{1}$ y su pronóstico an el tiempo $T-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cfaaf-f4c9-4a6f-b1fc-0bf4ac6f7d46",
   "metadata": {},
   "source": [
    "3. Actualizar la estimación de $S_{t}$ usando\n",
    "\n",
    "   $$\n",
    "   \\hat{S}_{T}=\\lambda_{3}(y_{T}-\\hat{L}_{T})+(1-\\lambda_{3})\\hat{S}_{T-s},\n",
    "   $$(seasonal_forecast_additive_model)\n",
    "\n",
    "   donde $0<\\lambda_{3}<1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77446810-82f2-44ef-9a02-8b397bf1b00c",
   "metadata": {},
   "source": [
    "4. Finalmente, el pronóstico $\\tau$ pasos hacia adelante, $\\hat{y}_{T+\\tau}(T)$, es\n",
    "\n",
    "   $$\n",
    "   \\hat{y}_{T+\\tau}(T)=\\hat{L}_{T}+\\hat{\\beta}_{1, T}\\tau+\\hat{S}_{T}(\\tau-s).\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a74781a-7c08-4a0f-b069-5e653285af0b",
   "metadata": {},
   "source": [
    "- Estimar los valores iniciales de una suavización exponencial es crucial. Para un conjunto dado de datos históricos con $n$ estaciones ($ns$ observaciones), podemos usar las estimaciones de mínimos cuadrados del siguiente modelo:\n",
    "\n",
    "  $$\n",
    "  y_{t}=\\beta_{0}+\\beta_{1}t+\\sum_{i=1}^{s-1}\\gamma_{i}(I_{t, i}-I_{t, s})+\\varepsilon_{t},\n",
    "  $$(ols_estimation_additive_model)\n",
    "\n",
    "  donde\n",
    "\n",
    "  $$\n",
    "  I_{t, i}=\n",
    "  \\begin{cases}\n",
    "  1, & t=i, i+s, i+2s, \\dots\\\\\n",
    "  0, & \\text{en otro caso}.\n",
    "  \\end{cases}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ceeb5d-b2dc-4c98-aba5-cb30c1f825a5",
   "metadata": {},
   "source": [
    "- Las estimaciones de mínimos cuadrados de los parámetros de la Ecuación {eq}`ols_estimation_additive_model` se utilizan para obtener los valores iniciales como"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddce40-e246-4a95-b4a8-765d5a9314e8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\beta}_{0, 0} &= \\hat{L}_{0}=\\hat{\\beta}_{0}\\\\\n",
    "\\hat{\\beta}_{1, 0} &= \\hat{\\beta}_{1}\\\\\n",
    "\\hat{S}_{j-s} &= \\hat{Y}_{j},~\\text{para}~ 1\\leq j\\leq s-1\\\\\n",
    "\\hat{S}_{0} &= -\\sum_{j=1}^{s-1}\\hat{y}_{j}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ea57d-0de5-4a7b-b823-2364761d2035",
   "metadata": {},
   "source": [
    "- Estos son valores iniciales de los parámetros del modelo en el origen original de tiempo, $t = 0$. Para realizar pronósticos desde el origen correcto de tiempo, la componente permanente debe trasladarse al tiempo $T$ calculando $\\hat{L}_{T}=\\hat{L}_{0}+ns\\hat{\\beta}_{1}$. Alternativamente, se podrían suavizar los parámetros usando las Ecuaciones {eq}`LT_estimation`-{eq}`seasonal_forecast_additive_model` para periodos de tiempo $t = 1, 2,\\dots, T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf4a1e-891f-47b3-863e-ca73fa189771",
   "metadata": {},
   "source": [
    "````{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "- Intervalos de predicción como en el caso del suavizado no estacional, requerirían `estimación de la varianza del error de predicción`. El enfoque más común es utilizar la `relación entre las técnicas de suavizado exponencial y modelos ARIMA` que abordaremos en el siguiente capítulo, y estimar la variación del error de predicción acordemente.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf9cb31-d6cb-472a-a6c0-84e80152536a",
   "metadata": {},
   "source": [
    "### Modelo estacional multiplicativo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693dcf6b-4712-4259-84a8-45a1eaf56ca5",
   "metadata": {},
   "source": [
    "- Si la amplitud del patrón estacional es proporcional al nivel medio de la serie temporal estacional, el siguiente modelo estacional multiplicativo será el más adecuado\n",
    "\n",
    "  $$\n",
    "  y_{t}=L_{t}S_{t}+\\varepsilon_{t},\n",
    "  $$(multiplicative_stational_model)\n",
    "\n",
    "  donde $L_{t}$ una vez más representa la componente permanente (i.e. $\\beta_{0}+\\beta_{1}t$); $S_{t}$ representa el ajuste estacional con $S_{t}=S_{t+s}=S_{t+2s}=\\cdots$ para $t=1,2,\\dots, s-1$, donde $s$ es la longitud del periodo de los ciclos; y $\\varepsilon_{t}$ se supone no correlacionado con media 0 y varianza constante $\\sigma_{\\varepsilon}^{2}$.\n",
    "\n",
    "- La restricción para los ajustes estacionales en este caso se convierte en\n",
    "\n",
    "  $$\n",
    "  \\sum_{t}^{s}S_{t}=s.\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6259b21a-bc0c-4940-a999-730b83b2b2fe",
   "metadata": {},
   "source": [
    "- Como en el modelo aditivo, emplearemos tres suavizadores exponenciales para estimar los parámetros en la Ecuación {eq}`multiplicative_stational_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4561b-ec89-4f5e-a824-6b2f24c0e09b",
   "metadata": {},
   "source": [
    "1. Actualizar el estimador de $L_{T}$ usando\n",
    "\n",
    "   $$\n",
    "   \\hat{L}_{T}=\\lambda_{1}\\frac{y_{T}}{\\hat{S}_{T-s}}+(1-\\lambda_{1})(\\hat{L}_{T-1}+\\hat{\\beta}_{1, T-1}), \n",
    "   $$\n",
    "\n",
    "   donde $0<\\lambda_{1}<1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7193853-6596-4e77-88ee-52914dbe776c",
   "metadata": {},
   "source": [
    "2. Actualizar el estimador de $\\beta_{1}$ utilizando\n",
    "\n",
    "   $$\n",
    "   \\hat{\\beta}_{1, T}=\\lambda_{2}(\\hat{L}_{T}-\\hat{L}_{T-1})+(1-\\lambda_{2})\\hat{\\beta}_{1, T-1},\n",
    "   $$\n",
    "\n",
    "   donde $0<\\lambda<1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd58ee1-466f-42ed-99cd-1f39b10498a7",
   "metadata": {},
   "source": [
    "3. Actualizar la estimación de $S_{t}$ usando\n",
    "\n",
    "   $$\n",
    "   \\hat{S}_{T}=\\lambda_{3}\\frac{y_{T}}{\\hat{L}_{T}}+(1-\\lambda_{3})\\hat{S}_{T-s},\n",
    "   $$\n",
    "\n",
    "   donde $0<\\lambda_{3}<1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bef451-fe33-468c-b0bd-d004d0698ce5",
   "metadata": {},
   "source": [
    "4. El pronóstico $\\tau$ pasos hacia adelante, $\\hat{y}_{T+\\tau}(T)$, es\n",
    "   $$\n",
    "   \\hat{y}_{T+\\tau}(T)=(\\hat{L}_{T}+\\hat{\\beta}_{1, T}\\tau)\\hat{S}_{T}(\\tau-s).\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6998a23-08e2-489f-b0bb-1d9bea24765b",
   "metadata": {},
   "source": [
    "- Supongamos que un registro que consta de $n$ estaciones de datos es disponible. De este conjunto de datos históricos, los valores iniciales, $\\hat{\\beta}_{0, 0}, \\hat{\\beta}_{1, 0}$ y $\\hat{S}_{0}$, se puede calcular como\n",
    "\n",
    "  $$\n",
    "  \\hat{\\beta}_{0, 0}=\\hat{L}_{0}=\\frac{\\overline{y}_{n}-\\overline{y}_{1}}{(n-1)s},\n",
    "  $$\n",
    "\n",
    "  donde\n",
    "\n",
    "  $$\n",
    "  \\overline{y}_{i}=\\frac{1}{s}\\sum_{t=(i-1)s+1}^{is}y_{t}\n",
    "  $$\n",
    "\n",
    "  y\n",
    "\n",
    "  $$\n",
    "  \\begin{align*}\n",
    "  \\hat{\\beta}_{1, 0} &= \\overline{y}_{1}-\\frac{s}{2}\\hat{\\beta}_{0, 0}\\\\\n",
    "  \\hat{S}_{j-s} &= s\\frac{\\hat{S}_{j}^{\\star}}{\\sum_{i=1}^{s}\\hat{S}_{i}^{\\star}}~\\text{para}~1\\leq j\\leq s, \n",
    "  \\end{align*}\n",
    "  $$\n",
    "\n",
    "  donde\n",
    "\n",
    "  $$\n",
    "  \\hat{S}_{j}^{\\star}=\\frac{1}{n}\\sum_{t=1}^{n}\\frac{y_{(t-1)s+j}}{\\overline{y}_{t}-((s+1)/2-j)\\hat{\\beta}_{0}}.\n",
    "  $$\n",
    "\n",
    "  Para más detalles ver {cite}`montgomery1990forecasting` y {cite}`abraham2009statistical`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ccaaf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c058532d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f6c641b",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb3d7b21",
   "metadata": {},
   "source": [
    "- La eficacia de la extracción de la tendencia y ruido estimados depende de otros parámetros relacionados con la composición de la señal de la serie temporal, como la presencia de `tendencia, estacionalidad y residual (ruido)`. Para tratar cada uno de estos componentes de las series temporales, se requieren diferentes tratamientos. En este capítulo se tratarán `múltiples enfoques de suavización para manejar los diferentes componentes de la señal` de la serie temporal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82c7d138",
   "metadata": {},
   "source": [
    "- Un ejemplo de una señal de serie temporal compuesta por `tendencia, estacionalidad y ruido blanco`, es el conjunto de datos de `nacimientos mensuales en Nueva York desde enero de 1946 hasta diciembre de 1959`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b145ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pmdarima import arima\n",
    "from pmdarima import datasets\n",
    "from pmdarima import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f9dd4fc-1c56-4752-9566-e44b857fd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4629afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='robjhyndman.com', port=443): Max retries exceeded with url: /tsdldata/data/nybirths.dat (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002138F837100>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\urllib3\\util\\connection.py:72\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[0;32m     69\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     )\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     73\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\socket.py:954\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    953\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    955\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\urllib3\\connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\urllib3\\connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\urllib3\\connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    188\u001b[0m     )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000002138F837100>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\urllib3\\connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    796\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 798\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='robjhyndman.com', port=443): Max retries exceeded with url: /tsdldata/data/nybirths.dat (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002138F837100>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fopen \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://robjhyndman.com/tsdldata/data/nybirths.dat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m      2\u001b[0m ds\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(io\u001b[38;5;241m.\u001b[39mStringIO(fopen\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)),  header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbirthcount\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m ds\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ml_tf\\lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='robjhyndman.com', port=443): Max retries exceeded with url: /tsdldata/data/nybirths.dat (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000002138F837100>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "fopen = requests.get(\"https://robjhyndman.com/tsdldata/data/nybirths.dat\").content\n",
    "ds=pd.read_csv(io.StringIO(fopen.decode('utf-8')),  header=None, names=['birthcount'])\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874e6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=pd.date_range(\"1946-01-01\", \"1959-12-31\", freq=\"1M\")\n",
    "ds['Date']=pd.DataFrame(date)\n",
    "ds = ds.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b054b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.index = pd.to_datetime(ds.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a738ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds.birthcount);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06d27006",
   "metadata": {},
   "source": [
    "- Observar el cambio en la `magnitud de las fluctuaciones` de manera visual, suele ser una tarea tediosa para ciertas series de tiempo. Por lo tanto, utilizaremos técnicas basadas en el cálculo del término ($s_{t}+\\varepsilon_{t}$) o factor ($s_{t}\\times\\varepsilon_{t}$) y su respectiva visualización. Si la magnitud de las fluctuaciones obtenidas son similares entonces concluimos que la serie de tiempo es aditiva o multiplicativa según sea el caso. `Es un buen ejercicio para el estudiante calcular la magnitud de dichas fluctuaciones y visualizarlas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_kwargs = {'figsize': (6, 6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed = arima.decompose(ds.birthcount.values, 'multiplicative', m=12)\n",
    "# axes = utils.decomposed_plot(decomposed, figure_kwargs=figure_kwargs, show=False)\n",
    "# axes[0].set_title(\"Seasonal Multiplicative Decomposition\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ee30f22",
   "metadata": {},
   "source": [
    "- Las diferentes funciones del código se utilizan como sigue:\n",
    "    - La función `requests.get` del script anterior se utiliza para obtener los datos del url `DATA_URL`.\n",
    "    - Para manejar el conjunto de datos, se utiliza un `DataFrame` de `pandas`.\n",
    "    \n",
    "- La función `seasonal_decompose` del módulo `stats models` se utiliza para `descomponer la señal de la serie temporal en componentes de tendencia, estacionalidad y residual`. La descomposición puede ser `aditiva o multiplicativa`. Un ejemplo de las diferentes componentes de una señal se muestra en el siguiente script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eabb4243",
   "metadata": {},
   "source": [
    "````{admonition} Observación\n",
    ":class: tip\n",
    "\n",
    "Normalmente, podemos identificar una serie temporal `aditiva`\n",
    "\n",
    "$$y_{t}=f_{t}+s_{t}+\\varepsilon_{t}$$\n",
    "\n",
    "o `multiplicativa`\n",
    "\n",
    "$$y_{t}=f_{t}\\times s_{t}\\times\\varepsilon_{t}$$\n",
    "\n",
    "a partir de su variación. `Si la magnitud del componente estacional cambia con el tiempo, la serie es multiplicativa`. En caso contrario, la serie es aditiva.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3613929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_airpass = pd.date_range(start='1949-01', periods=len(datasets.load_airpassengers()), freq='M').strftime('%Y-%m')\n",
    "air_pass_df = pd.DataFrame({'Date': dates_airpass.tolist(),\n",
    "                            'Passengers': datasets.load_airpassengers().tolist()})\n",
    "air_pass_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_aust = pd.read_csv('datasets/quarterly-beer-production-in-aus-March 1956-June 1994.csv')\n",
    "beer_aust.rename(columns={\"Quarterly beer production in Australia: megalitres. March 1956 ? June 1994\": \"beer_prod\"}, inplace=True)\n",
    "beer_aust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b587fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed = arima.decompose(beer_aust.beer_prod, 'additive', m=4)\n",
    "axes = utils.decomposed_plot(decomposed, figure_kwargs=figure_kwargs, show=False)\n",
    "axes[0].set_title(\"Ausbeer Seasonal Additive Decomposition\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3811970",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed = arima.decompose(datasets.load_airpassengers(), 'multiplicative', m=12)\n",
    "axes = utils.decomposed_plot(decomposed, figure_kwargs=figure_kwargs, show=False)\n",
    "axes[0].set_title(\"Airpassengers Seasonal Multiplicative Decomposition\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c652727",
   "metadata": {},
   "source": [
    "- Las propiedades de estacionalidad aditiva y multiplicativa, se pueden estimar de manera experimental de la siguiente forma: `si la estacionalidad es aditiva, entonces el resultado de la resta entre la serie de tiempo origina y su media movil tendrá fluctuaciones casi similares en magnitud. Si es multiplicativa, entonces el resultado de la división tiene esta propiedad`. Veamos esto en el siguiente ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c060a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingaverage(interval, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return pd.DataFrame(np.convolve(interval, window, 'same'), columns=['MA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b535dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_airpassl = dates_airpass.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c6e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates_airpassl, datasets.load_airpassengers())\n",
    "ax.plot(dates_airpassl, movingaverage(datasets.load_airpassengers(), 12));\n",
    "freq = int(20)\n",
    "ax.set_xticks(dates_airpassl[::freq]);\n",
    "ax.set_xticklabels(dates_airpassl[::freq]);\n",
    "ax.set_xlabel('Date');\n",
    "ax.set_ylabel('Passengers');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b64ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates_airpassl, datasets.load_airpassengers()/movingaverage(datasets.load_airpassengers(), 12).MA.values);\n",
    "freq = int(20)\n",
    "ax.set_xticks(dates_airpassl[::freq]);\n",
    "ax.set_xticklabels(dates_airpassl[::freq]);\n",
    "ax.set_xlabel('Date');\n",
    "ax.set_ylabel('Residual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_beerpl = beer_aust.Quarter.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64efb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates_beerpl, beer_aust.beer_prod)\n",
    "ax.plot(movingaverage(beer_aust.beer_prod, 4));\n",
    "freq = int(19)\n",
    "ax.set_xticks(dates_beerpl[::freq]);\n",
    "ax.set_xticklabels(dates_beerpl[::freq]);\n",
    "ax.set_xlabel('Date');\n",
    "ax.set_ylabel('Beer Production');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates_beerpl, beer_aust.beer_prod-movingaverage(beer_aust.beer_prod, 4).MA.values);\n",
    "freq = int(19)\n",
    "ax.set_xticks(dates_beerpl[::freq]);\n",
    "ax.set_xticklabels(dates_beerpl[::freq]);\n",
    "ax.set_xlabel('Date');\n",
    "ax.set_ylabel('Residual');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d992a48",
   "metadata": {},
   "source": [
    "- Nótese que si calculamos el `residuo para una serie temporal con estacionalidad multiplicativa`, la serie resultante contiene componentes estacionales cuya `varianza aumenta cada periodo de frecuencia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f961b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates_airpassl, datasets.load_airpassengers()-movingaverage(datasets.load_airpassengers(), 12).MA.values);\n",
    "freq = int(20)\n",
    "ax.set_xticks(dates_airpassl[::freq]);\n",
    "ax.set_xticklabels(dates_airpassl[::freq]);\n",
    "ax.set_xlabel('Date');\n",
    "ax.set_ylabel('Residual');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e11f910",
   "metadata": {},
   "source": [
    "- Una alternativa para verificar estacionalidad sin usar visualziaciones, es utilizar una `prueba no paramétrica` (ver [Kruskal-Wallis test](https://jdemetradocumentation.github.io/JDemetra-documentation/pages/theory/Tests_KW.html)). Este test comprueba la `correlación entre la observación real y la observación retardada`. `Si las observaciones son independientes entre sí, siguen una distribución` $\\chi^2$. `Cuando se rechaza esta hipótesis, se confirma una autocorrelación significativa, que es señal de movimientos estacionales en la serie`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "def seasonality_test(series, period):\n",
    "        seasonal = False\n",
    "        idx = np.arange(len(series)) % period\n",
    "        H_statistic, p_value = kruskal(series, idx)\n",
    "        if p_value <= 0.05:\n",
    "            seasonal = True\n",
    "        return seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is the Air Passengers time series seasonal?:\", seasonality_test(dates_airpassl, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352771c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is the Beer production time series seasonal?:\", seasonality_test(beer_aust.beer_prod, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97ff9f86",
   "metadata": {},
   "source": [
    "- La señal de la serie temporal anterior está compuesta por la tendencia, la estacionalidad y el residuo (ruido blanco). `El suavizado ayuda a eliminar el componente residual y capta los componentes de tendencia y estacionalidad para la predicción de la señal`.\n",
    "\n",
    "- `El primer paso del modelo para incluir la media, la tendencia y los patrones no estacionales es extrapolarla mediante la suavización`. La suavización básica mediante la media móvil se discutió en el capítulo anterior. El suavizado de media móvil evalúa las esperanzas $\\textsf{E}(y_{t})$ utilizando todas las observaciones anteriores, como sigue:\n",
    "\n",
    "$$\\hat{y}_{t}=\\frac{1}{N}\\sum_{i=1}^{T}y_{t-i}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a66bbb38",
   "metadata": {},
   "source": [
    "- Por lo general, `la media móvil simple se realiza sobre ventanas de predicción`; así, la predicción estimada se evalua en ventanas óptimas con el objetivo de minimizar la función de error\n",
    "\n",
    "$$\\min\\left(\\sum_{i=1}^{T}(y_{t}-\\hat{y}_{t})^{2}\\right)=\\min\\sum_{i=1}^{T}\\left(y_{t}-\\frac{1}{N}\\sum_{i=1}^{T}y_{t-i}\\right)^{2}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac608571",
   "metadata": {},
   "source": [
    "- Los métodos de suavización se basan en el `supuesto de que los datos de las series temporales son localmente estacionarios con pequeñas variaciones en la media`. Así, podemos utilizar la media en el momento $t$ para predecir en $t+1$ cuando el $\\Delta t$ es lo suficientemente pequeño como para mantener la señal estacionaria. Este modelo es un compromiso entre la media y el modelo de paseo aleatorio sin desviación. \n",
    "\n",
    "- Los modelos también se denominan modelos de suavización, ya que `suavizan choques del conjunto de datos`. La principal limitación de estos métodos basados en la media móvil es que tratan todas las $n$ muestras utilizadas en el suavizado dando mayor peso a las observaciones recientes, como se muestra en la siguiente ecuación:\n",
    "\n",
    "$$\\hat{y}_{t}=\\frac{1}{N}\\sum_{i=1}^{T}w_{i}y_{t-i}.$$\n",
    "\n",
    "- Aquí $w_{1}>w_{2}>\\cdots>w_{T}$ y $T$ es la `longitud de la ventana`. Esto supone otro reto para la evaluación de las ponderaciones. La limitación de la media móvil y la media móvil ponderada se aborda mediante la `suavización exponencial aplicando ponderaciones con decaimiento exponencial en las observaciones`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1fbe252",
   "metadata": {},
   "source": [
    "- Tomemos ahora como ejemplo el `precio de cierre de las acciones de IBM` para pronosticarlo utilizando el método de suavizado simple. El primer paso es cargar los módulos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958ab694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_df = pd.read_csv('datasets/ibm-common-stock-closing-prices.csv')\n",
    "ibm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e45101",
   "metadata": {},
   "source": [
    "- Por comodidad, cambiaremos el nombre de las columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41863c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_df.rename(columns={'IBM common stock closing prices': 'Close_Price'}, inplace=True)\n",
    "ibm_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "149175c9",
   "metadata": {},
   "source": [
    "- En el método de `suavización exponencial simple`, los valores predichos se generan como sigue usando Ec. {eq}`eq:simple_exp_smoothing3`. Esta serie se puede implementar en `Python` de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_exp_smoothing(x, alpha):\n",
    "    F = [x[0]]\n",
    "    for t in range(1, len(x)):\n",
    "        F.append(alpha * x[t] + (1 - alpha) * F[t-1])\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666cd89",
   "metadata": {},
   "source": [
    "- La función `single_exp_smoothing` configurada con el valor inicial previsto se asigna como el primer valor de la serie. Evaluemos primero los casos extremos de previsión con $α = 0$ y $α = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c58afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_df['SES0'] = single_exp_smoothing(ibm_df['Close_Price'], 0)\n",
    "ibm_df['SES1'] = single_exp_smoothing(ibm_df['Close_Price'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, sharex=True)\n",
    "f.set_size_inches(10, 5)\n",
    "\n",
    "\n",
    "ibm_df['Close_Price'].iloc[:45].plot(color='b', linestyle = '-', ax=axarr[0])\n",
    "ibm_df['SES0'].iloc[:45].plot(color='r', linestyle = '--', ax=axarr[0])\n",
    "axarr[0].set_title(r'Suavización Exponencial Simple $\\alpha=0$');\n",
    "\n",
    "ibm_df['Close_Price'].iloc[:45].plot(color='b', linestyle = '-', ax=axarr[1])\n",
    "ibm_df['SES1'].iloc[:45].plot(color='r', linestyle = '--', ax=axarr[1])\n",
    "axarr[1].set_title(r'Suavización Exponencial Simple $\\alpha=1$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480678f5",
   "metadata": {},
   "source": [
    "- La figura anterior ilustra que, para $\\alpha=0$, la predicción es una constante, y para $\\alpha=1$, la serie pronosticada se desplaza un desfase temporal. El pronóstico de alisamiento único para valores de suavización $0.2, 0.6, 0.8$ puede evaluarse como sigue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623fa206",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_df['SES2'] = single_exp_smoothing(ibm_df['Close_Price'], 0.2)\n",
    "ibm_df['SES6'] = single_exp_smoothing(ibm_df['Close_Price'], 0.6)\n",
    "ibm_df['SES8'] = single_exp_smoothing(ibm_df['Close_Price'], 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24849bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 3, sharex=True)\n",
    "f.set_size_inches(15, 5)\n",
    "\n",
    "ibm_df['Close_Price'].iloc[:45].plot(color='b', linestyle = '-', ax=axarr[0])\n",
    "ibm_df['SES2'].iloc[:45].plot(color='r', linestyle = '--', ax=axarr[0])\n",
    "axarr[0].set_title(r'Suavización Exponencial Simple $\\alpha=0.2$');\n",
    "\n",
    "ibm_df['Close_Price'].iloc[:45].plot(color='b', linestyle = '-', ax=axarr[1])\n",
    "ibm_df['SES6'].iloc[:45].plot(color='r', linestyle = '--', ax=axarr[1])\n",
    "axarr[1].set_title(r'Suavización Exponencial Simple $\\alpha=0.6$');\n",
    "\n",
    "ibm_df['Close_Price'].iloc[:45].plot(color='b', linestyle = '-', ax=axarr[2])\n",
    "ibm_df['SES8'].iloc[:45].plot(color='r', linestyle = '--', ax=axarr[2])\n",
    "axarr[2].set_title(r'Suavización Exponencial Simple $\\alpha=0.8$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95638d61",
   "metadata": {},
   "source": [
    "- La figura anterior ilustra que $\\alpha$ tiene un gran impacto en la previsión; por lo tanto, obtener el valor correcto de $\\alpha$ es fundamental a la hora de establecer la previsión"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa7fe52b",
   "metadata": {},
   "source": [
    "- La precisión del modelo puede evaluarse en una muestra retenida utilizando funciones objetivo estándar como el `error cuadrático medio (ECM)` o `el error absoluto medio (EAM)`\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MSE}&=\\frac{1}{N}\\sum_{i=1}^{N}(y_{t}-\\tilde{y}_{t})^{2}\\\\\n",
    "\\text{MAD}&=\\frac{1}{N}\\sum_{i=1}^{N}|y_{t}-\\tilde{y}_{t}|\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Como el suavizado ayuda a reducir la varianza del conjunto de datos, reducirá la varianza de serie pronosticada entre cero y la varianza real del conjunto de datos\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Var}(\\tilde{y}_{T})&=\\text{Var}\\left(\\alpha\\sum_{i=0}^{\\infty}(1-\\alpha)^{i}y_{T-i}\\right)\\\\\n",
    "&=\\frac{\\alpha}{2-\\alpha}\\text{Var}(y_{T})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Aquí, $T$ es la longitud de la serie temporal. Para una varianza unitaria de la serie $y_{T}$, la varianza captada por la serie pronosticada variará en función del parámetro de suavización α"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f850ed",
   "metadata": {},
   "source": [
    "## Suavización exponencial de segundo orden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3aaa092",
   "metadata": {},
   "source": [
    "- Si la suavización exponencial de primer orden no entrega la mejor bondad de ajuste, entonces `existe tendencia en los datos de la serie temporal`. La tendencia se observa habitualmente en muchos ámbitos, como cuando en las empresas de comercio electrónico o de marketing existe un `aumento de las ventas` o cuando cualquier `buen rendimiento anual de una empresa tendrá un efecto alcista en el precio de sus acciones`. La `tendencia lineal` puede ocurrir entre el tiempo y la respuesta:\n",
    "\n",
    "    $$\n",
    "    y_{t}=\\omega_{0}+\\omega_{1}t+\\varepsilon_{t}\n",
    "    $$\n",
    "    donde, $\\omega$ es el coeficiente que conduce a la tendencia. \n",
    "\n",
    "- El `suavizado exponencial de segundo orden` ayuda a `capturar la tendencia en los datos de serie temporal` incluyendo otro término en el suavizado exponencial de primer orden como sigue\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F_{t+1}&=\\alpha y_{t}+(1-\\alpha)(F_{t-1}+T_{t-1})\\\\\n",
    "T_{t+1}&=\\beta(F_{t}-F_{t-1})+(1-\\beta)T_{t-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Aquí $\\alpha$ es el `factor de suavización` de los datos y $\\beta$ es el `factor de suavización de la tendencia` con valores en el intervalo $[0, 1]$. La predicción en el tiempo posterior $t+1$ puede generarse como sigue\n",
    "\n",
    "$$\n",
    "\\hat{y}_{t+1}=F_{t}+T_{t}\n",
    "$$\n",
    "\n",
    "- En la `suavización de segundo orden`, el `valor inicial del componente de tendencia` puede asignarse de múltiples maneras:\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}T_{1}&=y_{2}-y_{1}\\\\\n",
    "    T_{1}&=(y_{n}-y_{1})/(n-1)\n",
    "    \\end{align*}\n",
    "    $$\n",
    "    donde, $n$ es el número de observaciones. \n",
    "    \n",
    "- Estudiemos un ejemplo donde usemos `suavización de segundo orden`. Utilizaremos datos de `producción de cerveza` estudiados anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6335434",
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df = pd.read_csv('datasets/quarterly-beer-production-in-aus-March 1956-June 1994.csv')\n",
    "print('Dimensión del dataframe:', beer_df.shape)\n",
    "\n",
    "beer_df.rename(columns={'Quarterly beer production in Australia: megalitres. March 1956 ? June 1994':'Beer_Prod'},\n",
    "               inplace=True)\n",
    "    \n",
    "missing = (pd.isnull(beer_df['Quarter'])) | (pd.isnull(beer_df['Beer_Prod']))\n",
    "print('Número de filas con al menos un valor igual a cero:', missing.sum())\n",
    "beer_df = beer_df.loc[~missing, :]\n",
    "\n",
    "print('Dimensiones después de eliminar los valores perdidos:', beer_df.shape)\n",
    "beer_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6b3d99c",
   "metadata": {},
   "source": [
    "- Creamos la función para el `suavizado exponencial doble`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c386c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_exp_smoothing(x, alpha, beta):\n",
    "    yhat = [x[0]] # first value is same as series\n",
    "    for t in range(1, len(x)):\n",
    "        if t==1:\n",
    "            F, T= x[0], x[1] - x[0]\n",
    "        F_n_1, F = F, alpha*x[t] + (1-alpha)*(F+T)\n",
    "        T=beta*(F-F_n_1)+(1-beta)*T\n",
    "        yhat.append(F+T)\n",
    "    return yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02c0196c",
   "metadata": {},
   "source": [
    "- La función anterior toma como entrada la serie temporal `x` con `alpha` y `beta`. La implementación anterior utiliza la `diferencia de las dos primeras ocurrencias para establecer el valor de la tendencia inicial`. Vamos a evaluar el rendimiento en los casos límite, es decir, `(alpha, beta)`$\\in\\{(0, 0), (0, 1), (1, 0), (1, 1)\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ceb94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df['DEF00'] = double_exp_smoothing(beer_df['Beer_Prod'], 0, 0)\n",
    "beer_df['DEF01'] = double_exp_smoothing(beer_df['Beer_Prod'], 0, 1)\n",
    "beer_df['DEF10'] = double_exp_smoothing(beer_df['Beer_Prod'], 1, 0)\n",
    "beer_df['DEF11'] = double_exp_smoothing(beer_df['Beer_Prod'], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073833ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(2, 2, sharex=True)\n",
    "f.set_size_inches(10, 10)\n",
    "\n",
    "beer_df['Beer_Prod'].iloc[:45].plot(color='b', linestyle = '-', ax=axarr[0, 0])\n",
    "beer_df['DEF00'].iloc[:45].plot(color='r', linestyle = '--', ax=axarr[0, 0])\n",
    "axarr[0, 0].set_title(r'Suavización Exponencial Doble $\\alpha=0,\\,\\beta=0$')\n",
    "\n",
    "beer_df['Beer_Prod'].iloc[:45].plot(color='b', linestyle = '-', ax=axarr[0, 1])\n",
    "beer_df['DEF01'].iloc[:45].plot(color='r', linestyle = '--', ax=axarr[0, 1])\n",
    "axarr[0, 1].set_title(r'Suavización Exponencial Doble $\\alpha=0,\\,\\beta=1$')\n",
    "\n",
    "beer_df['Beer_Prod'].iloc[:45].plot(color='b', linestyle = '-', ax=axarr[1, 0])\n",
    "beer_df['DEF10'].iloc[:45].plot(color='r', linestyle = '--', ax=axarr[1, 0])\n",
    "axarr[1, 0].set_title(r'Suavización Exponencial Doble $\\alpha=1,\\,\\beta=0$');\n",
    "\n",
    "beer_df['Beer_Prod'].iloc[:45].plot(color='b', linestyle = '-', ax=axarr[1, 1])\n",
    "beer_df['DEF11'].iloc[:45].plot(color='r', linestyle = '--', ax=axarr[1, 1])\n",
    "axarr[1, 1].set_title(r'Suavización Exponencial Doble $\\alpha=1,\\,\\beta=1$');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a871c50",
   "metadata": {},
   "source": [
    "- Cuando `alpha=0`, los `valores iniciales permanecen constantes`; por lo tanto, `el parámetro de tendencia no desempeña ningún papel`. Sin embargo, cuando `alpha = 1 y beta = 0`, la `suavización exponencial de segundo orden` puede escribirse de la siguiente manera\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F_{t}&=y_{t-1}\\\\\n",
    "T_{t}&=T_{t-1}\\\\\n",
    "\\hat{y}_{t}&=y_{t-1}+T_{t-1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- `La predicción en el tiempo` $t$ `depende del valor anterior y de los componentes de tendencia`. Como `beta se fija en cero`, el componente de tendencia en $t-1$ dependerá de $t-2$, el de $t-2$ de $t-3$ y así sucesivamente\n",
    "\n",
    "$$\n",
    "T_{t}=T_{t-1}=T_{t-2}=T_{t-3}=\\cdots=T_{0}\n",
    "$$\n",
    "\n",
    "- Así, `el valor del componente de tendencia depende del valor inicial asignado y es una constante`. Del mismo modo, para `alpha = 1 y beta = 1`, la `suavización exponencial de segundo orden` puede simplificarse como sigue\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F_{t}&=y_{t-1}\\\\\n",
    "T_{t}&=(F_{t-1}-F_{t-2})=y_{t-2}-y_{t-3}\\\\\n",
    "\\hat{y}_{t}&=y_{t-1}+(y_{t-2}-y_{t-3})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- Con `alpha = 1 y beta = 1`, la diferencia de las observaciones en los tiempos $t-2$ y $t-3$ se añaden al valor de predicción en el tiempo $t$ en comparación con la configuración de `alpha = 1 y beta = 0`, que da el desplazamiento a la predicción y `se acerca más a la previsión real`\n",
    "\n",
    "- Realizamos a continuación un `suavizado exponencial doble` con los datos de producción de cerveza `utilizando valores intermedios de alpha y beta` de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c86dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df['DEF'] = double_exp_smoothing(beer_df['Beer_Prod'], 0.4, 0.7)\n",
    "beer_df['Single_Exponential_Forecast'] = single_exp_smoothing(beer_df['Beer_Prod'], 0.4)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(5, 5)\n",
    "beer_df['Beer_Prod'].iloc[:45].plot(color='b', linestyle = '-', label = 'Beer Prod.')\n",
    "beer_df['DEF'].iloc[:45].plot(color='r', linestyle = '--', label = r'DES $\\alpha=0.4,\\,\\beta=0.7$')\n",
    "beer_df['Single_Exponential_Forecast'].iloc[:45].plot(color='k', linestyle = '-.', label = r'SES $\\alpha=0.4$')\n",
    "leg = ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1a00468",
   "metadata": {},
   "source": [
    "- La figura anterior muestra que `el suavizado exponencial doble es capaz de capturar mejor la variación de la señal real para el conjunto de datos actual en comparación con el suavizado exponencial simple`. Sin embargo, en los escenarios en los que el componente de tendencia tienden a cero, el rendimiento del suavizado exponencial simple y doble es comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b84c1",
   "metadata": {},
   "source": [
    "## Suavización exponencial de orden superior"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd2fa32d",
   "metadata": {},
   "source": [
    "- El concepto puede extenderse a la `suavización exponencial de orden superior` con un modelo polinómico de $n^{\\text{th}}$ orden\n",
    "\n",
    "$$\n",
    "y_{t}=\\alpha_{0}+\\alpha_{1}t+\\frac{\\alpha_{2}}{2!}t^{2}+\\cdots+\\frac{\\alpha_{n}}{n!}t^{n}+\\varepsilon_{t}\n",
    "$$\n",
    "\n",
    "- Aquí, el error $\\varepsilon_{t}\\sim N(0,\\sigma^{2})$ se distribuye normalmente con media $0$ y varianza $\\sigma^{2}$. Los suavizadores exponenciales utilizados para el orden superior son los siguientes\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde{x}_{t}^{(1)}&=k y_{t}+(1-k)\\tilde{x}_{t-1}^{(1)}\\\\\n",
    "\\tilde{x}_{t}^{(2)}&=k \\tilde{x}_{t}^{(1)}+(1-k)\\tilde{x}_{t-1}^{(2)}\\\\\n",
    "&\\vdots\\\\\n",
    "\\tilde{x}_{t}^{(n)}&=k \\tilde{x}_{t}^{(n-1)}+(1-k)\\tilde{x}_{t-1}^{(n)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- En este caso, se trata de pesos para los suavizadores. `Normalmente, la suavización exponencial de orden superior no se utiliza, el cálculo es muy difícil`, mas bien, se utilizan modelos `Autorregrsivos Integrados de Media Movil(ARIMA)` los cuales se tratarán con más detalle en la siguiente sección."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89e4bf68",
   "metadata": {},
   "source": [
    "- Otro suavizado exponencial muy popular es el `suavizado exponencial triple  (Holt Winters)`. El `suavizado exponencial triple` permite capturar la `estacionalidad con el nivel (valor suavizado de la parte constante en el tiempo) y la tendencia`. La relación entre: `niveles, tendencias y la estacionalidad` se define mediante el siguiente conjunto de ecuaciones\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "F_{t}&=\\alpha(y_{t}-S_{t-L})+(1-\\alpha)(F_{t-1}+T_{t-1})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- En estas ecuaciones, $F_{t}$ `capta los niveles de observación ` en el momento $t$. Asimismo, $T_{t}$ y $S_{t}$ capturan `tendencia y estacionalidad` en el tiempo $t$. Los coeficientes $\\alpha, \\beta$ `representan: factor de suavización de los datos, factor de suavización de la tendencia y factor de suavización de la estacionalidad`, respectivamente con valores en el intervalo $[0,1]$. Estas ecuaciones pueden utilizarse para pronosticar el siguiente período de tiempo de la siguiente manera\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "T_{t}&=\\beta(F_{t}-F_{t-1})+(1-\\beta)T_{t-1}\\\\\n",
    "S_{t}&=\\gamma(y_{t}-F_{t})+(1-\\gamma)S_{t-C}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "- El término $F_{t}$ `recoge el desfase de los componentes estacionales con respecto a la última tendencia estacional observada`. Usemos `suavización exponencial triple para los datos de empleo de Wisconsin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2905537",
   "metadata": {},
   "outputs": [],
   "source": [
    "wisc_emp = pd.read_csv('datasets/wisconsin-employment-time-series.csv')\n",
    "print('Shape of the DataFrame:', wisc_emp.shape)\n",
    "wisc_emp.head();\n",
    "wisc_emp.plot();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ff9b4d8",
   "metadata": {},
   "source": [
    "- La figura anterior muestra el conjunto de datos de series temporales, `Employment` de `Wisconsin`. El conjunto de datos consta de `tendencia anual y estacionalidad mensual`. Dado que el patrón de estacionalidad de los datos es conocido, la información sobre la estacionalidad puede utilizarse para `derivar el valor inicial de la tendencia como el valor medio de las estaciones` utilizando la siguiente ecuación\n",
    "\n",
    "$$\n",
    "T_{0}=\\frac{1}{C}\\left(\\frac{y_{C+1}-y_{1}}{C}+\\frac{y_{C+2}-y_{2}}{C}+\\frac{y_{C+3}-y_{3}}{C}+\\cdots\\right)\n",
    "$$\n",
    "\n",
    "- La ecuación anterior puede implementarse en `Python` como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9230265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_T(x, seasonLength):\n",
    "    total=0.0\n",
    "    for i in range(seasonLength):\n",
    "        total+=float(x[i+seasonLength]-x[i])/seasonLength\n",
    "    return total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a563305",
   "metadata": {},
   "source": [
    "- Por ejemplo, el valor de `tendencia inicial` generado por la función anterior es $1.69$ utilizando el script `initialize_T()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_T(wisc_emp['Employment'], 12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc703f8e",
   "metadata": {},
   "source": [
    "- La `estacionalidad inicial` es de suma importancia y se puede calcular con la siguiente función. Los valores iniciales de las estaciones se calculan como valor medio de la respuesta $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_seasonalilty(x, seasonLength):\n",
    "    seasons={}\n",
    "    seasonsMean=[]\n",
    "    num_season=int(len(x)/seasonLength)\n",
    "\n",
    "    for i in range(num_season):\n",
    "        seasonsMean.append(sum(x[seasonLength*i:seasonLength*i+seasonLength])/float(seasonLength))\n",
    "    \n",
    "    for i in range(seasonLength):\n",
    "        tot=0.0\n",
    "        for j in range(num_season):\n",
    "            tot+=x[seasonLength*j+i]-seasonsMean[j]\n",
    "        seasons[i]=tot/num_season\n",
    "    return seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_seasonalilty(wisc_emp['Employment'], 12)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "029554af",
   "metadata": {},
   "source": [
    "- Una vez obtenidos los valores, estamos listos para establecer la predicción utilizando `suavización exponencial triple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac535e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triple_exp_smoothing(x, seasonLength, alpha, beta, gamma, h):\n",
    "    yhat=[]\n",
    "    S = initialize_seasonalilty(x, seasonLength)\n",
    "    for i in range(len(x)+h):\n",
    "        if i == 0:\n",
    "            F = x[0]\n",
    "            T = initialize_T(x, seasonLength)\n",
    "            yhat.append(x[0])\n",
    "            continue\n",
    "        if i >= len(x):\n",
    "            m = i - len(x) + 1\n",
    "            yhat.append((F + m*T) + S[i%seasonLength])\n",
    "        else:\n",
    "            obsval = x[i]\n",
    "            F_last, F= F, alpha*(obsval-S[i%seasonLength]) + (1-alpha)*(F+T)\n",
    "            T = beta * (F-F_last) + (1-beta)*T\n",
    "            S[i%seasonLength] = gamma*(obsval-F) + (1-gamma)*S[i%seasonLength]\n",
    "            yhat.append(F+T+S[i%seasonLength])\n",
    "    return yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84fb1a8d",
   "metadata": {},
   "source": [
    "- La `suavización exponencial triple` está controlada por `alpha, beta y gamma`. La presencia o ausencia de cualquier escenario tendrá un efecto drástico en el resultado. Hagamos una comparación empírica para diferentes escenarios extremos. Vamos a `ejecutar el suavizado exponencial con parámetros intermedios`, como se muestra en la siguiente figura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76664cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wisc_emp['TES']=triple_exp_smoothing(wisc_emp['Employment'], 12, 0.4, 0.6, 0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "wisc_emp['Employment'].plot(ax=ax, color='b', linestyle = '-', label = 'Employment')\n",
    "wisc_emp['TES'].plot(ax=ax, color='r', linestyle = '--', \n",
    "                     label = r'TES: $L=12,\\,\\alpha=0.4,\\,\\beta=0.6,\\,\\gamma=0.2,\\,h=0$')\n",
    "leg = ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e00958aa",
   "metadata": {},
   "source": [
    "- Si comparamos este ajuste con las `suavizaciones de primer y segundo orden obtenemos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wisc_emp['DES'] = double_exp_smoothing(wisc_emp['Employment'], 0.4, 0.6)\n",
    "wisc_emp['SES'] = single_exp_smoothing(wisc_emp['Employment'], 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d67ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "wisc_emp['Employment'].iloc[:45].plot(ax=ax, color='b', linestyle = '-', label = 'Employment')\n",
    "wisc_emp['TES'].iloc[:45].plot(ax=ax, color='r', linestyle = '--', \n",
    "                     label = r'TES: $L=12,\\,\\alpha=0.4,\\,\\beta=0.6,\\,\\gamma=0.2,\\,h=0$')\n",
    "wisc_emp['DES'].iloc[:45].plot(ax=ax, color='g', linestyle = '--', label = r'DES: $\\alpha=0.4,\\,\\beta=0.6$')\n",
    "wisc_emp['SES'].iloc[:45].plot(ax=ax, color='k', linestyle = '--', label = r'SES: $\\alpha=0.4$')\n",
    "leg = ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30e87e06",
   "metadata": {},
   "source": [
    "- Según los estudios numéricos, el nivel único que utiliza el suavizado o la estacionalidad es capaz de capturar tendencia en los datos; por tanto, todos los modelos funcionaron bien, ya que `el suavizado exponencial simple y el doble fueron capaces de utilizar el factor de suavización para hacer las predicciones` y la `suavización exponencial triple es capaz de capturas las predicciones utilizando los factores de suavización o estacionalidad`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06c9c249",
   "metadata": {},
   "source": [
    "````{admonition} Resumen\n",
    ":class: tip\n",
    "\n",
    "- Este capítulo trata de los enfoques de suavización exponencial para datos de series temporales. Los enfoques pueden ampliarse fácilmente para la predicción mediante la inclusión de términos como el `factor de suavización, factor de tendencia y factor de estacionalidad`. \n",
    "\n",
    "- El suavizado exponencial de orden único realiza la suavización utilizando solo el factor $\\lambda$, que se amplía con los factores $\\alpha,\\,\\beta$ de segundo orden al incluir tendencia. `La suavización de tercer orden incorpora todos los factores, tendencia y estacionalidad en el modelo`.\n",
    "\n",
    "- En este capítulo se han tratado todos estos modelos en detalle con su implementación en `Python`. El enfoque de `suavizado exponencial puede utilizarse para pronosticar series temporales estacionarias`. Sin embargo, esta suposición puede no ser cierta. `Se recomienda el suavizado exponencial de orden superior para estos casos`, pero su cálculo es difícil y costoso computacionalmente. Por lo tanto, para hacer frente a este planteamiento, `se utilizan otras técnicas de previsión como el modelo ARIMA, que se tratará en la siguiente sección`.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7170093-f9e9-4431-b088-cf70463c4d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01135a2-75ab-455f-b5d4-b2f9aaca3785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_tf",
   "language": "python",
   "name": "ml_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
