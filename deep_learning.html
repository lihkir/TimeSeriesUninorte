
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Deep Learning para la predicción de series temporales &#8212; Análisis y Predicción de Series de Tiempo</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'deep_learning';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Bibliografía" href="biblio.html" />
    <link rel="prev" title="Modelos autorregresivos integrados de media móvil" href="arima_model.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo-uninorte.png" class="logo__image only-light" alt="Análisis y Predicción de Series de Tiempo - Home"/>
    <script>document.write(`<img src="_static/logo-uninorte.png" class="logo__image only-dark" alt="Análisis y Predicción de Series de Tiempo - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Análisis y Predicción de Series de Tiempo
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="statistical_background.html">Introducción a las Series de Tiempo</a></li>

<li class="toctree-l1"><a class="reference internal" href="exponential_smoothing.html">Métodos de Suavización Exponencial</a></li>

<li class="toctree-l1"><a class="reference internal" href="arima_model.html">Modelos autorregresivos integrados de media móvil</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Deep Learning para la predicción de series temporales</a></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fdeep_learning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/deep_learning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deep Learning para la predicción de series temporales</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales">Redes neuronales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradiente-descendiente">Gradiente descendiente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-perceptron">El perceptrón</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-totalmente-conectadas">Redes Totalmente Conectadas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-algoritmo-de-backpropagation">El Algoritmo De Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-esquema-de-backpropagation-para-gradiente-descendiente">El Esquema De Backpropagation Para Gradiente Descendiente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-gradientes">Cálculo de gradientes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-delta-nj-r">Cálculo de <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#las-capas-ocultas">Las capas ocultas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-recurrentes">Redes Neuronales Recurrentes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-en-tiempo">Backpropagation en tiempo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#desvanecimiento-y-explosion-de-gradientes">Desvanecimiento y explosión de gradientes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#red-de-memoria-a-largo-plazo-lstm">Red de memoria a largo plazo (LSTM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#series-de-tiempo">Series de Tiempo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptrones-multicapa">Perceptrones multicapa</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entrenamiento-de-mlp">Entrenamiento de MLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp-para-la-prediccion-de-series-temporales">MLP para la predicción de series temporales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-para-la-prediccion-de-series-de-tiempo">LSTM para la predicción de series de tiempo</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deep-learning-para-la-prediccion-de-series-temporales">
<h1>Deep Learning para la predicción de series temporales<a class="headerlink" href="#deep-learning-para-la-prediccion-de-series-temporales" title="Link to this heading">#</a></h1>
<section id="redes-neuronales">
<h2>Redes neuronales<a class="headerlink" href="#redes-neuronales" title="Link to this heading">#</a></h2>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Las redes neuronales son <em><strong>sistemas de aprendizaje compuestos por neuronas conectadas en capas que ajustan sus conexiones para aprender</strong></em>. Tras un período de <em><strong>25 años desde su inicio, las redes neuronales se convirtieron en la norma en el aprendizaje automático</strong></em>.</p></li>
<li><p>En un principio, dominaron durante una década, pero <em><strong>luego fueron superadas por máquinas de vectores de soporte</strong></em>. Sin embargo, <em><strong>desde 2010, las redes neuronales profundas se han vuelto populares gracias a mejoras en la tecnología y la disponibilidad de grandes conjuntos de datos</strong></em>, impulsando el campo del aprendizaje automático.</p></li>
</ul>
</div>
</section>
<section id="gradiente-descendiente">
<h2>Gradiente descendiente<a class="headerlink" href="#gradiente-descendiente" title="Link to this heading">#</a></h2>
<ul>
<li><p>El <em><strong>método de gradiente descendiente</strong></em> es uno de los mas ampliamente usados para la <em><strong>minimización iterativa de una función de costo diferenciable</strong></em>, <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta}),~\boldsymbol{\theta}\in\mathbb{R}^{l}\)</span>. Como cualquier otra técnica iterativa, el método <em><strong>parte de una estimación inicial</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span>, <em><strong>y genera una sucesión</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)},~i=1,2,\dots,\)</span> tal que:</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)},~ i &gt;0,~\mu_{i}&gt;0.
    \]</div>
</li>
<li><p>La diferencia entre cada método radica en la forma que <span class="math notranslate nohighlight">\(\mu_{i}\)</span> y <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> son seleccionados. <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> es conocido como la <em><strong>dirección de actualización o de búsqueda</strong></em>. La sucesión <span class="math notranslate nohighlight">\(\mu_{i}\)</span> es conocida como el <em><strong>tamaño o longitud de paso</strong></em> en la <span class="math notranslate nohighlight">\(i\)</span>-ésima iteración, estos valores pueden ser constantes o cambiar. En el método de gradiente descendiente, <em><strong>la selección de</strong></em> <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> <em><strong>es realizada para garantizar que</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta}^{(i)})&lt;J(\boldsymbol{\theta}^{(i-1)})\)</span>, excepto en el minimizador <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\star}\)</span>.</p></li>
</ul>
<figure class="align-center" id="curva-nivel">
<a class="reference internal image-reference" href="_images/curva_nivel.png"><img alt="_images/curva_nivel.png" src="_images/curva_nivel.png" style="width: 504.8px; height: 386.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Función de coste en el espacio de parámetros bidimensional.</span><a class="headerlink" href="#curva-nivel" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Suponga que en la iteración <span class="math notranslate nohighlight">\(i-1\)</span> el valor <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span> <em><strong>ha sido obtenido</strong></em></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta}^{(i)})=J(\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)})\approx J(\boldsymbol{\theta}^{(i-1)})+\mu_{i}\cdot\nabla^{T}J(\boldsymbol{\theta}^{(i-1)})\Delta\boldsymbol{\theta}^{(i-1)}.
\]</div>
<ul class="simple">
<li><p>Nótese que <em><strong>seleccionando la dirección tal que</strong></em> <span class="math notranslate nohighlight">\(\nabla^{T}J(\boldsymbol{\theta}^{(i-1)})\Delta\boldsymbol{\theta}^{(i)}&lt;0\)</span>, <em><strong>garantizará que</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)})&lt;J(\boldsymbol{\theta}^{(i-1)})\)</span>. Tal selección de <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> y <span class="math notranslate nohighlight">\(\nabla J(\boldsymbol{\theta}^{(i-1)})\)</span> debe formar un <em><strong>ángulo obtuso</strong></em>. Las curvas de nivel asociadas a <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span> pueden tomar cualquier forma, la cual va a <em><strong>depender de como está definido</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span> se supone diferenciable, por lo tanto, las <em><strong>curvas de nivel o contornos deben ser suaves y aceptar un plano tangente en cualquier punto</strong></em>. Además, de los cursos de cálculo sabemos que el <em><strong>vector gradiente</strong></em> <span class="math notranslate nohighlight">\(\nabla J(\boldsymbol{\theta})\)</span> <em><strong>es perpendicular al plano tangente</strong></em> (recta tangente) <em><strong>a la correspondiente curva de nivel en el punto</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. Nótese que <em><strong>seleccionando la dirección de búsqueda</strong></em> <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}^{(i)}\)</span> <em><strong>que forma un angulo obtuso con el gradiente, se coloca a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}+\mu_{i}\Delta\boldsymbol{\theta}^{(i)}\)</span> <em><strong>en un punto sobre el contorno el cual corresponde a un valor menor que</strong></em> <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>Dos problemas surgen ahora:</p>
<ol class="arabic simple">
<li><p><em><strong>Escoger la mejor dirección de búsqueda</strong></em></p></li>
<li><p>Calcular <em><strong>que tan lejos es aceptable un movimiento a traves de esta dirección</strong></em>.</p></li>
</ol>
</li>
</ul>
<figure class="align-center" id="maximun-dec-cost-function">
<a class="reference internal image-reference" href="_images/maximun_dec_cost_function.png"><img alt="_images/maximun_dec_cost_function.png" src="_images/maximun_dec_cost_function.png" style="width: 495.20000000000005px; height: 365.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">El vector gradiente en un punto <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> es perpendicular al plano tangente (línea punteada) en la curva de nivel que cruza <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. La dirección de descenso forma un ángulo obtuso, <span class="math notranslate nohighlight">\(\phi\)</span>, con el vector gradiente.</span><a class="headerlink" href="#maximun-dec-cost-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Nótese que <em><strong>si</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}\|\Delta\boldsymbol{\theta}^{(i)}\|\)</span> <em><strong>es demasiado grande, entonces el nuevo punto puede ser colocado en un contorno correspondiente a un valor mayor al del actual</strong></em> contorno.</p></li>
</ul>
<figure class="align-center" id="curva-nivel-cost-function">
<a class="reference internal image-reference" href="_images/curva_nivel_cost_function.png"><img alt="_images/curva_nivel_cost_function.png" src="_images/curva_nivel_cost_function.png" style="width: 484.2px; height: 359.1px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">Las correspondientes curvas de nivel para la función de coste, en el plano bidimensional. Nótese que a medida que nos alejamos del valor óptimo, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\star}\)</span>, los valores de <span class="math notranslate nohighlight">\(c\)</span> aumentan.</span><a class="headerlink" href="#curva-nivel-cost-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Para abordar (1), <em><strong>supongamos que</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}=1\)</span> y <em><strong>buscamos todos los vectores</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> <em><strong>con norma Euclidiana unitaria, con inicio (cola) en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span>. Entonces, para todas las posibles direcciones, la que entrega el valor más negativo del producto interno, <span class="math notranslate nohighlight">\(\nabla^{T}J(\boldsymbol{\theta}^{(i-1)})z\)</span>, es aquella de gradiente negativo</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
z=-\frac{\nabla J(\boldsymbol{\theta}^{(i-1)})}{\|\nabla J(\boldsymbol{\theta}^{(i-1)}\|}
\]</div>
<ul class="simple">
<li><p>Centrando <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span> en la bola con norma Euclideana uno. <em><strong>De todos los vectores con norma unitaria y origen en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span>, <em><strong>seleccionamos aquel que apunta en la dirección negativa del gradiente</strong></em>. Por lo tanto, para todos los vectores con norma Euclidiana 1, la <em><strong>dirección de descenso mas pronunciada coincide con la dirección del gradiente descendiente, negativo</strong></em>, y la correspondiente actualización recursiva se convierte en</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}-\mu_{i}\nabla J(\boldsymbol{\theta}^{(i-1)}),\quad\text{Gradiente descendiente}.
\]</div>
<figure class="align-center" id="fig-desc-gradient">
<a class="reference internal image-reference" href="_images/desc_gradient.png"><img alt="_images/desc_gradient.png" src="_images/desc_gradient.png" style="width: 480.6px; height: 352.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">Representación del gradiente negativo, el cual conduce a la máxima disminución de la función de coste.</span><a class="headerlink" href="#fig-desc-gradient" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La selección de <span class="math notranslate nohighlight">\(\mu_{i}\)</span> debe ser realizada de tal forma que <em><strong>garantice convergencia de la secuencia de minimización</strong></em>. Nótese que <em><strong>el algoritmo puede oscilar en torno al mínimo sin converger, si no seleccionamos la dirección correcta</strong></em>. La selección de <span class="math notranslate nohighlight">\(\mu_{i}\)</span> <em><strong>dependerá de la convergencia a cero del error entre</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)}\)</span> <em><strong>y el mínimo real en forma de serie geométrica</strong></em>.</p></li>
<li><p>Por ejemplo, para el caso de la función de coste del error cuadrático medio, la longitud de paso está dada por: <span class="math notranslate nohighlight">\(0&lt;\mu&lt;2/\lambda_{\max}\)</span>, donde <span class="math notranslate nohighlight">\(\lambda_{\max}\)</span> el máximo eigenvalor de la matriz de covarianza <span class="math notranslate nohighlight">\(\Sigma_{x}=\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^{T}]\)</span>, donde <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})=\text{E}[(y-\boldsymbol{\theta}^{T}\boldsymbol{x})^{2}]\)</span> (ver <span id="id1">[<a class="reference internal" href="biblio.html#id31" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>).</p></li>
</ul>
</section>
<section id="el-perceptron">
<h2>El perceptrón<a class="headerlink" href="#el-perceptron" title="Link to this heading">#</a></h2>
<ul>
<li><p>Nuestro punto de partida será considerar el problema simple de una <em><strong>tarea de clasificación conformada por dos clases linealmente separables</strong></em>. En otras palabras, dado un conjunto de muestras de entrenamiento, <span class="math notranslate nohighlight">\((y_{n}, \boldsymbol{x}_{n})\)</span>, <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span>, con <span class="math notranslate nohighlight">\(y_{n}\in\{-1,+1\},~\boldsymbol{x}_{n}\in\mathbb{R}^{l}\)</span>, suponemos que existe un hiperplano</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\theta}_{\star}^{T}\boldsymbol{x}=0,
    \]</div>
<p>tal que,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{cases}
    \boldsymbol{\theta}_{\star}^{T}\boldsymbol{x}&amp;&gt;0,\quad\text{si}\quad\boldsymbol{x}\in\omega_{1}\\
    \boldsymbol{\theta}_{\star}^{T}\boldsymbol{x}&amp;&lt;0,\quad\text{si}\quad\boldsymbol{x}\in\omega_{2}
    \end{cases}
    \end{split}\]</div>
<p>En otras palabras, <em><strong>dicho hiperplano clasifica correctamente todos los puntos del conjunto de entrenamiento</strong></em>. Para simplificar, el <em><strong>término de sesgo del hiperplano ha sido absorbido en</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{\star}\)</span> después de extender la dimensionalidad del espacio de entrada en uno. El objetivo ahora es <em><strong>desarrollar un algoritmo que calcule iterativamente un hiperplano que clasifique correctamente todos los patrones de ambas clases</strong></em>. Para ello, se adopta una función de costo.</p>
</li>
</ul>
<ul class="simple">
<li><p>Sea <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> la <em><strong>estimación del vector de parámetros desconocidos, disponible en la actual iteración</strong></em>. Entonces hay dos posibilidades. La primera es que <em><strong>todos los puntos estén clasificados correctamente</strong></em>; esto significa que se ha obtenido una solución. La otra alternativa es que <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> <em><strong>clasifique correctamente algunos de los puntos</strong></em> y el resto estén mal clasificados.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Costo perceptrón</p>
<p>Sea <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> el conjunto de todas las muestras mal clasificadas. La <strong><code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">costo,</span> <span class="pre">perceptrón</span></code></strong> se define como</p>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta})=-\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}:\quad\textit{Costo perceptrón},
\]</div>
<p>donde</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y_{n}=
\begin{cases}
+1,&amp;\quad\text{si}~\boldsymbol{x}\in\omega_{1}\\
-1,&amp;\quad\text{si}~\boldsymbol{x}\in\omega_{2}.
\end{cases}
\end{split}\]</div>
</div>
<ul class="simple">
<li><p>Nótese que la función <em><strong>la función de costo es no negativa</strong></em>. En efecto, dado que la suma es sobre los puntos mal clasificados, si <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\in\omega_{1}~(\omega_{2}),~\)</span> entonces <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}\leq (\geq)~0\)</span>, entregando así un producto <span class="math notranslate nohighlight">\(-y_{n}\boldsymbol{\theta}^{T}\boldsymbol{x}_{n}\geq0\)</span>.</p></li>
<li><p><em><strong>La función de costo es cero, si no existen puntos mal clasificados</strong></em>, esto es, <span class="math notranslate nohighlight">\(\mathcal{Y}=\emptyset\)</span>. La función de costo perceptrón <em><strong>no es diferenciable en todos los puntos, es lineal por tramos</strong></em>. Si reescribimos <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span> en una forma ligeramente diferente:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta})=\left(-\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}^{T}\right)\boldsymbol{\theta}.
\]</div>
<ul class="simple">
<li><p><em><strong>Nótese que esta es una función lineal con respeto a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, siempre que el conjunto de puntos mal clasificados permanezca igual. Además, <em><strong>nótese que ligeros cambios del valor</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> <em><strong>corresponden a cambios de posición del respectivo hiperplano</strong></em>. Como consecuencia, existirá un punto donde el número de muestras mal clasificadas en <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, repentinamente cambia; este es el tiempo donde <em><strong>una muestra en el conjunto de entrenamiento cambia su posición relativa con respecto a el hiperplano en movimiento</strong></em>, y en consecuencia, el conjunto <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> es modificado. Después de este cambio, el conjunto, <span class="math notranslate nohighlight">\(J(\boldsymbol{\theta})\)</span>, corresponderá a una nueva  función lineal.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">El algoritmo perceptrón</p>
<p>A partir del <em><strong>método de subgradientes</strong></em> se puede verificar fácilmente que, iniciando desde un punto arbitrario, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span>, el siguiente método iterativo,</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\mu_{i}\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}:\quad\text{Regla perceptrón}, 
\]</div>
<p>converge después de un <em>número finito de pasos</em>. La sucesión de parámetros <span class="math notranslate nohighlight">\(\mu_{i}\)</span> es seleccionada adecuadamente para garantizar convergencia.</p>
</div>
<ul class="simple">
<li><p>Nótese que usando el <em><strong>método de subgradiente (ver apéndice)</strong></em> se tiene que</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{\theta}^{(i)}&amp;=\boldsymbol{\theta}^{(i-1)}-\mu_{i}J'(\boldsymbol{\theta}^{(i-1)})\\
&amp;=\boldsymbol{\theta}^{(i-1)}-\mu_{i}\left(-\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}^{T}\right)\\
&amp;=\boldsymbol{\theta}^{(i-1)}+\mu_{i}\sum_{n:\boldsymbol{x}_{n}\in\mathcal{Y}}y_{n}\boldsymbol{x}_{n}.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Otra versión del algoritmo considera una <em><strong>muestra por iteración en un esquema cíclico, hasta que el algoritmo converge</strong></em>. Denotemos por <span class="math notranslate nohighlight">\(y_{(i)}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{x}_{i},~i\in\{1,2,\dots,N\}\)</span> los <em><strong>pares de entrenamiento presentados al algoritmo en la iteración</strong></em> <span class="math notranslate nohighlight">\(i\)</span><em><strong>-ésima</strong></em>. Entonces, la iteración de actualización se convierte en:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-perceptron-algo2">
<span class="eqno">(72)<a class="headerlink" href="#equation-eq-perceptron-algo2" title="Link to this equation">#</a></span>\[\begin{split}
\boldsymbol{\theta}^{(i)}=
\begin{cases}
\boldsymbol{\theta}^{(i-1)}+\mu_{i}y_{(i)}\boldsymbol{x}_{(i)},&amp;\quad\text{si}\,\boldsymbol{x}_{(i)}\,\text{es mal clasificado por}\,\boldsymbol{\theta}^{(i-1)},\\
\boldsymbol{\theta}^{(i-1)},&amp;\quad\text{otro caso}.
\end{cases}
\end{split}\]</div>
<ul class="simple">
<li><p>Esto es, partiendo de una estimación inicial de forma random, <em><strong>inicializando</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span> <em><strong>con algunos valores pequeños, testeamos cada una de las muestras</strong></em>, <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n},~n=1,2,\dots,N\)</span>. <em><strong>Cada vez que una muestra es mal clasificada, se toma acción por medio de la regla perceptrón para una corrección</strong></em>. <em><strong>En otro caso, ninguna acción es requerida</strong></em>. Una vez que todas las muestras han sido consideradas, decimos que una <strong><code class="docutils literal notranslate"><span class="pre">época</span> <span class="pre">(epoch)</span></code></strong> ha sido completada. Si no se obtiene convergencia, todas las muestras son reconsideradas en una segunda época, y así sucesivamente. La versión de este algoritmo es conocida como esquema <strong><code class="docutils literal notranslate"><span class="pre">pattern-by-pattern</span></code></strong>. Algunas veces también es referido como el <em><strong>algoritmo online</strong></em>. Nótese que el número total de datos muestrales es fijo, y que el algoritmo las considera en forma cíclica, época por época (<em><strong>epoch-by-epoch</strong></em>).</p></li>
<li><p>Después de un número finito de épocas, se garantiza que el algoritmo es convergente. <em><strong>Nótese que para obtener dicha convergencia, la sucesión</strong></em> <span class="math notranslate nohighlight">\(\mu_{i}\)</span> <em><strong>debe ser seleccionada apropiadamente</strong></em>. Sin embargo para el caso del <em><strong>algoritmo perceptrón, la convergencia es garantizada</strong></em> (<span class="math notranslate nohighlight">\(J\)</span> convexa), aún cuando <span class="math notranslate nohighlight">\(\mu_{i}\)</span> es una constante positiva, <span class="math notranslate nohighlight">\(\mu_{i}=\mu&gt;0\)</span>, <em><strong>usualmente tomado igual a uno</strong></em>. La formulación en <a class="reference internal" href="#equation-eq-perceptron-algo2">(72)</a> es conocida también como la filosofía de aprendizaje <strong><code class="docutils literal notranslate"><span class="pre">reward-punishment</span></code></strong>. Si la actual estimación es exitosa en la predicción de la clase del respectivo patron, ninguna acción es tomada (<code class="docutils literal notranslate"><span class="pre">reward</span></code>), en otro caso, el algoritmo es obligado a realizar una actualización (<code class="docutils literal notranslate"><span class="pre">punishment</span></code>).</p></li>
</ul>
<figure class="align-center" id="perceptron-rule">
<img alt="_images/perceptron_rule.png" src="_images/perceptron_rule.png" />
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">El punto <span class="math notranslate nohighlight">\(x\)</span> está mal clasificado por la línea roja. La regla perceptrón gira el hiperplano hacia el punto <span class="math notranslate nohighlight">\(x\)</span>, para intentar incluirlo en el lado correcto del nuevo hiperplano y clasificarlo correctamente. El nuevo hiperplano está definido por <span class="math notranslate nohighlight">\(θ^{(i)}\)</span> y se muestra con la línea negra.</span><a class="headerlink" href="#perceptron-rule" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>La <a class="reference internal" href="#perceptron-rule"><span class="std std-numref">Fig. 8</span></a> ofrece una interpretación geométrica de la <em><strong>regla del perceptrón</strong></em>. Supongamos que la muestra <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> está mal clasificada por el hiperplano, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span>. Como sabemos, por geometría analítica, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i-1)}\)</span> <em><strong>corresponde a un vector que es perpendicular al hiperplano que está definido por este vector</strong></em>. Como <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> se encuentra en el lado <span class="math notranslate nohighlight">\((-)\)</span> del hiperplano y está mal clasificado, pertenece a la clase <span class="math notranslate nohighlight">\(\omega_{1}\)</span>; asumiendo <span class="math notranslate nohighlight">\(\mu = 1\)</span>, la corrección aplicada por el algoritmo es</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\boldsymbol{x},
    \end{split}\]</div>
<p>y su efecto es <em><strong>girar el hiperplano en dirección a</strong></em> <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> para colocarlo en el lado <span class="math notranslate nohighlight">\((+)\)</span> del nuevo hiperplano, que está definido por la estimación actualizada <span class="math notranslate nohighlight">\(\boldsymbol{\theta^{(i)}}\)</span>. El <em><strong>algoritmo perceptrón</strong></em> en su modo de funcionamiento patrón por patrón (<em><strong>pattern-by-pattern</strong></em>) se resume en el siguiente algoritmo.</p>
</li>
</ul>
<div class="proof algorithm admonition" id="my_algorithm_pattern_by_pattern">
<p class="admonition-title"><span class="caption-number">Algorithm 1 </span> (Algoritmo perceptrón <em>pattern-by-pattern</em>)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inicialización</strong></p>
<ol class="arabic simple">
<li><p>Inicializar <span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(0)}\)</span>; usualmente, de forma <code class="docutils literal notranslate"><span class="pre">random,</span> <span class="pre">un</span> <span class="pre">número</span> <span class="pre">pequeño</span></code></p></li>
<li><p>Seleccionar <span class="math notranslate nohighlight">\(\mu\)</span>; usualmente <code class="docutils literal notranslate"><span class="pre">establecido</span> <span class="pre">como</span> <span class="pre">uno</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(i=1\)</span></p></li>
</ol>
<p><strong>Repeat</strong> Cada iteración corresponde a un <code class="docutils literal notranslate"><span class="pre">epoch</span></code></p>
<ol class="arabic">
<li><p><code class="docutils literal notranslate"><span class="pre">counter</span> <span class="pre">=</span> <span class="pre">0</span></code>; Contador del número de actualizaciones por <code class="docutils literal notranslate"><span class="pre">epoch</span></code></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span> <strong>Do</strong> Para cada <code class="docutils literal notranslate"><span class="pre">epoch</span></code>, todas las muestras son presentadas una vez</p>
<p><strong>If</strong>(<span class="math notranslate nohighlight">\(y_{n}\boldsymbol{x}_{n}^{T}\leq0\)</span>) <strong>Then</strong></p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}^{(i)}=\boldsymbol{\theta}^{(i-1)}+\mu y_{n}\boldsymbol{x}_{n}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(i=i+1\)</span></p></li>
<li><p>counter = counter + 1</p></li>
</ol>
<p><strong>End For</strong></p>
</li>
<li><p><strong>Until</strong> counter = 0</p></li>
</ol>
</section>
</div><ul class="simple">
<li><p>Una vez que el <code class="docutils literal notranslate"><span class="pre">algoritmo</span> <span class="pre">perceptrón</span> <span class="pre">se</span> <span class="pre">ha</span> <span class="pre">ejecutado</span> <span class="pre">y</span> <span class="pre">converge</span></code>, tenemos los <code class="docutils literal notranslate"><span class="pre">pesos</span></code>, <span class="math notranslate nohighlight">\(\theta_{i},~i = 1,2,\dots,l\)</span>, <code class="docutils literal notranslate"><span class="pre">de</span> <span class="pre">las</span> <span class="pre">sinapsis</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">neurona/perceptrón</span></code> asociada, así como el término de sesgo <span class="math notranslate nohighlight">\(\theta_{0}\)</span>. Ahora se pueden <code class="docutils literal notranslate"><span class="pre">utilizar</span> <span class="pre">para</span> <span class="pre">clasificar</span> <span class="pre">patrones</span> <span class="pre">desconocidos</span></code>. Las características <span class="math notranslate nohighlight">\(x_{i}, i = 1, 2,\dots,l\)</span>, se aplican a los nodos de entrada. A su vez, <code class="docutils literal notranslate"><span class="pre">cada</span> <span class="pre">característica</span> <span class="pre">se</span> <span class="pre">multiplica</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">sinapsis</span> <span class="pre">respectiva</span> <span class="pre">(peso),</span> <span class="pre">y</span> <span class="pre">luego</span> <span class="pre">se</span> <span class="pre">añade</span> <span class="pre">el</span> <span class="pre">término</span> <span class="pre">de</span> <span class="pre">sesgo</span> <span class="pre">en</span> <span class="pre">su</span> <span class="pre">combinación</span> <span class="pre">lineal</span></code>. El resultado de esta operación <code class="docutils literal notranslate"><span class="pre">pasa</span> <span class="pre">por</span> <span class="pre">una</span> <span class="pre">función</span> <span class="pre">no</span> <span class="pre">lineal</span></code>, <span class="math notranslate nohighlight">\(f\)</span>, conocida como <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">activación</span></code> (ver <a class="reference external" href="https://en.wikipedia.org/wiki/Activation_function">Activation function</a>). Dependiendo de la forma de la no linealidad, se producen diferentes tipos de neuronas. La mas clásica conocida como <code class="docutils literal notranslate"><span class="pre">neurona</span> <span class="pre">McCulloch-Pitts</span></code>, la función de <code class="docutils literal notranslate"><span class="pre">activación</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">de</span> <span class="pre">Heaviside</span></code>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
f(z)=
\begin{cases}
1,&amp;\quad\text{si}~z&gt;0,\\
0,&amp;\quad\text{si}~z\leq0.
\end{cases}
\end{split}\]</div>
<figure class="align-center" id="mcculloch-pitts">
<a class="reference internal image-reference" href="_images/mcculloch_pitts.png"><img alt="_images/mcculloch_pitts.png" src="_images/mcculloch_pitts.png" style="width: 532.0px; height: 167.20000000000002px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">Arquitectura básica de neuronas/perceptrones.</span><a class="headerlink" href="#mcculloch-pitts" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En la arquitectura básica de neuronas/perceptrones, las <code class="docutils literal notranslate"><span class="pre">características</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">se</span> <span class="pre">aplican</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">nodos</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">y</span> <span class="pre">se</span> <span class="pre">ponderan</span> <span class="pre">por</span> <span class="pre">los</span> <span class="pre">respectivos</span> <span class="pre">pesos</span> <span class="pre">que</span> <span class="pre">definen</span> <span class="pre">las</span> <span class="pre">sinapsis</span></code>. A continuación <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">añade</span> <span class="pre">el</span> <span class="pre">término</span> <span class="pre">de</span> <span class="pre">sesgo</span> <span class="pre">en</span> <span class="pre">su</span> <span class="pre">combinación</span> <span class="pre">lineal</span> <span class="pre">y</span> <span class="pre">el</span> <span class="pre">resultado</span> <span class="pre">es</span> <span class="pre">empujado</span> <span class="pre">a</span> <span class="pre">través</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span></code>. En la neurona <code class="docutils literal notranslate"><span class="pre">McCulloch-Pitts</span></code>, la salida es 1 para los patrones de la clase <span class="math notranslate nohighlight">\(\omega_{1}\)</span> o 0 para la clase <span class="math notranslate nohighlight">\(\omega_{2}\)</span>. La suma y la operación no lineal se unen para simplificar el gráfico.</p></li>
</ul>
<ul class="simple">
<li><p>Para las capas ocultas, la función de <code class="docutils literal notranslate"><span class="pre">activación</span> <span class="pre">tangente</span> <span class="pre">hiperbólica</span> <span class="pre">suele</span> <span class="pre">funcionar</span> <span class="pre">mejor</span> <span class="pre">que</span> <span class="pre">la</span> <span class="pre">sigmoidea</span> <span class="pre">logística</span></code>. Tanto la función <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> como <code class="docutils literal notranslate"><span class="pre">tanh</span></code> pueden hacer que el modelo sea más <code class="docutils literal notranslate"><span class="pre">susceptible</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">problemas</span> <span class="pre">durante</span> <span class="pre">el</span> <span class="pre">entrenamiento,</span> <span class="pre">a</span> <span class="pre">través</span> <span class="pre">del</span> <span class="pre">llamado</span> <span class="pre">problema</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">desvanecientes</span></code>. Los modelos modernos de redes neuronales con arquitecturas comunes, como <code class="docutils literal notranslate"><span class="pre">MLP</span> <span class="pre">y</span> <span class="pre">CNN,</span> <span class="pre">harán</span> <span class="pre">uso</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">activación</span> <span class="pre">ReLU</span></code>, o extensiones.</p></li>
<li><p>Las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">recurrentes</span> <span class="pre">recurrentes</span></code> suelen utilizar funciones de activación <code class="docutils literal notranslate"><span class="pre">tanh</span></code> o <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>, o incluso ambas. Por ejemplo, la <code class="docutils literal notranslate"><span class="pre">LSTM</span> <span class="pre">suele</span> <span class="pre">utilizar</span> <span class="pre">la</span> <span class="pre">activación</span> <span class="pre">sigmoid</span> <span class="pre">para</span> <span class="pre">las</span> <span class="pre">conexiones</span> <span class="pre">recurrentes</span> <span class="pre">y</span> <span class="pre">la</span> <span class="pre">activación</span> <span class="pre">tanh</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">salida</span></code>.</p></li>
</ul>
<figure class="align-center" id="output-layer-activation-function">
<a class="reference internal image-reference" href="_images/output_layer_activation_function.png"><img alt="_images/output_layer_activation_function.png" src="_images/output_layer_activation_function.png" style="width: 485.79999999999995px; height: 301.7px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Selección de función de activación para <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">layers</span></code>. (Fuente <span id="id2">[<a class="reference internal" href="biblio.html#id32" title="J. Brownlee and Machine Learning Mastery. Deep Learning with Python: Develop Deep Learning Models on Theano and TensorFlow Using Keras. Machine Learning Mastery, 2017. URL: https://books.google.com.co/books?id=eJw2nQAACAAJ.">Brownlee and Mastery, 2017</a>]</span>).</span><a class="headerlink" href="#output-layer-activation-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Si su problema es de <code class="docutils literal notranslate"><span class="pre">regresión,</span> <span class="pre">debe</span> <span class="pre">utilizar</span> <span class="pre">una</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">activación</span> <span class="pre">lineal</span></code>. Si su problema es de <code class="docutils literal notranslate"><span class="pre">clasificación</span></code>, el modelo predice la probabilidad de pertenencia a una clase, que se puede convertir en una <code class="docutils literal notranslate"><span class="pre">etiqueta</span> <span class="pre">de</span> <span class="pre">clase</span> <span class="pre">mediante</span> <span class="pre">redondeo</span> <span class="pre">(para</span> <span class="pre">sigmoid)</span></code> o <code class="docutils literal notranslate"><span class="pre">argmax</span> <span class="pre">(para</span> <span class="pre">softmax)</span></code>.</p></li>
</ul>
</section>
<section id="redes-totalmente-conectadas">
<h2>Redes Totalmente Conectadas<a class="headerlink" href="#redes-totalmente-conectadas" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Para resumir de manera más formal el tipo de <code class="docutils literal notranslate"><span class="pre">operaciones</span> <span class="pre">que</span> <span class="pre">tienen</span> <span class="pre">lugar</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">red</span> <span class="pre">totalmente</span> <span class="pre">conectada</span></code>, centrémonos en, por ejemplo, la <code class="docutils literal notranslate"><span class="pre">capa</span></code> <span class="math notranslate nohighlight">\(r\)</span> <code class="docutils literal notranslate"><span class="pre">de</span> <span class="pre">una</span> <span class="pre">red</span> <span class="pre">neuronal</span> <span class="pre">multicapa</span> <span class="pre">y</span> <span class="pre">supongamos</span> <span class="pre">que</span> <span class="pre">está</span> <span class="pre">formada</span> <span class="pre">por</span></code> <span class="math notranslate nohighlight">\(k_{r}\)</span> <code class="docutils literal notranslate"><span class="pre">neuronas</span></code>. El vector de <code class="docutils literal notranslate"><span class="pre">entrada</span> <span class="pre">a</span> <span class="pre">esta</span> <span class="pre">capa</span> <span class="pre">está</span> <span class="pre">formado</span> <span class="pre">por</span> <span class="pre">las</span> <span class="pre">salidas</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">nodos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">anterior,</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">denomina</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{y}^{r-1}\)</span>. Sea <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}\)</span> el vector de los <code class="docutils literal notranslate"><span class="pre">pesos</span> <span class="pre">sinápticos,</span> <span class="pre">incluido</span> <span class="pre">el</span> <span class="pre">término</span> <span class="pre">de</span> <span class="pre">sesgo,</span> <span class="pre">asociado</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">neurona</span></code> <span class="math notranslate nohighlight">\(j\)</span> <code class="docutils literal notranslate"><span class="pre">de</span> <span class="pre">la</span> <span class="pre">capa</span></code> <span class="math notranslate nohighlight">\(r\)</span>, donde <span class="math notranslate nohighlight">\(j = 1,2,\dots, k_{r}\)</span>. La <code class="docutils literal notranslate"><span class="pre">dimensión</span> <span class="pre">respectiva</span></code> de este vector es <span class="math notranslate nohighlight">\(k_{r-1} + 1\)</span>, donde <span class="math notranslate nohighlight">\(k_{r-1}\)</span> es el <code class="docutils literal notranslate"><span class="pre">número</span> <span class="pre">de</span> <span class="pre">neuronas</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">anterior</span></code>, <span class="math notranslate nohighlight">\(r-1\)</span>, <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">el</span> <span class="pre">aumento</span> <span class="pre">en</span> <span class="pre">1</span> <span class="pre">representa</span> <span class="pre">el</span> <span class="pre">término</span> <span class="pre">de</span> <span class="pre">sesgo</span></code>. Entonces las operaciones realizadas, antes de la no linealidad, son los productos internos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
z_{j}^{r}=\boldsymbol{\theta}_{j}^{rT}\boldsymbol{y}^{r-1},\quad j=1,2,\dots,k_{r}.
\]</div>
<ul class="simple">
<li><p>Colocando todos los valores de salida en un vector <span class="math notranslate nohighlight">\(\boldsymbol{z}^{r}=[z_{1}^{r}, z_{2}^{r},\dots,z_{k_{r}}^{r}]^{T}\)</span>, y <code class="docutils literal notranslate"><span class="pre">agrupando</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">sinápticos</span> <span class="pre">como</span> <span class="pre">filas</span></code>, una debajo de la otra, en una matriz, podemos escribir colectivamente</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{z}^{r}=\Theta\boldsymbol{y}^{r-1},\quad\text{donde}\quad\Theta:=[\boldsymbol{\theta}_{1}^{r}, \boldsymbol{\theta}_{2}^{r},\dots, \boldsymbol{\theta}_{k_{r}}^{r}].
\]</div>
<ul class="simple">
<li><p>El vector de las salidas de la <span class="math notranslate nohighlight">\(r\)</span> th capa oculta, después de <code class="docutils literal notranslate"><span class="pre">empujar</span> <span class="pre">cada</span></code> <span class="math notranslate nohighlight">\(z_{i}^{r}\)</span> <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">través</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span></code> <span class="math notranslate nohighlight">\(f\)</span>, está finalmente dado por</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{y}^{r}=
\begin{bmatrix}
1\\
f(\boldsymbol{z}^{r})
\end{bmatrix}
\end{split}\]</div>
<ul class="simple">
<li><p>La notación anterior significa que <span class="math notranslate nohighlight">\(f\)</span> <code class="docutils literal notranslate"><span class="pre">actúa</span> <span class="pre">sobre</span> <span class="pre">cada</span> <span class="pre">uno</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">respectivos</span> <span class="pre">componentes</span> <span class="pre">del</span> <span class="pre">vector</span></code>, individualmente, y la <code class="docutils literal notranslate"><span class="pre">extensión</span> <span class="pre">del</span> <span class="pre">vector</span> <span class="pre">en</span> <span class="pre">uno</span> <span class="pre">es</span> <span class="pre">para</span> <span class="pre">dar</span> <span class="pre">cuenta</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">términos</span> <span class="pre">de</span> <span class="pre">sesgo</span></code> en la práctica estándar. Para redes grandes, con muchas capas y muchos nodos por capa, este tipo de conectividad resulta ser muy costoso en términos del número de parámetros (pesos), que es del orden de <span class="math notranslate nohighlight">\(k_{r}k_{r-1}\)</span>. Por ejemplo, si <span class="math notranslate nohighlight">\(k_{r-1} = 1000\)</span> y <span class="math notranslate nohighlight">\(k_{r} = 1000\)</span>, <code class="docutils literal notranslate"><span class="pre">esto</span> <span class="pre">equivale</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">orden</span> <span class="pre">de</span> <span class="pre">1</span> <span class="pre">millón</span> <span class="pre">de</span> <span class="pre">parámetros</span></code>. Tenga en cuenta que este número es la contribución de los parámetros de una sola de las capas. Sin embargo, <code class="docutils literal notranslate"><span class="pre">un</span> <span class="pre">gran</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">hace</span> <span class="pre">que</span> <span class="pre">una</span> <span class="pre">red</span> <span class="pre">sea</span> <span class="pre">vulnerable</span> <span class="pre">al</span> <span class="pre">sobreajuste</span></code>, cuando se trata de entrenamiento</p></li>
<li><p>Se pueden emplear las llamadas <code class="docutils literal notranslate"><span class="pre">técnicas</span> <span class="pre">de</span> <span class="pre">reparto</span> <span class="pre">de</span> <span class="pre">pesos</span></code>, en las que un conjunto de parámetros es compartido entre un número de conexiones, a través de restricciones adecuadamente incorporadas. Las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">neuronales</span> <span class="pre">recurrentes</span> <span class="pre">y</span> <span class="pre">convolucionales</span></code> que se discutirán en en el curso <strong><code class="docutils literal notranslate"><span class="pre">time</span> <span class="pre">series</span> <span class="pre">forecasting</span></code></strong>, pertenecen a esta familia de redes de peso compartido. Como veremos, en una red convolucional, las <code class="docutils literal notranslate"><span class="pre">convoluciones</span> <span class="pre">sustituyen</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">operaciones</span> <span class="pre">de</span> <span class="pre">producto</span> <span class="pre">interno</span></code>, lo que permite un reparto de pesos importante que conduce a una reducción sustancial del número de parámetros.</p></li>
</ul>
</section>
<section id="el-algoritmo-de-backpropagation">
<h2>El Algoritmo De Backpropagation<a class="headerlink" href="#el-algoritmo-de-backpropagation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Una red neuronal considera una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">paramétrica</span> <span class="pre">no</span> <span class="pre">lineal</span></code>, <span class="math notranslate nohighlight">\(\hat{y} = f_{\boldsymbol{\theta}}(\boldsymbol{x})\)</span>, donde <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> representa todos los pesos/sesgo presentes en la red. Por lo tanto, el entrenamiento de una red neuronal no parece ser diferente del entrenamiento de cualquier otro modelo de predicción paramétrica. Todo lo que se necesita es <code class="docutils literal notranslate"><span class="pre">(a)</span></code> un <code class="docutils literal notranslate"><span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">muestras</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>, <code class="docutils literal notranslate"><span class="pre">(b)</span></code> una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span></code> <span class="math notranslate nohighlight">\(\mathcal{L}(y, \hat{y})\)</span>, y <code class="docutils literal notranslate"><span class="pre">(c)</span></code> un <code class="docutils literal notranslate"><span class="pre">esquema</span> <span class="pre">iterativo</span></code>, por ejemplo, el <code class="docutils literal notranslate"><span class="pre">gradiente</span> <span class="pre">descendiente</span></code>, para realizar la optimización de la función de coste asociada (<code class="docutils literal notranslate"><span class="pre">pérdida</span> <span class="pre">empírica</span></code>).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(\boldsymbol{\theta})=\sum_{n=1}^{N}\mathcal{L}(y_{n}, f_{\boldsymbol{\theta}}(\boldsymbol{x}_{n})).
\]</div>
<ul class="simple">
<li><p>La dificultad del entrenamiento de las redes neuronales radica en su <code class="docutils literal notranslate"><span class="pre">estructura</span> <span class="pre">multicapa</span> <span class="pre">que</span> <span class="pre">complica</span> <span class="pre">el</span> <span class="pre">cálculo</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">gradientes,</span> <span class="pre">que</span> <span class="pre">intervienen</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">optimización</span></code>. Además, la neurona <code class="docutils literal notranslate"><span class="pre">McCulloch-Pitts</span></code> se basa en la función de activación discontinua de <code class="docutils literal notranslate"><span class="pre">Heaviside,</span> <span class="pre">que</span> <span class="pre">no</span> <span class="pre">es</span> <span class="pre">diferenciable</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">neurona</span> <span class="pre">sigmoidea</span> <span class="pre">logística</span></code>: Una posibilidad es adoptar la función <code class="docutils literal notranslate"><span class="pre">sigmoidea</span> <span class="pre">logística</span></code>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f(z)=\sigma(z):=\frac{1}{1+\exp(-az)}.
\]</div>
<ul class="simple">
<li><p>Nótese que <code class="docutils literal notranslate"><span class="pre">cuanto</span> <span class="pre">mayor</span> <span class="pre">sea</span> <span class="pre">el</span> <span class="pre">valor</span> <span class="pre">del</span> <span class="pre">parámetro</span></code> <span class="math notranslate nohighlight">\(a\)</span>, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">gráfica</span> <span class="pre">correspondiente</span> <span class="pre">se</span> <span class="pre">acerca</span> <span class="pre">más</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">Heaviside</span></code> (ver <a class="reference internal" href="#sigmoid-act-function"><span class="std std-numref">Fig. 11</span></a>).</p></li>
</ul>
<figure class="align-center" id="sigmoid-act-function">
<a class="reference internal image-reference" href="_images/sigmoid_act_function.png"><img alt="_images/sigmoid_act_function.png" src="_images/sigmoid_act_function.png" style="width: 507.20000000000005px; height: 471.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">La función sigmoidea logística para diferentes valores del parámetro <span class="math notranslate nohighlight">\(a\)</span>.</span><a class="headerlink" href="#sigmoid-act-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Otra posibilidad sería <code class="docutils literal notranslate"><span class="pre">utilizar</span> <span class="pre">la</span> <span class="pre">función</span></code>,</p>
<div class="math notranslate nohighlight">
\[
    f(z)=a\tanh\left(\frac{cz}{2}\right),
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(c\)</span> y <span class="math notranslate nohighlight">\(a\)</span> son parámetros de control. El gráfico de esta función se muestra en la <a class="reference internal" href="#tanh-act-function"><span class="std std-numref">Fig. 12</span></a>. Nótese que a diferencia de la sigmoidea logística, ésta es una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">es</span> <span class="pre">no</span> <span class="pre">simétrica</span></code>, es decir, <span class="math notranslate nohighlight">\(f(-z)=-f(z)\)</span>. Ambas son también conocidas como <code class="docutils literal notranslate"><span class="pre">funciones</span> <span class="pre">de</span> <span class="pre">reducción,</span> <span class="pre">porque</span> <span class="pre">limitan</span> <span class="pre">la</span> <span class="pre">salida</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">rango</span> <span class="pre">finito</span> <span class="pre">de</span> <span class="pre">valores</span></code>.</p>
</li>
</ul>
<figure class="align-center" id="tanh-act-function">
<a class="reference internal image-reference" href="_images/tanh_act_function.png"><img alt="_images/tanh_act_function.png" src="_images/tanh_act_function.png" style="width: 693.6px; height: 435.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">Función de reducción de la tangente hiperbólica para <span class="math notranslate nohighlight">\(a = 1.7\)</span> y <span class="math notranslate nohighlight">\(c = 4/3\)</span>.</span><a class="headerlink" href="#tanh-act-function" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Recordemos que la regla de <code class="docutils literal notranslate"><span class="pre">actualización</span> <span class="pre">del</span> <span class="pre">algoritmo</span> <span class="pre">gradiente</span> <span class="pre">descendiente</span></code>, en su versión unidimensional se convierte en</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    \theta(new)=\theta(old)-\mu\left.\frac{d J}{d\theta}\right|_{\theta(old)},
    \end{split}\]</div>
<p>y las iteraciones parten de un punto inicial arbitrario, <span class="math notranslate nohighlight">\(\theta^{(0)}\)</span>. Si en la iteración actual el algoritmo está digamos, en el punto <span class="math notranslate nohighlight">\(\theta(old) = \theta_{1}\)</span>, entonces se moverá hacia el mínimo local, <span class="math notranslate nohighlight">\(\theta_{l}\)</span>. Esto se debe a que la derivada del coste en <span class="math notranslate nohighlight">\(\theta_{1}\)</span> es igual a la tangente de <span class="math notranslate nohighlight">\(\phi_{1}\)</span>, que es negativa (el ángulo es obtuso) y la actualización, <span class="math notranslate nohighlight">\(\theta(new)\)</span>, se moverá a la derecha, hacia el mínimo local, <span class="math notranslate nohighlight">\(\theta_{l}\)</span>.</p>
</li>
</ul>
<figure class="align-center" id="convex-function-saddle-point">
<a class="reference internal image-reference" href="_images/convex_function_saddle_point.png"><img alt="_images/convex_function_saddle_point.png" src="_images/convex_function_saddle_point.png" style="width: 635.2px; height: 354.40000000000003px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">Función no convexa global, con mínimos locales y puntos de silla.</span><a class="headerlink" href="#convex-function-saddle-point" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">elección</span> <span class="pre">del</span> <span class="pre">tamaño</span> <span class="pre">del</span> <span class="pre">paso</span></code>, <span class="math notranslate nohighlight">\(\mu\)</span>, <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">crítica</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">convergencia</span> <span class="pre">del</span> <span class="pre">algoritmo</span></code>. En problemas reales en espacios multidimensionales, el número de mínimos locales puede ser grande, por lo que el algoritmo puede converger a uno local. Sin embargo, esto no es necesariamente una mala noticia. Si este <code class="docutils literal notranslate"><span class="pre">mínimo</span> <span class="pre">local</span> <span class="pre">es</span> <span class="pre">lo</span> <span class="pre">suficientemente</span> <span class="pre">profundo</span></code>, es decir, si el valor de la función de coste en este punto, por ejemplo, <span class="math notranslate nohighlight">\(J(\theta_{l})\)</span>, <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">es</span> <span class="pre">mucho</span> <span class="pre">mayor</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">alcanzado</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">mínimo</span> <span class="pre">global</span></code>, es decir, <span class="math notranslate nohighlight">\(J(\theta_{g})\)</span>, la <code class="docutils literal notranslate"><span class="pre">convergencia</span> <span class="pre">a</span> <span class="pre">dicho</span> <span class="pre">mínimo</span> <span class="pre">local</span> <span class="pre">puede</span> <span class="pre">corresponder</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">buena</span> <span class="pre">solución</span></code>.</p></li>
</ul>
</section>
<section id="el-esquema-de-backpropagation-para-gradiente-descendiente">
<h2>El Esquema De Backpropagation Para Gradiente Descendiente<a class="headerlink" href="#el-esquema-de-backpropagation-para-gradiente-descendiente" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Habiendo adoptado una función de activación diferenciable, estamos listos para proceder a desarrollar el <code class="docutils literal notranslate"><span class="pre">esquema</span> <span class="pre">iterativo</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">descendiente</span> <span class="pre">para</span> <span class="pre">la</span> <span class="pre">minimización</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span></code>. Formularemos la tarea en un marco general.</p></li>
<li><p>Sea <span class="math notranslate nohighlight">\((\boldsymbol{y}_{n}, \boldsymbol{x}_{n}), n = 1, 2,\dots, N\)</span>, es el conjunto de muestras de entrenamiento. <code class="docutils literal notranslate"><span class="pre">Nótese</span> <span class="pre">que</span> <span class="pre">hemos</span> <span class="pre">asumido</span> <span class="pre">múltiples</span> <span class="pre">variables</span> <span class="pre">output</span></code>, como vectores. Suponemos que la red consta de <span class="math notranslate nohighlight">\(L\)</span> capas, <span class="math notranslate nohighlight">\(L-1\)</span> capas ocultas y una capa de salida. Cada capa consta de <span class="math notranslate nohighlight">\(k_{r}, r = 1, 2,\dots, L\)</span>, neuronas. Así, los vectores de salida (objetivo/deseado) son</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y}_{n}=[y_{n1}, y_{n2},\dots, y_{nk_{L}}]^{T}\in\mathbb{R}^{K_{L}},\quad n=1,2,\dots,N.
\]</div>
<ul class="simple">
<li><p>Para ciertas derivaciones matemáticas, también denotamos el número de nodos de entrada como <span class="math notranslate nohighlight">\(k_{0}\)</span>; es decir <span class="math notranslate nohighlight">\(k_{0} = l\)</span>, donde <span class="math notranslate nohighlight">\(l\)</span> es la <code class="docutils literal notranslate"><span class="pre">dimensionalidad</span> <span class="pre">del</span> <span class="pre">espacio</span> <span class="pre">de</span> <span class="pre">características</span> <span class="pre">de</span> <span class="pre">entrada</span></code>.</p></li>
<li><p>Sea <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}\)</span> denota el <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">sinápticos</span> <span class="pre">asociados</span> <span class="pre">a</span> <span class="pre">la</span></code> <span class="math notranslate nohighlight">\(j\)</span><code class="docutils literal notranslate"><span class="pre">-th</span> <span class="pre">neurona</span> <span class="pre">de</span> <span class="pre">la</span></code> <span class="math notranslate nohighlight">\(r\)</span><code class="docutils literal notranslate"><span class="pre">-th</span> <span class="pre">capa</span></code>, con <span class="math notranslate nohighlight">\(j = 1, 2,\dots, k_{r}\)</span> y <span class="math notranslate nohighlight">\(r = 1, 2,\dots,L\)</span>, donde el término de sesgo se incluye en <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}\)</span> , es decir,</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-parameters-vector-def">
<span class="eqno">(73)<a class="headerlink" href="#equation-parameters-vector-def" title="Link to this equation">#</a></span>\[
\boldsymbol{\theta}_{j}^{r}:=[\theta_{j0}^{r}, \theta_{j1}^{r},\dots, \theta_{jk_{r-1}}^{r}]^{T}.
\]</div>
<ul class="simple">
<li><p>Los pesos sinápticos enlazan la neurona respectiva con todas las neuronas de la capa <span class="math notranslate nohighlight">\(k_{r-1}\)</span> (véase la <a class="reference internal" href="#synaptic-weights-link"><span class="std std-numref">Fig. 14</span></a>). El paso iterativo básico para el esquema de gradiente descendiente se escribe como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-update-equations-gd">
<span class="eqno">(74)<a class="headerlink" href="#equation-update-equations-gd" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\boldsymbol{\theta}_{j}^{r}(\text{new})&amp;=\boldsymbol{\theta}_{j}^{r}(old)+\Delta\boldsymbol{\theta}_{j}^{r},\\
\Delta\boldsymbol{\theta}_{j}^{r}&amp;:=-\mu\left.\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{r}}\right|_{\boldsymbol{\theta}_{j}^{r}(old)}.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>El parámetro <span class="math notranslate nohighlight">\(\mu\)</span> es el tamaño de paso definido por el usuario (también puede depender de la iteración) y <span class="math notranslate nohighlight">\(J\)</span> denota la función de coste.</p></li>
</ul>
<figure class="align-center" id="synaptic-weights-link">
<a class="reference internal image-reference" href="_images/synaptic_weights_link.png"><img alt="_images/synaptic_weights_link.png" src="_images/synaptic_weights_link.png" style="width: 529.6px; height: 355.20000000000005px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">Enlaces y las variables asociadas de la <span class="math notranslate nohighlight">\(j\)</span> th neurona en la <span class="math notranslate nohighlight">\(r\)</span> th capa. <span class="math notranslate nohighlight">\(y_{k}^{r-1}\)</span> es la salida de la <span class="math notranslate nohighlight">\(k\)</span> th neurona de la <span class="math notranslate nohighlight">\((r - 1)\)</span> th capa y <span class="math notranslate nohighlight">\(\theta_{jk}^{r}\)</span> es el peso respectivo que conecta estas dos neuronas.</span><a class="headerlink" href="#synaptic-weights-link" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul>
<li><p>Las ecuaciones de actualización <a class="reference internal" href="#equation-update-equations-gd">(74)</a> comprenden el par del esquema de gradiente descendiente  para la optimización. Como se ha dicho anteriormente, la dificultad de las redes neuronales <code class="docutils literal notranslate"><span class="pre">feed-forward</span></code> surge de su estructura multicapa. Para calcular los gradientes en la Ecuación <a class="reference internal" href="#equation-update-equations-gd">(74)</a>, para todas las neuronas en todas las capas, se deben seguir dos pasos en su cálculo</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Forward</span> <span class="pre">computations</span></code>: Para un vector de entrada dado <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}, n = 1, 2,\dots, N\)</span>, se utilizan las estimaciones actuales de los parámetros (pesos sinápticos) (<span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}(old)\)</span>) y calcula todas las salidas de todas las neuronas en todas las capas, denotadas como <span class="math notranslate nohighlight">\(y_{nj}^{r}\)</span>; en la <a class="reference internal" href="#synaptic-weights-link"><span class="std std-numref">Fig. 14</span></a>, se ha suprimido el índice <span class="math notranslate nohighlight">\(n\)</span> para no afectar la notación.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Backward</span> <span class="pre">computations</span></code>: Utilizando las salidas neuronales calculadas anteriormente junto con los valores objetivo conocidos, <span class="math notranslate nohighlight">\(y_{nk}\)</span>, de la capa de salida, se calculan los gradientes de la función de coste. Esto implica <span class="math notranslate nohighlight">\(L\)</span> pasos, es decir, tantos como el número de capas. La secuencia de los pasos algorítmicos se indica a continuación:</p>
<ul>
<li><p>Calcular el gradiente de la función de coste con respecto a los parámetros de las neuronas de la última capa, es decir, <span class="math notranslate nohighlight">\(\displaystyle{\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{L}}, j = 1, 2,\dots, k_{L}}\)</span>.</p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r = L-1\)</span> to <span class="math notranslate nohighlight">\(1\)</span>, <strong>Do</strong></p>
<p>Calcular los gradientes con respecto a los parámetros asociados a las neuronas de la <span class="math notranslate nohighlight">\(r\)</span> th capa, es decir, <span class="math notranslate nohighlight">\(\displaystyle{\frac{\partial J}{\partial\boldsymbol{\theta}_{k}^{r}}, k= 1, 2,\dots, k_{r}}\)</span> basado en todos los gradientes <span class="math notranslate nohighlight">\(\displaystyle{\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{r+1}}, j= 1, 2,\dots, k_{r+1}}\)</span>, con respecto a los parámetros de la capa <span class="math notranslate nohighlight">\(r + 1\)</span> que se han calculado en el paso anterior.</p>
</li>
<li><p><strong>End For</strong></p></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>El esquema de cálculo hacia atrás <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code> es una aplicación directa de la <code class="docutils literal notranslate"><span class="pre">regla</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">cadena</span> <span class="pre">para</span> <span class="pre">las</span> <span class="pre">derivadas</span></code>, y comienza con el paso inicial de <code class="docutils literal notranslate"><span class="pre">calcular</span> <span class="pre">las</span> <span class="pre">derivadas</span> <span class="pre">asociadas</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">última</span> <span class="pre">capa</span> <span class="pre">(de</span> <span class="pre">salida)</span></code>, que resulta ser sencillo. A continuación, el algoritmo “fluye” hacia atrás en la jerarquía de capas. Esto se debe a la naturaleza de la red multicapa, donde las <code class="docutils literal notranslate"><span class="pre">salidas,</span> <span class="pre">capa</span> <span class="pre">tras</span> <span class="pre">capa,</span> <span class="pre">se</span> <span class="pre">forman</span> <span class="pre">como</span> <span class="pre">funciones</span> <span class="pre">de</span> <span class="pre">funciones</span></code>. En efecto, centrémonos en la salida <span class="math notranslate nohighlight">\(y_{k}^{r}\)</span> de la neurona <span class="math notranslate nohighlight">\(k\)</span> en la capa <span class="math notranslate nohighlight">\(r\)</span>. Entonces tenemos</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    y_{k}^{r}=f(\boldsymbol{\theta}_{k}^{r^T}\boldsymbol{y}^{r-1}),\quad k=1,2,\dots, k_{r},
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\boldsymbol{y}^{r-1}\)</span> es el vector (ampliado) que comprende todas las salidas de la capa anterior, <span class="math notranslate nohighlight">\(r-1\)</span>, y <span class="math notranslate nohighlight">\(f\)</span> denota la no-linealidad.</p>
</li>
</ul>
<ul>
<li><p>De acuerdo con lo anterior, la salida de la <span class="math notranslate nohighlight">\(j\)</span> th neurona en la siguiente capa viene dada por</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    y_{j}^{r+1}=f(\boldsymbol{\theta}_{j}^{r+1^T}\boldsymbol{y}^{r})=f\left(\boldsymbol{\theta}_{j}^{r+1^{T}}
    \begin{bmatrix}
    1\\
    f(\Theta^{r}\boldsymbol{y}^{r-1})
    \end{bmatrix}
    \right),
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\Theta^{r}:=[\boldsymbol{\theta}_{1}^{r}, \boldsymbol{\theta}_{2}^{r},\dots,\boldsymbol{\theta}_{k_{r}}]^{T}\)</span> denota la matriz cuyas columnas corresponden al vector de pesos en el layer <span class="math notranslate nohighlight">\(r\)</span>.</p>
</li>
</ul>
<ul class="simple">
<li><p>Nótese que obtuvimos <code class="docutils literal notranslate"><span class="pre">evaluación</span> <span class="pre">de</span> <span class="pre">&quot;una</span> <span class="pre">función</span> <span class="pre">interna</span> <span class="pre">bajo</span> <span class="pre">una</span> <span class="pre">función</span> <span class="pre">externa&quot;</span></code>. Claramente, esto continúa a medida que avanzamos en la jerarquía. Esta <code class="docutils literal notranslate"><span class="pre">estructura</span> <span class="pre">de</span> <span class="pre">evaluación</span> <span class="pre">de</span> <span class="pre">funciones</span> <span class="pre">internas</span> <span class="pre">por</span> <span class="pre">funciones</span> <span class="pre">externas</span></code>, es el subproducto de la <code class="docutils literal notranslate"><span class="pre">naturaleza</span> <span class="pre">multicapa</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">redes</span> <span class="pre">neuronales,</span> <span class="pre">la</span> <span class="pre">cual</span> <span class="pre">es</span> <span class="pre">una</span> <span class="pre">operación</span> <span class="pre">altamente</span> <span class="pre">no</span> <span class="pre">lineal</span></code>, que da lugar a la dificultad de calcular los gradientes, a diferencia de otros modelos, como por ejemplo <code class="docutils literal notranslate"><span class="pre">SVM</span></code>. Sin embargo, se puede observar fácilmente que <strong><code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">cálculo</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">parámetros</span> <span class="pre">que</span> <span class="pre">definen</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">de</span> <span class="pre">salida</span> <span class="pre">no</span> <span class="pre">plantea</span> <span class="pre">ninguna</span> <span class="pre">dificultad</span></code></strong>. En efecto, la salida de la <span class="math notranslate nohighlight">\(j\)</span> th neurona de la última capa (que es en realidad la respectiva estimación de salida actual) se escribe como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{y}_{j}:=y_{j}^{L}=f(\boldsymbol{\theta}_{j}^{L^{T}}\boldsymbol{y}^{L-1}).
\]</div>
<ul class="simple">
<li><p>Dado que <span class="math notranslate nohighlight">\(\boldsymbol{y}^{L-1}\)</span> es conocido, después de los cálculos durante el paso adelante, tomando la derivada con respecto a <span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{L}\)</span> es sencillo; <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">hay</span> <span class="pre">ninguna</span> <span class="pre">operación</span> <span class="pre">de</span> <span class="pre">función</span> <span class="pre">sobre</span> <span class="pre">función</span></code>. Por esto es que <code class="docutils literal notranslate"><span class="pre">empezamos</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">superior</span> <span class="pre">y</span> <span class="pre">luego</span> <span class="pre">nos</span> <span class="pre">movemos</span> <span class="pre">hacia</span> <span class="pre">atrás</span></code>. Debido a su <code class="docutils literal notranslate"><span class="pre">importancia</span> <span class="pre">histórica</span></code>, se dará la derivación completa del algoritmo <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code>.</p></li>
</ul>
<ul>
<li><p>Para la derivación detallada del algoritmo backpropagation, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">adopta</span> <span class="pre">como</span> <span class="pre">ejemplo</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span> <span class="pre">del</span> <span class="pre">error</span> <span class="pre">cuadrático</span></code>, es decir</p>
<div class="math notranslate nohighlight" id="equation-gradient-desc-scheme">
<span class="eqno">(75)<a class="headerlink" href="#equation-gradient-desc-scheme" title="Link to this equation">#</a></span>\[
    J(\boldsymbol{\theta})=\sum_{n=1}^{N}J_{n}(\boldsymbol{\theta})\quad\text{y}\quad J_{n}(\boldsymbol{\theta})=\frac{1}{2}\sum_{k=1}^{k_{L}}(\hat{y}_{nk}-y_{nk})^{2},
    \]</div>
<p>donde <span class="math notranslate nohighlight">\(\hat{y}_{nk},~k=1,2,\dots,k_{L}\)</span>, son las estimaciones proporcionadas en los correspondientes nodos de salida de la red. Las consideraremos como los elementos de un vector correspondiente, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>.</p>
</li>
</ul>
</section>
<section id="calculo-de-gradientes">
<h2>Cálculo de gradientes<a class="headerlink" href="#calculo-de-gradientes" title="Link to this heading">#</a></h2>
<ul>
<li><p>Sea <span class="math notranslate nohighlight">\(z_{nj}^{r}\)</span> la <code class="docutils literal notranslate"><span class="pre">salida</span> <span class="pre">del</span> <span class="pre">combinador</span> <span class="pre">lineal</span></code> de la <span class="math notranslate nohighlight">\(j\)</span>-th neurona en la capa <span class="math notranslate nohighlight">\(r\)</span> en el <code class="docutils literal notranslate"><span class="pre">instante</span> <span class="pre">de</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(n\)</span>, cuando se aplica el patrón <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> en los nodos de entrada (véase la <a class="reference internal" href="#synaptic-weights-link"><span class="std std-numref">Fig. 14</span></a>). Entonces, para <span class="math notranslate nohighlight">\(n, j\)</span> fijos, podemos escribir</p>
<div class="math notranslate nohighlight" id="equation-eq-znj">
<span class="eqno">(76)<a class="headerlink" href="#equation-eq-znj" title="Link to this equation">#</a></span>\[
    z_{nj}^{r}=\sum_{m=1}^{k_{r-1}}\theta_{jm}^{r}y_{nm}^{r-1}+\theta_{j0}^{r}=\sum_{m=0}^{k_{r-1}}\theta_{jm}^{r}y_{nm}^{r-1}=\boldsymbol{\theta}_{j}^{r^{T}}\boldsymbol{y}_{n}^{r-1},
    \]</div>
<p>donde por definición</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{y}_{n}^{r-1}:=[1, y_{n1}^{r-1},\dots, y_{nk_{r-1}}^{r-1}]^{T},
    \]</div>
<p>y <span class="math notranslate nohighlight">\(y_{n0}^{r}\equiv 1,~\forall~r, n\)</span> y <span class="math notranslate nohighlight">\(\theta_{j}^{r}\)</span> ha sido definido en la Ecuación <a class="reference internal" href="#equation-parameters-vector-def">(73)</a>.</p>
</li>
</ul>
<ul class="simple">
<li><p>Para las neuronas de la capa de salida <span class="math notranslate nohighlight">\(r=L,~y_{nm}^{L}=\hat{y}_{nm},~m=1,2,\dots, k_{L}\)</span>, y para <span class="math notranslate nohighlight">\(r=1\)</span>, tenemos <span class="math notranslate nohighlight">\(y_{nm}^{1}=x_{nm},~m=1,2,\dots, k_{1}\)</span>; esto es, <span class="math notranslate nohighlight">\(y_{nm}^{1}\)</span> se fijan iguales a los <code class="docutils literal notranslate"><span class="pre">valores</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">características</span> <span class="pre">de</span> <span class="pre">entrada</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>Por lo tanto, podemos escribir ahora</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial J_{n}}{\partial\boldsymbol{\theta}_{j}^{r}}=\frac{\partial J_{n}}{\partial z_{nj}^{r}}\frac{\partial z_{nj}^{r}}{\partial\boldsymbol{\theta}_{j}^{r}}=\frac{\partial J_{n}}{\partial z_{nj}^{r}}\boldsymbol{y}_{n}^{r-1}.
\]</div>
<ul class="simple">
<li><p>Definamos</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\delta_{nj}^{r}:=\frac{\partial J_{n}}{\partial z_{nj}^{r}}.
\]</div>
<ul class="simple">
<li><p>Entonces la Ecuación <a class="reference internal" href="#equation-update-equations-gd">(74)</a> puede escribirse como</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-delta-theta-jr">
<span class="eqno">(77)<a class="headerlink" href="#equation-eq-delta-theta-jr" title="Link to this equation">#</a></span>\[
\Delta\boldsymbol{\theta}_{j}^{r}=\left.-\mu\frac{\partial J}{\partial\boldsymbol{\theta}_{j}^{r}}\right|_{\boldsymbol{\theta}_{j}^{r}(\text{old})}=-\mu\frac{\partial}{\partial\boldsymbol{\theta}_{j}^{r}}\left.\sum_{n=1}^{N}J_{n}\right|_{\boldsymbol{\theta}_{j}^{r}(\text{old})}=-\mu\sum_{n=1}^{N}\delta_{nj}^{r}\boldsymbol{y}_{n}^{r-1},\quad r=1,2,\dots,L.
\]</div>
</section>
<section id="calculo-de-delta-nj-r">
<h2>Cálculo de <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span><a class="headerlink" href="#calculo-de-delta-nj-r" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Este es el <code class="docutils literal notranslate"><span class="pre">cálculo</span> <span class="pre">principal</span> <span class="pre">del</span> <span class="pre">algoritmo</span> <span class="pre">backpropagation</span></code>. Para el cálculo de los gradientes, <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span>, se <code class="docutils literal notranslate"><span class="pre">comienza</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">última</span> <span class="pre">capa</span></code>, <span class="math notranslate nohighlight">\(r = L\)</span>, y se <code class="docutils literal notranslate"><span class="pre">procede</span> <span class="pre">hacia</span> <span class="pre">atrás</span></code>, hacia <span class="math notranslate nohighlight">\(r = 1\)</span>; esta “filosofía” justifica el nombre dado al algoritmo.</p></li>
</ul>
<ol class="arabic">
<li><p><span class="math notranslate nohighlight">\(r=L\)</span>: Tenemos que</p>
<div class="math notranslate nohighlight">
\[
    \delta_{nj}^{L}:=\frac{\partial J_{n}}{\partial z_{nj}^{L}}.
    \]</div>
<p>Para la función de pérdida del <code class="docutils literal notranslate"><span class="pre">error</span> <span class="pre">al</span> <span class="pre">cuadrado</span></code>,</p>
<div class="math notranslate nohighlight">
\[
    J_{n}=\frac{1}{2}\sum_{k=1}^{k_{L}}\left(\hat{y}_{nk}-y_{nk}\right)^{2}=\frac{1}{2}\sum_{k=1}^{k_{L}}\left(f(z_{nk}^{L})-y_{nk}\right)^{2}.
    \]</div>
<p>Por lo tanto,</p>
<div class="math notranslate nohighlight" id="equation-eq-delta-njl">
<span class="eqno">(78)<a class="headerlink" href="#equation-eq-delta-njl" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \delta_{nj}^{L}=\frac{\partial}{\partial z_{nj}^{L}}\left(\frac{1}{2}\sum_{k=1}^{k_{L}}\left(f(z_{nk}^{L})-y_{nk}\right)^{2}\right)&amp;=(f(z_{nj}^{L})-y_{nj})f'(z_{nj}^{L})\\
    &amp;=(\hat{y}_{nj}-y_{nj})f'(z_{nj}^{L})=e_{nj}f'(z_{nj}^{L}),
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(j=1,2,\dots, k_{L}\)</span>, <span class="math notranslate nohighlight">\(f'\)</span> denota la derivada de <span class="math notranslate nohighlight">\(f\)</span> y <span class="math notranslate nohighlight">\(e_{nj}\)</span> es el error asociado con el <span class="math notranslate nohighlight">\(j\)</span> th output en el tiempo <span class="math notranslate nohighlight">\(n\)</span>. <code class="docutils literal notranslate"><span class="pre">Nótese</span> <span class="pre">que</span> <span class="pre">para</span> <span class="pre">el</span> <span class="pre">último</span> <span class="pre">layer,</span> <span class="pre">el</span> <span class="pre">cálculo</span> <span class="pre">del</span> <span class="pre">gradiente</span></code>, <span class="math notranslate nohighlight">\(\delta_{nj}^{L}\)</span> <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">sencillo</span></code>.</p>
</li>
</ol>
<ul>
<li><p>Manteniendo la misma notación en la Ecuación <a class="reference internal" href="#equation-eq-delta-njl">(78)</a>, definimos</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    e_{nj}^{r-1}:=\sum_{k=1}^{k_{r}}\delta_{nk}^{r}\theta_{kj}^{r},
    \end{split}\]</div>
<p>y finalmente obtenemos,</p>
<div class="math notranslate nohighlight" id="equation-eq-delta-njr-1">
<span class="eqno">(79)<a class="headerlink" href="#equation-eq-delta-njr-1" title="Link to this equation">#</a></span>\[
    \delta_{nj}^{r-1}=e_{nj}^{r-1}f'(z_{nj}^{r-1}).
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>El único cálculo que queda es la derivada de <span class="math notranslate nohighlight">\(f\)</span>. Para el caso de la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">sigmoidea</span> <span class="pre">logística</span></code> se demuestra fácilmente que es igual a</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f'(z)=af(z)(1-f(z)).
\]</div>
<ul class="simple">
<li><p>La derivación se ha completado y el esquema <code class="docutils literal notranslate"><span class="pre">backpropagation</span> <span class="pre">neural</span> <span class="pre">network</span></code> se resume en el siguiente algoritmo</p></li>
</ul>
<div class="proof algorithm admonition" id="my_algorithm_backpropagation">
<p class="admonition-title"><span class="caption-number">Algorithm 2 </span> (Algoritmo Backpropagation Gradiente Descendiente)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inicialización</strong></p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Inicializar</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">y</span> <span class="pre">sesgos</span> <span class="pre">sinápticos</span> <span class="pre">al</span> <span class="pre">azar</span> <span class="pre">con</span> <span class="pre">valores</span> <span class="pre">pequeños</span></code>, pero no muy pequeños.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Seleccione</span> <span class="pre">el</span> <span class="pre">tamaño</span> <span class="pre">del</span> <span class="pre">paso</span></code> <span class="math notranslate nohighlight">\(\mu\)</span>.</p></li>
<li><p>Fije <span class="math notranslate nohighlight">\(y_{nj}^{1}=x_{nj},\quad j=1,2,\dots,k_{1}:=l,\quad n=1,2,\dots,N\)</span></p></li>
</ol>
<p><strong>Repeat</strong> Cada repetición completa un <code class="docutils literal notranslate"><span class="pre">epoch</span></code></p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(n=1,2,\dots,N\)</span> <strong>Do</strong></p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r=1,2,\dots,L\)</span> <strong>Do</strong> Cálculo <code class="docutils literal notranslate"><span class="pre">Forward</span></code></p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j=1,2,\dots,k_{r}\)</span> <strong>Do</strong></p>
<p>Calcule <span class="math notranslate nohighlight">\(z_{nj}^{r}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-znj">(76)</a>
Calcule <span class="math notranslate nohighlight">\(y_{nj}^{r}=f(z_{nj}^{r})\)</span></p>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j = 1, 2,\dots, k_{L}\)</span>, <strong>Do</strong>; Cálculo <code class="docutils literal notranslate"><span class="pre">Backward</span></code> (<code class="docutils literal notranslate"><span class="pre">output</span> <span class="pre">layer</span></code>)</p>
<p>Calcule <span class="math notranslate nohighlight">\(\delta_{nj}^{L}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-delta-njr-1">(79)</a></p>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r=L, L-1,\dots, 2\)</span>, <strong>Do</strong>; Cálculo <code class="docutils literal notranslate"><span class="pre">Backward</span></code> (<code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">layers</span></code>)</p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j=1,2,\dots, k_{r}\)</span>, <strong>Do</strong></p>
<p>Calcule <span class="math notranslate nohighlight">\(\delta_{nj}^{r-1}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-delta-njr-1">(79)</a></p>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(r=1,2,\dots,L\)</span>, <strong>Do</strong>: Actualice los pesos</p>
<ol class="arabic">
<li><p><strong>For</strong> <span class="math notranslate nohighlight">\(j=1,2,\dots,k_{r}\)</span>, <strong>Do</strong></p>
<p>Calcule <span class="math notranslate nohighlight">\(\Delta\boldsymbol{\theta}_{j}^{r}\)</span> a partir de la Ecuación <a class="reference internal" href="#equation-eq-delta-theta-jr">(77)</a></p>
<p><span class="math notranslate nohighlight">\(\boldsymbol{\theta}_{j}^{r}=\boldsymbol{\theta}_{j}^{r}+\Delta\boldsymbol{\theta}_{j}^{r}\)</span></p>
</li>
<li><p><strong>End For</strong></p></li>
</ol>
</li>
<li><p><strong>End For</strong></p></li>
<li><p><strong>Until</strong> Un criterio de parada se cumpla.</p></li>
</ol>
</section>
</div><ul class="simple">
<li><p>El algoritmo de <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code> puede reivindicar una serie de padres. La popularización del algoritmo se asocia con el artículo clásico <span id="id3">[<a class="reference internal" href="biblio.html#id33" title="David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-propagating errors. nature, 323(6088):533–536, 1986.">Rumelhart <em>et al.</em>, 1986</a>]</span>, donde se proporciona la derivación del algoritmo. La idea de <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code> también aparece en <span id="id4">[<a class="reference internal" href="biblio.html#id34" title="Arthur E Bryson Jr, Walter F Denham, and Stewart E Dreyfus. Optimal programming problems with inequality constraints. AIAA journal, 1(11):2544–2550, 1963.">Bryson Jr <em>et al.</em>, 1963</a>]</span> en el contexto del control óptimo.</p></li>
<li><p>Existen diferentes variaciones del algoritmo <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code>, tales como: <code class="docutils literal notranslate"><span class="pre">Gradiende</span> <span class="pre">descendiente</span> <span class="pre">con</span> <span class="pre">término</span> <span class="pre">de</span> <span class="pre">momento,</span> <span class="pre">Algoritmo</span> <span class="pre">de</span> <span class="pre">momentos</span> <span class="pre">de</span> <span class="pre">Nesterov's,</span> <span class="pre">Algoritmo</span> <span class="pre">AdaGrad,</span> <span class="pre">RMSProp</span> <span class="pre">con</span> <span class="pre">momento</span> <span class="pre">de</span> <span class="pre">Nesterov,</span> <span class="pre">Algortimo</span> <span class="pre">de</span> <span class="pre">estimación</span> <span class="pre">de</span> <span class="pre">momentos</span> <span class="pre">adaptativo</span></code> los cuales pueden ser utlizados para resolver la tarea de optimización (ver <span id="id5">[<a class="reference internal" href="biblio.html#id31" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>).</p></li>
</ul>
</section>
<section id="las-capas-ocultas">
<h2>Las capas ocultas<a class="headerlink" href="#las-capas-ocultas" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">¿Cuántas</span> <span class="pre">capas</span> <span class="pre">ocultas?</span></code>. Si los datos son <code class="docutils literal notranslate"><span class="pre">linealmente</span> <span class="pre">separables</span> <span class="pre">(lo</span> <span class="pre">que</span> <span class="pre">a</span> <span class="pre">menudo</span> <span class="pre">se</span> <span class="pre">sabe</span> <span class="pre">cuando</span> <span class="pre">se</span> <span class="pre">empieza</span> <span class="pre">a</span> <span class="pre">codificar</span> <span class="pre">una</span> <span class="pre">ANN,</span> <span class="pre">SVM</span> <span class="pre">puede</span> <span class="pre">servir</span> <span class="pre">de</span> <span class="pre">test)</span></code>, entonces no se necesita ninguna capa oculta. Por supuesto, tampoco se necesita una ANN para resolver los datos, pero está seguirá haciendo su trabajo.</p></li>
<li><p>Sobre la configuración de las capas ocultas en las ANNs, existe un consenso dentro de este tema, y es la diferencia de rendimiento al añadir capas ocultas adicionales: <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">situaciones</span> <span class="pre">en</span> <span class="pre">las</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">rendimiento</span> <span class="pre">mejora</span> <span class="pre">con</span> <span class="pre">una</span> <span class="pre">segunda</span> <span class="pre">(o</span> <span class="pre">tercera,</span> <span class="pre">etc.)</span> <span class="pre">capa</span> <span class="pre">oculta</span> <span class="pre">son</span> <span class="pre">muy</span> <span class="pre">pocas</span></code>. Una capa oculta es suficiente para la gran mayoría de los problemas.</p></li>
<li><p>Entonces, <code class="docutils literal notranslate"><span class="pre">¿qué</span> <span class="pre">pasa</span> <span class="pre">con</span> <span class="pre">el</span> <span class="pre">tamaño</span> <span class="pre">de</span> <span class="pre">la(s)</span> <span class="pre">capa(s)</span> <span class="pre">oculta(s),</span> <span class="pre">cuántas</span> <span class="pre">neuronas?</span></code>. Existen algunas reglas empíricas; de ellas, la más utilizada es <strong><code class="docutils literal notranslate"><span class="pre">'The</span> <span class="pre">optimal</span> <span class="pre">size</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">hidden</span> <span class="pre">layer</span> <span class="pre">is</span> <span class="pre">usually</span> <span class="pre">between</span> <span class="pre">the</span> <span class="pre">size</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">input</span> <span class="pre">and</span> <span class="pre">size</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">output</span> <span class="pre">layers'</span></code></strong>. <code class="docutils literal notranslate"><span class="pre">Jeff</span> <span class="pre">Heaton,</span> <span class="pre">the</span> <span class="pre">author</span> <span class="pre">of</span> <span class="pre">Introduction</span> <span class="pre">to</span> <span class="pre">Neural</span> <span class="pre">Networks</span> <span class="pre">in</span> <span class="pre">Java</span></code>.</p></li>
</ul>
<ul>
<li><p>Hay una regla empírica adicional que ayuda en los problemas de aprendizaje supervisado. Normalmente <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">puede</span> <span class="pre">evitar</span> <span class="pre">el</span> <span class="pre">sobreajuste</span> <span class="pre">si</span> <span class="pre">se</span> <span class="pre">mantiene</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">neuronas</span> <span class="pre">por</span> <span class="pre">debajo</span> <span class="pre">de</span></code>:</p>
<div class="math notranslate nohighlight">
\[
    N_{h}=\frac{N_{s}}{(\alpha\cdot(N_{i}+N_{o}))}
    \]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N_{i}=\)</span> número de neuronas de entrada</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{o}=\)</span> número de neuronas de salida</p></li>
<li><p><span class="math notranslate nohighlight">\(N_{s}=\)</span> número de muestras en el conjunto de datos de entrenamiento</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha=\)</span> un factor de escala arbitrario, normalmente 2-10</p></li>
</ul>
</li>
<li><p>Un valor de <span class="math notranslate nohighlight">\(\alpha=2\)</span> suele funcionar <code class="docutils literal notranslate"><span class="pre">sin</span> <span class="pre">sobreajustar</span></code>. Se puede pensar en <span class="math notranslate nohighlight">\(\alpha\)</span> como el <code class="docutils literal notranslate"><span class="pre">factor</span> <span class="pre">de</span> <span class="pre">ramificación</span> <span class="pre">efectivo</span> <span class="pre">o</span> <span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">pesos</span> <span class="pre">distintos</span> <span class="pre">de</span> <span class="pre">cero</span> <span class="pre">para</span> <span class="pre">cada</span> <span class="pre">neurona</span></code>. Las capas de salida harán que el factor de ramificación “efectivo” sea muy inferior al factor de ramificación medio real de la red. Para <code class="docutils literal notranslate"><span class="pre">profundizar</span> <span class="pre">mas</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">diseño</span> <span class="pre">de</span> <span class="pre">redes</span> <span class="pre">neuronales,</span> <span class="pre">ver</span> <span class="pre">el</span> <span class="pre">siguiente</span> <span class="pre">texto</span> <span class="pre">de</span></code> <a class="reference external" href="https://hagan.okstate.edu/nnd.html">Martin Hagan</a>.</p></li>
</ul>
<ul class="simple">
<li><p>En resumen, para la mayoría de los problemas, probablemente se podría obtener un rendimiento decente (incluso sin un segundo paso de optimización) estableciendo la configuración de la capa oculta utilizando sólo dos reglas:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">capas</span> <span class="pre">ocultas</span> <span class="pre">es</span> <span class="pre">igual</span> <span class="pre">a</span> <span class="pre">uno</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">neuronas</span> <span class="pre">de</span> <span class="pre">esa</span> <span class="pre">capa</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">media</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">neuronas</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">capas</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">y</span> <span class="pre">salida.</span></code> ¡Nótese que en el ejemplo de esta sección el número de columnas para <span class="math notranslate nohighlight">\(X\)</span> es 100!.</p></li>
</ul>
</li>
</ul>
</section>
<section id="redes-neuronales-recurrentes">
<h2>Redes Neuronales Recurrentes<a class="headerlink" href="#redes-neuronales-recurrentes" title="Link to this heading">#</a></h2>
<div class="admonition-introduccion admonition">
<p class="admonition-title">Introducción</p>
<ul class="simple">
<li><p>Recordemos de la sección anterior que en el corazón de las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">convolucionales</span></code> se encuentra el concepto de <code class="docutils literal notranslate"><span class="pre">peso</span> <span class="pre">compartido</span></code>. Es decir, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">misma</span> <span class="pre">matriz</span> <span class="pre">de</span> <span class="pre">filtro</span> <span class="pre">se</span> <span class="pre">desliza</span> <span class="pre">sobre</span> <span class="pre">una</span> <span class="pre">matriz</span> <span class="pre">de</span> <span class="pre">imágenes</span> <span class="pre">en</span> <span class="pre">lugar</span> <span class="pre">de</span> <span class="pre">dedicar</span> <span class="pre">un</span> <span class="pre">peso</span> <span class="pre">específico</span> <span class="pre">a</span> <span class="pre">cada</span> <span class="pre">píxel</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">imagen</span></code>. De este modo, una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">neuronal</span></code> puede <code class="docutils literal notranslate"><span class="pre">escalarse</span> <span class="pre">fácilmente</span> <span class="pre">a</span> <span class="pre">imágenes</span> <span class="pre">de</span> <span class="pre">diferentes</span> <span class="pre">dimensiones</span></code>.</p></li>
<li><p>Nuestro interés en esta sección se centra en el caso de los <code class="docutils literal notranslate"><span class="pre">datos</span> <span class="pre">secuenciales</span></code>. Es decir, los <code class="docutils literal notranslate"><span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">no</span> <span class="pre">son</span> <span class="pre">independientes,</span> <span class="pre">sino</span> <span class="pre">que</span> <span class="pre">aparecen</span> <span class="pre">en</span> <span class="pre">secuencia</span></code>. Además, el <code class="docutils literal notranslate"><span class="pre">orden</span> <span class="pre">específico</span> <span class="pre">en</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">producen</span> <span class="pre">encierra</span> <span class="pre">información</span> <span class="pre">importante</span></code>. Por ejemplo, este tipo de secuencias se dan en el <strong><code class="docutils literal notranslate"><span class="pre">reconocimiento</span> <span class="pre">del</span> <span class="pre">habla</span> <span class="pre">y</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">procesamiento</span> <span class="pre">del</span> <span class="pre">lenguaje,</span> <span class="pre">como</span> <span class="pre">la</span> <span class="pre">traducción</span> <span class="pre">automática,</span> <span class="pre">así</span> <span class="pre">como</span> <span class="pre">también</span> <span class="pre">el</span> <span class="pre">pronostico</span> <span class="pre">de</span> <span class="pre">series</span> <span class="pre">de</span> <span class="pre">tiempo</span> <span class="pre">financieras</span></code></strong>. Sin duda, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">secuencia</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">producen</span> <span class="pre">las</span> <span class="pre">palabras</span> <span class="pre">es</span> <span class="pre">de</span> <span class="pre">suma</span> <span class="pre">importancia</span></code>.</p></li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">reparto</span> <span class="pre">de</span> <span class="pre">pesos</span> <span class="pre">mediante</span> <span class="pre">convoluciones</span> <span class="pre">también</span> <span class="pre">podría</span> <span class="pre">ser</span> <span class="pre">y</span> <span class="pre">ha</span> <span class="pre">sido</span> <span class="pre">utilizado</span> <span class="pre">para</span> <span class="pre">estos</span> <span class="pre">casos</span></code> <span id="id6">[<a class="reference internal" href="biblio.html#id35" title="Kevin J Lang, Alex H Waibel, and Geoffrey E Hinton. A time-delay neural network architecture for isolated word recognition. Neural networks, 3(1):23–43, 1990.">Lang <em>et al.</em>, 1990</a>]</span>. Tales redes se conocen como <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">neuronales</span> <span class="pre">de</span> <span class="pre">retardo</span> <span class="pre">temporal</span></code>. Sin embargo, <code class="docutils literal notranslate"><span class="pre">deslizar</span> <span class="pre">un</span> <span class="pre">filtro</span> <span class="pre">a</span> <span class="pre">través</span> <span class="pre">del</span> <span class="pre">tiempo</span></code> para formar convoluciones es una <code class="docutils literal notranslate"><span class="pre">operación</span> <span class="pre">de</span> <span class="pre">naturaleza</span> <span class="pre">local</span></code>. La salida es una función de las muestras de entrada dentro de una ventana temporal que abarca la <code class="docutils literal notranslate"><span class="pre">longitud</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">respuesta</span> <span class="pre">al</span> <span class="pre">impulso</span> <span class="pre">del</span> <span class="pre">filtro</span></code>, que por razones prácticas no puede ser muy larga.</p></li>
</ul>
</div>
<div class="admonition-redes-neuronales-recurrentes admonition">
<p class="admonition-title">Redes Neuronales Recurrentes</p>
<ul>
<li><p>Las variables que intervienen en una <code class="docutils literal notranslate"><span class="pre">RNN</span></code> son:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Vector</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(n\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">denotado</span> <span class="pre">como</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. El símbolo nos recuerda que <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> es un vector de variables ocultas (capa oculta en la jerga de las redes neuronales); <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">vector</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">constituye</span> <span class="pre">la</span> <span class="pre">memoria</span> <span class="pre">del</span> <span class="pre">sistema</span></code>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Vector</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">momento</span></code> <span class="math notranslate nohighlight">\(n\)</span>, denominado <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Vector</span> <span class="pre">de</span> <span class="pre">salida</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">momento</span></code> <span class="math notranslate nohighlight">\(n\)</span>, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>, y el vector de salida objetivo, <span class="math notranslate nohighlight">\(\boldsymbol{y}_{n}\)</span>.</p></li>
</ul>
</li>
<li><p>El modelo se describe mediante un <code class="docutils literal notranslate"><span class="pre">conjunto</span> <span class="pre">de</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">desconocidos</span></code>, a saber, <span class="math notranslate nohighlight">\(U, W, V , \boldsymbol{b}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span>, que <code class="docutils literal notranslate"><span class="pre">deben</span> <span class="pre">aprenderse</span> <span class="pre">durante</span> <span class="pre">el</span> <span class="pre">entrenamiento</span></code>.</p></li>
<li><p>Las <code class="docutils literal notranslate"><span class="pre">ecuaciones</span> <span class="pre">que</span> <span class="pre">describen</span> <span class="pre">un</span> <span class="pre">modelo</span> <span class="pre">RNN</span></code> son</p>
<div class="math notranslate nohighlight" id="equation-rnn-system-eq">
<span class="eqno">(80)<a class="headerlink" href="#equation-rnn-system-eq" title="Link to this equation">#</a></span>\[\begin{split}
    \begin{align*}
    \boldsymbol{h}_{n}&amp;=f(U\boldsymbol{x}_{n}+W\boldsymbol{h}_{n-1}+\boldsymbol{b})\\
    \hat{\boldsymbol{y}}_{n}&amp;=g(V\boldsymbol{h}_{n}+\boldsymbol{c}).
    \end{align*}
    \end{split}\]</div>
<p>donde <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">funciones</span> <span class="pre">no</span> <span class="pre">lineales</span></code> <span class="math notranslate nohighlight">\(f\)</span> y <span class="math notranslate nohighlight">\(g\)</span><code class="docutils literal notranslate"> <span class="pre">actúan</span> <span class="pre">elemento</span> <span class="pre">a</span> <span class="pre">elemento</span> <span class="pre">(element-wise)</span></code> y <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">aplican</span> <span class="pre">individualmente</span> <span class="pre">a</span> <span class="pre">cada</span> <span class="pre">elemento</span> <span class="pre">de</span> <span class="pre">sus</span> <span class="pre">argumentos</span> <span class="pre">vectoriales</span></code>.</p>
</li>
<li><p>En otras palabras, <code class="docutils literal notranslate"><span class="pre">una</span> <span class="pre">vez</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">ha</span> <span class="pre">observado</span> <span class="pre">un</span> <span class="pre">nuevo</span> <span class="pre">vector</span> <span class="pre">de</span> <span class="pre">entrada,</span> <span class="pre">se</span> <span class="pre">actualiza</span> <span class="pre">el</span> <span class="pre">vector</span> <span class="pre">de</span> <span class="pre">estado</span></code>. Su nuevo valor <code class="docutils literal notranslate"><span class="pre">depende</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">información</span> <span class="pre">más</span> <span class="pre">reciente,</span> <span class="pre">transmitida</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">entrada</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> así como de la <code class="docutils literal notranslate"><span class="pre">historia</span> <span class="pre">pasada,</span> <span class="pre">ya</span> <span class="pre">que</span> <span class="pre">ésta</span> <span class="pre">se</span> <span class="pre">ha</span> <span class="pre">acumulado</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n-1}\)</span>. La salida depende del <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">actualizado</span></code>, <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. Es decir, <code class="docutils literal notranslate"><span class="pre">depende</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">&quot;historia&quot;</span> <span class="pre">hasta</span> <span class="pre">el</span> <span class="pre">instante</span> <span class="pre">actual</span></code> <span class="math notranslate nohighlight">\(n\)</span>, tal y como se expresa en <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>.</p></li>
<li><p>Las opciones típicas para <span class="math notranslate nohighlight">\(f\)</span> son la <code class="docutils literal notranslate"><span class="pre">tangente</span> <span class="pre">hiperbólica,</span> <span class="pre">tanh,</span> <span class="pre">o</span> <span class="pre">las</span> <span class="pre">no</span> <span class="pre">linealidades</span> <span class="pre">ReLU</span></code>. El valor inicial <span class="math notranslate nohighlight">\(\boldsymbol{h}_{0}\)</span> <code class="docutils literal notranslate"><span class="pre">suele</span> <span class="pre">ser</span> <span class="pre">igual</span> <span class="pre">al</span> <span class="pre">vector</span> <span class="pre">cero</span></code>. La <code class="docutils literal notranslate"><span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">de</span> <span class="pre">salida,</span></code> <span class="math notranslate nohighlight">\(g\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">se</span> <span class="pre">elige</span> <span class="pre">a</span> <span class="pre">menudo</span> <span class="pre">para</span> <span class="pre">ser</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">softmax</span></code>.</p></li>
</ul>
</div>
<figure class="align-center" id="recurrent-neural-network-arch-numref">
<a class="reference internal image-reference" href="_images/recurrent_neural_network_arch.png"><img alt="_images/recurrent_neural_network_arch.png" src="_images/recurrent_neural_network_arch.png" style="width: 552.0px; height: 304.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Arquitectura de una <code class="docutils literal notranslate"><span class="pre">Red</span> <span class="pre">Neuronal</span> <span class="pre">Recurrente</span></code>. Fuente <span id="id7">[<a class="reference internal" href="biblio.html#id31" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#recurrent-neural-network-arch-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>De las ecuaciones anteriores se deduce que las <code class="docutils literal notranslate"><span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">se</span> <span class="pre">comparten</span> <span class="pre">en</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">instantes</span> <span class="pre">temporales</span></code>. Durante el entrenamiento, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">inicializan</span> <span class="pre">mediante</span> <span class="pre">números</span> <span class="pre">aleatorios</span></code>. El modelo gráfico
asociado con Eq. <a class="reference internal" href="#equation-rnn-system-eq">(80)</a> se muestra en la <a class="reference internal" href="#recurrent-neural-network-arch-numref"><span class="std std-numref">Fig. 15</span></a>A. En la <a class="reference internal" href="#recurrent-neural-network-arch-numref"><span class="std std-numref">Fig. 15</span></a>B, <code class="docutils literal notranslate"><span class="pre">el</span> <span class="pre">gráfico</span> <span class="pre">se</span> <span class="pre">despliega</span> <span class="pre">sobre</span> <span class="pre">los</span> <span class="pre">distintos</span> <span class="pre">instantes</span> <span class="pre">de</span> <span class="pre">tiempo</span></code> para los que se dispone de observaciones. Por ejemplo, <code class="docutils literal notranslate"><span class="pre">si</span> <span class="pre">la</span> <span class="pre">secuencia</span> <span class="pre">de</span> <span class="pre">interés</span> <span class="pre">es</span> <span class="pre">una</span> <span class="pre">frase</span> <span class="pre">de</span> <span class="pre">10</span> <span class="pre">palabras</span></code>, entonces <span class="math notranslate nohighlight">\(N\)</span> se establece igual a 10, mientras que <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span> es el <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">que</span> <span class="pre">codifica</span> <span class="pre">las</span> <span class="pre">respectivas</span> <span class="pre">palabras</span> <span class="pre">de</span> <span class="pre">entrada</span></code>.</p></li>
</ul>
<section id="backpropagation-en-tiempo">
<h3>Backpropagation en tiempo<a class="headerlink" href="#backpropagation-en-tiempo" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>El entrenamiento de las <code class="docutils literal notranslate"><span class="pre">RNN</span></code> sigue una <code class="docutils literal notranslate"><span class="pre">lógica</span> <span class="pre">similar</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">del</span> <span class="pre">algoritmo</span> <span class="pre">backpropagation</span></code> para el entrenamiento de redes neuronales de avance. Después de todo, <code class="docutils literal notranslate"><span class="pre">una</span> <span class="pre">RNN</span> <span class="pre">puede</span> <span class="pre">verse</span> <span class="pre">como</span> <span class="pre">una</span> <span class="pre">red</span> <span class="pre">feed-forward</span> <span class="pre">con</span></code> <span class="math notranslate nohighlight">\(N\)</span> <code class="docutils literal notranslate"><span class="pre">capas</span></code>. La <code class="docutils literal notranslate"><span class="pre">capa</span> <span class="pre">superior</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">del</span> <span class="pre">instante</span> <span class="pre">de</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(N\)</span> y la <code class="docutils literal notranslate"><span class="pre">primera</span> <span class="pre">capa</span> <span class="pre">corresponde</span> <span class="pre">al</span> <span class="pre">instante</span> <span class="pre">de</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(n = 1\)</span>. Una diferencia radica en que las <code class="docutils literal notranslate"><span class="pre">capas</span> <span class="pre">ocultas</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">RNN</span> <span class="pre">también</span> <span class="pre">producen</span> <span class="pre">salidas</span></code>, es decir, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>, y se alimentan directamente con entradas. Sin embargo, <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">lo</span> <span class="pre">que</span> <span class="pre">respecta</span> <span class="pre">al</span> <span class="pre">entrenamiento,</span> <span class="pre">estas</span> <span class="pre">diferencias</span> <span class="pre">no</span> <span class="pre">afectan</span> <span class="pre">al</span> <span class="pre">razonamiento</span> <span class="pre">principal</span></code>.</p></li>
<li><p>El <code class="docutils literal notranslate"><span class="pre">aprendizaje</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">desconocidos</span></code> se consigue mediante un esquema de gradiente descendiente, de acuerdo con Eq. <a class="reference internal" href="#equation-update-equations-gd">(74)</a>. Resulta que los <code class="docutils literal notranslate"><span class="pre">gradientes</span> <span class="pre">requeridos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span></code>, con respecto a los parámetros desconocidos, <code class="docutils literal notranslate"><span class="pre">tienen</span> <span class="pre">lugar</span> <span class="pre">recursivamente,</span> <span class="pre">comenzando</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">último</span> <span class="pre">instante</span> <span class="pre">de</span> <span class="pre">tiempo,</span></code> <span class="math notranslate nohighlight">\(N\)</span> , y retrocediendo en el tiempo, <span class="math notranslate nohighlight">\(n = N-1, N-2,\dots,1\)</span>. Esta es la razón por la que el algoritmo se conoce como <code class="docutils literal notranslate"><span class="pre">bakpropagation</span> <span class="pre">a</span> <span class="pre">traves</span> <span class="pre">del</span> <span class="pre">tiempo</span> <span class="pre">(BPTT)</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">suma</span> <span class="pre">a</span> <span class="pre">lo</span> <span class="pre">largo</span> <span class="pre">del</span> <span class="pre">tiempo,</span></code> <span class="math notranslate nohighlight">\(n\)</span>, de las correspondientes <code class="docutils literal notranslate"><span class="pre">contribuciones</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span></code>, que dependen de los valores respectivos de <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}, \boldsymbol{x}_{n}\)</span>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
J(U, W, V, \boldsymbol{b}, \boldsymbol{c})=\sum_{n=1}^{N}J_{n}(U, W, V, \boldsymbol{b}, \boldsymbol{c}).
\]</div>
<ul>
<li><p>Por ejemplo, para el caso de la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span> <span class="pre">de</span> <span class="pre">entropía</span> <span class="pre">cruzada</span></code>,</p>
<div class="math notranslate nohighlight">
\[
    J_{n}(U, W, V, \boldsymbol{b}, \boldsymbol{c}):=-\sum_{k}y_{nk}\ln\hat{y}_{nk},
    \]</div>
<p>donde <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">suma</span> <span class="pre">es</span> <span class="pre">sobre</span> <span class="pre">la</span> <span class="pre">dimensionalidad</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, y</p>
<div class="math notranslate nohighlight">
\[
    \hat{\boldsymbol{y}}_{n}=g(\boldsymbol{h}_{n}, V, \boldsymbol{c})~\text{y}~\boldsymbol{h}_{n}=f(\boldsymbol{x}_{n}, \boldsymbol{h}_{n-1}, U, W, \boldsymbol{b}).
    \]</div>
</li>
</ul>
<ul>
<li><p>En el corazón del <code class="docutils literal notranslate"><span class="pre">cálculo</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span> <span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">diversas</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span></code> se encuentra el cálculo de los <code class="docutils literal notranslate"><span class="pre">gradientes</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(J\)</span> <code class="docutils literal notranslate"><span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">estado,</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. Una vez calculados estos últimos, el resto de los gradientes, con respecto a las <code class="docutils literal notranslate"><span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">desconocidos</span></code>, es una tarea sencilla. Para ello, nótese que cada <span class="math notranslate nohighlight">\(h_{n},~n=1, 2,\dots,N-1\)</span>, afecta a <span class="math notranslate nohighlight">\(J\)</span> de dos maneras:</p>
<ul class="simple">
<li><p>Directamente, <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">traves</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(J_{n}\)</span></p></li>
<li><p>Indirectamente, <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">través</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">cadena</span> <span class="pre">que</span> <span class="pre">impone</span> <span class="pre">la</span> <span class="pre">estructura</span> <span class="pre">RNN</span></code>, es decir,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{h}_{n}\rightarrow\boldsymbol{h}_{n+1}\rightarrow\cdots\rightarrow\boldsymbol{h}_{N}.
    \]</div>
<p>Es decir, <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>, además de <span class="math notranslate nohighlight">\(J_{n}\)</span>, <code class="docutils literal notranslate"><span class="pre">también</span> <span class="pre">afecta</span> <span class="pre">a</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">de</span> <span class="pre">coste</span> <span class="pre">posteriores</span></code>, <span class="math notranslate nohighlight">\(J_{n+1},\dots, J_{N}\)</span>. Nótese que, a traves de la cadena <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n+1}=f(\boldsymbol{x}_{n+1}, \boldsymbol{h}_{n}, U, W, \boldsymbol{b}).\)</span></p>
</li>
</ul>
<ul>
<li><p>Empleando la <code class="docutils literal notranslate"><span class="pre">regla</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">cadena</span></code> para las derivadas, las <code class="docutils literal notranslate"><span class="pre">dependencias</span> <span class="pre">anteriores</span> <span class="pre">conducen</span> <span class="pre">al</span> <span class="pre">siguiente</span> <span class="pre">cálculo</span> <span class="pre">recursivo</span></code>:</p>
<div class="math notranslate nohighlight" id="equation-indirect-direct-rec-part">
<span class="eqno">(81)<a class="headerlink" href="#equation-indirect-direct-rec-part" title="Link to this equation">#</a></span>\[
    \frac{\partial J}{\partial\boldsymbol{h}_{n}}=\underbrace{{\left(\frac{\partial\boldsymbol{h}_{n+1}}{\partial\boldsymbol{h}_{n}}\right)^{T}}\frac{\partial J}{\partial\boldsymbol{h}_{n+1}}}_{\text{parte recursiva indirecta}}+\underbrace{\left(\frac{\partial\hat{\boldsymbol{y}}_{n}}{\partial\boldsymbol{h}_{n}}\right)^{T}\frac{\partial J}{\partial\hat{\boldsymbol{y}}_{n}}}_{\text{parte directa}},
    \]</div>
<p>donde, por definición, la <code class="docutils literal notranslate"><span class="pre">derivada</span> <span class="pre">de</span> <span class="pre">un</span> <span class="pre">vector</span></code>, digamos, <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, <code class="docutils literal notranslate"><span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">otro</span> <span class="pre">vector</span></code>, digamos, <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, se define como la matriz</p>
<div class="math notranslate nohighlight">
\[
    \left[\frac{\partial\boldsymbol{y}}{\partial\boldsymbol{x}}\right]_{ij}:=\frac{\partial y_{i}}{\partial x_{j}}.
    \]</div>
</li>
</ul>
<ul class="simple">
<li><p>Nótese que el <code class="docutils literal notranslate"><span class="pre">gradiente</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">coste</span></code>, con respecto a los <code class="docutils literal notranslate"><span class="pre">parámetros</span> <span class="pre">ocultos</span> <span class="pre">(vector</span> <span class="pre">de</span> <span class="pre">estado)</span></code> en la capa “<span class="math notranslate nohighlight">\(n\)</span>”, se da como una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">del</span> <span class="pre">gradiente</span> <span class="pre">respectivo</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">anterior</span></code>, es decir, con respecto al <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">de</span> <span class="pre">estado</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">tiempo</span></code> <span class="math notranslate nohighlight">\(n + 1\)</span>. Las dos <code class="docutils literal notranslate"><span class="pre">pasadas</span> <span class="pre">requeridas</span> <span class="pre">por</span> <span class="pre">backpropagation</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">tiempo</span></code> se resumen a continuación.</p></li>
</ul>
<div class="admonition-pasadas-de-backpropagation-en-tiempo admonition">
<p class="admonition-title">Pasadas de Backpropagation en Tiempo</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Paso</span> <span class="pre">hacia</span> <span class="pre">adelante</span></code>:</p>
<ul class="simple">
<li><p>Iniciando en <span class="math notranslate nohighlight">\(n=1\)</span> y <code class="docutils literal notranslate"><span class="pre">utilizando</span> <span class="pre">las</span> <span class="pre">estimaciones</span> <span class="pre">actuales</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">de</span> <span class="pre">parámetros</span> <span class="pre">implicados</span></code>, calcular en secuencia,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    (\boldsymbol{h}_{1}, \hat{\boldsymbol{y}}_{1})\rightarrow(\boldsymbol{h}_{2}, \hat{\boldsymbol{y}}_{2})\rightarrow\cdots\rightarrow(\boldsymbol{h}_{N}, \hat{\boldsymbol{y}}_{N}).
    \]</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">Paso</span> <span class="pre">hacia</span> <span class="pre">atrás</span></code>:</p>
<ul class="simple">
<li><p>Empezando en <span class="math notranslate nohighlight">\(n = N\)</span>, calcular en secuencia,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
    \frac{\partial J}{\partial\boldsymbol{h}_{N}}\rightarrow\frac{\partial J}{\partial\boldsymbol{h}_{N-1}}\rightarrow\cdots\rightarrow\frac{\partial J}{\partial\boldsymbol{h}_{1}}.
    \]</div>
</li>
</ul>
</div>
<ul class="simple">
<li><p>Nótese que el <code class="docutils literal notranslate"><span class="pre">cálculo</span> <span class="pre">del</span> <span class="pre">gradiente</span></code> <span class="math notranslate nohighlight">\(\partial J/\partial\boldsymbol{h}_{N}\)</span> es sencillo, y <code class="docutils literal notranslate"><span class="pre">solo</span> <span class="pre">involucra</span> <span class="pre">la</span> <span class="pre">parte</span> <span class="pre">directa</span> <span class="pre">en</span></code> Eq. <a class="reference internal" href="#equation-indirect-direct-rec-part">(81)</a>.</p></li>
<li><p>Para la implementación de la <code class="docutils literal notranslate"><span class="pre">BPTT</span></code>, se procede a</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">inicializar</span> <span class="pre">aleatoriamente</span> <span class="pre">las</span> <span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">desconocidos</span> <span class="pre">implicados,</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">calcular</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">requeridos,</span> <span class="pre">siguiendo</span> <span class="pre">los</span> <span class="pre">pasos</span> <span class="pre">indicados</span> <span class="pre">anteriormente</span></code>, y</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">realizar</span> <span class="pre">las</span> <span class="pre">actualizaciones</span> <span class="pre">según</span> <span class="pre">el</span> <span class="pre">esquema</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">descendiente</span></code>.</p></li>
</ol>
</li>
<li><p>Los pasos <code class="docutils literal notranslate"><span class="pre">(2)</span> <span class="pre">y</span> <span class="pre">(3)</span> <span class="pre">se</span> <span class="pre">realizan</span> <span class="pre">de</span> <span class="pre">forma</span> <span class="pre">iterativa</span> <span class="pre">hasta</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">cumple</span> <span class="pre">un</span> <span class="pre">criterio</span> <span class="pre">de</span> <span class="pre">convergencia</span></code>, de forma análoga al <code class="docutils literal notranslate"><span class="pre">algoritmo</span> <span class="pre">estándar</span> <span class="pre">de</span> <span class="pre">backpropagation</span></code>.</p></li>
</ul>
</section>
<section id="desvanecimiento-y-explosion-de-gradientes">
<h3>Desvanecimiento y explosión de gradientes<a class="headerlink" href="#desvanecimiento-y-explosion-de-gradientes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>La tarea de <code class="docutils literal notranslate"><span class="pre">desvanecimiento</span> <span class="pre">y</span> <span class="pre">explosión</span> <span class="pre">de</span> <span class="pre">gradientes</span></code> se ha introducido y discutido en secciones anteriores, en el contexto del algoritmo de backpropagation. <code class="docutils literal notranslate"><span class="pre">Los</span> <span class="pre">mismos</span> <span class="pre">problemas</span> <span class="pre">se</span> <span class="pre">presentan</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">algoritmo</span> <span class="pre">BPTT</span></code>, dado que, este último es una <code class="docutils literal notranslate"><span class="pre">forma</span> <span class="pre">específica</span> <span class="pre">del</span> <span class="pre">concepto</span> <span class="pre">de</span> <span class="pre">backpropagation</span></code> y, como se ha dicho, una <code class="docutils literal notranslate"><span class="pre">RNN</span></code> puede considerarse como una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">multicapa,</span> <span class="pre">donde</span> <span class="pre">cada</span> <span class="pre">instante</span> <span class="pre">de</span> <span class="pre">tiempo</span> <span class="pre">corresponde</span> <span class="pre">a</span> <span class="pre">una</span> <span class="pre">capa</span> <span class="pre">diferente</span></code>. De hecho en las <code class="docutils literal notranslate"><span class="pre">RNN</span></code>, el fenómeno de <code class="docutils literal notranslate"><span class="pre">desvanecimiento/explosión</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">aparece</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">forma</span> <span class="pre">bastante</span> <span class="pre">&quot;agresiva&quot;</span></code>, teniendo en cuenta que <span class="math notranslate nohighlight">\(N\)</span> puede alcanzar valores grandes.</p></li>
</ul>
<ul class="simple">
<li><p>La naturaleza multiplicativa de la propagación de gradientes puede verse fácilmente en la Eq. <a class="reference internal" href="#equation-indirect-direct-rec-part">(81)</a>. Para ayudar a comprender el concepto principal, simplifiquemos el escenario y <code class="docutils literal notranslate"><span class="pre">supongamos</span> <span class="pre">que</span> <span class="pre">sólo</span> <span class="pre">interviene</span> <span class="pre">una</span> <span class="pre">variante</span> <span class="pre">de</span> <span class="pre">estado</span></code>. Entonces los vectores de estado se convierten en escalares, <span class="math notranslate nohighlight">\(h_{n}\)</span> , y la matriz <span class="math notranslate nohighlight">\(W\)</span> en un escalar <span class="math notranslate nohighlight">\(w\)</span>. Además, <code class="docutils literal notranslate"><span class="pre">supongamos</span> <span class="pre">que</span> <span class="pre">las</span> <span class="pre">salidas</span> <span class="pre">también</span> <span class="pre">son</span> <span class="pre">escalares</span></code>. Entonces la recursión en la Eq. <a class="reference internal" href="#equation-indirect-direct-rec-part">(81)</a> se simplifica como:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\frac{\partial J}{\partial h_{n}}=\frac{\partial h_{n+1}}{\partial h_{n}}\frac{\partial J}{\partial h_{n+1}}+\frac{\partial\hat{y}_{n}}{\partial h_{n}}\frac{\partial J}{\partial\hat{y}_{n}}.
\]</div>
<ul class="simple">
<li><p>Suponiendo en la Eq. <a class="reference internal" href="#equation-rnn-system-eq">(80)</a> que <span class="math notranslate nohighlight">\(f\)</span> <code class="docutils literal notranslate"><span class="pre">es</span> <span class="pre">la</span> <span class="pre">función</span></code> <span class="math notranslate nohighlight">\(\tanh(\cdot)\)</span> <code class="docutils literal notranslate"><span class="pre">estándar</span></code>, teniendo en cuenta que, <span class="math notranslate nohighlight">\(\text{sech}^2(\cdot)=1-\tanh^2(\cdot)\)</span> y <span class="math notranslate nohighlight">\(|\tanh(\cdot)|&lt;1\)</span>, sobre su dominio, se ve fácilmente que</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-partial-deriv-hn">
<span class="eqno">(82)<a class="headerlink" href="#equation-partial-deriv-hn" title="Link to this equation">#</a></span>\[
\frac{\partial h_{n+1}}{\partial h_{n}}=w(1-h_{n+1}^{2}).
\]</div>
<ul class="simple">
<li><p>Escribiendo la <code class="docutils literal notranslate"><span class="pre">recursión</span> <span class="pre">para</span> <span class="pre">dos</span> <span class="pre">pasos</span> <span class="pre">sucesivos</span></code>, repitiendo el proceso en Eq. <a class="reference internal" href="#equation-partial-deriv-hn">(82)</a>, por ejemplo, para <span class="math notranslate nohighlight">\(\partial h_{n+2}/\partial h_{n+1}\)</span> obtenemos que,</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-partialj-hn-eq">
<span class="eqno">(83)<a class="headerlink" href="#equation-partialj-hn-eq" title="Link to this equation">#</a></span>\[\begin{split}
\begin{align*}
\frac{\partial J}{\partial h_{n}}&amp;=\frac{\partial h_{n+1}}{\partial h_{n}}\frac{\partial J}{\partial h_{n+1}}+\frac{\partial\hat{y}_{n}}{\partial h_{n}}\frac{\partial J}{\partial\hat{y}_{n}}\\
&amp;=w^{2}(1-h_{n+1}^{2})(1-h_{n+2}^{2})\frac{\partial J}{\partial h_{n+2}}+\text{otro términos}
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>No es difícil ver que la <code class="docutils literal notranslate"><span class="pre">multiplicación</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">términos</span> <span class="pre">menores</span> <span class="pre">que</span> <span class="pre">uno</span> <span class="pre">puede</span> <span class="pre">llevar</span> <span class="pre">a</span> <span class="pre">valores</span> <span class="pre">de</span> <span class="pre">desvanecimiento</span></code>, sobre todo si tenemos en cuenta que, en la práctica, las <code class="docutils literal notranslate"><span class="pre">secuencias</span> <span class="pre">pueden</span> <span class="pre">ser</span> <span class="pre">bastante</span> <span class="pre">grandes,</span> <span class="pre">por</span> <span class="pre">ejemplo,</span></code> <span class="math notranslate nohighlight">\(N = 100\)</span>. Por lo tanto <code class="docutils literal notranslate"><span class="pre">para</span> <span class="pre">instantes</span> <span class="pre">de</span> <span class="pre">tiempo</span> <span class="pre">cercanos</span> <span class="pre">a</span></code> <span class="math notranslate nohighlight">\(n = 1\)</span>, la <code class="docutils literal notranslate"><span class="pre">contribución</span> <span class="pre">al</span> <span class="pre">gradiente</span> <span class="pre">del</span> <span class="pre">primer</span> <span class="pre">término</span> <span class="pre">del</span> <span class="pre">lado</span> <span class="pre">derecho</span> <span class="pre">en</span> <span class="pre">la</span></code> Eq. <a class="reference internal" href="#equation-partialj-hn-eq">(83)</a> implicará un <code class="docutils literal notranslate"><span class="pre">gran</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">productos</span> <span class="pre">de</span> <span class="pre">números</span> <span class="pre">menores</span> <span class="pre">que</span> <span class="pre">uno</span> <span class="pre">en</span> <span class="pre">magnitud</span></code>. Por otra parte, el valor de <span class="math notranslate nohighlight">\(w\)</span> estará contribuyendo en <span class="math notranslate nohighlight">\(w^{n}\)</span> potencia. Por lo tanto, <code class="docutils literal notranslate"><span class="pre">si</span> <span class="pre">su</span> <span class="pre">valor</span> <span class="pre">es</span> <span class="pre">mayor</span> <span class="pre">que</span> <span class="pre">uno,</span> <span class="pre">puede</span> <span class="pre">conducir</span> <span class="pre">a</span> <span class="pre">valores</span> <span class="pre">explosivos</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">respectivos</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>En varios casos, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">puede</span> <span class="pre">truncar</span> <span class="pre">el</span> <span class="pre">algoritmo</span> <span class="pre">de</span> <span class="pre">backpropagation</span> <span class="pre">a</span> <span class="pre">unos</span> <span class="pre">pocos</span> <span class="pre">pasos</span> <span class="pre">de</span> <span class="pre">tiempo</span></code>. Otra forma, es <code class="docutils literal notranslate"><span class="pre">sustituir</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">tanh</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">ReLU</span></code>. Para el caso del <code class="docutils literal notranslate"><span class="pre">valor</span> <span class="pre">explosivo</span></code>, se puede <code class="docutils literal notranslate"><span class="pre">introducir</span> <span class="pre">una</span> <span class="pre">técnica</span> <span class="pre">que</span> <span class="pre">recorte</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">a</span> <span class="pre">un</span> <span class="pre">umbral</span> <span class="pre">predeterminado</span></code>, una vez que los <code class="docutils literal notranslate"><span class="pre">valores</span> <span class="pre">superen</span> <span class="pre">ese</span> <span class="pre">umbral</span></code>. Sin embargo, otra técnica que suele emplearse en la práctica es <code class="docutils literal notranslate"><span class="pre">sustituir</span> <span class="pre">la</span> <span class="pre">formulación</span> <span class="pre">RNN</span></code> estándar descrita anteriormente por una <code class="docutils literal notranslate"><span class="pre">estructura</span> <span class="pre">alternativa,</span> <span class="pre">que</span> <span class="pre">puede</span> <span class="pre">hacer</span> <span class="pre">mejor</span> <span class="pre">frente</span> <span class="pre">a</span> <span class="pre">estos</span> <span class="pre">fenómenos</span> <span class="pre">causados</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">dependencia</span> <span class="pre">a</span> <span class="pre">largo</span> <span class="pre">plazo</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">RNN</span></code>.</p></li>
</ul>
<div class="proof observation admonition" id="observation_ann9">
<p class="admonition-title"><span class="caption-number">Observation 1 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">profundas</span></code></strong>: Además de la red <code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">básica</span> <span class="pre">que</span> <span class="pre">comprende</span> <span class="pre">una</span> <span class="pre">sola</span> <span class="pre">capa</span> <span class="pre">de</span> <span class="pre">estados</span></code>, se han propuesto <code class="docutils literal notranslate"><span class="pre">extensiones</span> <span class="pre">que</span> <span class="pre">implican</span> <span class="pre">múltiples</span> <span class="pre">capas</span> <span class="pre">de</span> <span class="pre">estados</span></code>, una encima de otra (ver <span id="id8">[<a class="reference internal" href="biblio.html#id36" title="Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026, 2013.">Pascanu <em>et al.</em>, 2013</a>]</span>).</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">bidireccionales</span></code></strong>: Como su nombre indica, en las <code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">bidireccionales</span> <span class="pre">hay</span> <span class="pre">dos</span> <span class="pre">variables</span> <span class="pre">de</span> <span class="pre">estado</span></code>, es decir, una denotada como <span class="math notranslate nohighlight">\(\overset{\rightarrow}{\boldsymbol{h}}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">propaga</span> <span class="pre">hacia</span> <span class="pre">adelante</span></code>, y otra, <span class="math notranslate nohighlight">\(\overset{\leftarrow}{\boldsymbol{h}}\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">propaga</span> <span class="pre">hacia</span> <span class="pre">atrás</span></code>. De este modo, <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">salidas</span> <span class="pre">dependen</span> <span class="pre">tanto</span> <span class="pre">del</span> <span class="pre">pasado</span> <span class="pre">como</span> <span class="pre">del</span> <span class="pre">futuro</span></code> (ver <span id="id9">[<a class="reference internal" href="biblio.html#id37" title="Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In 2013 IEEE international conference on acoustics, speech and signal processing, 6645–6649. Ieee, 2013.">Graves <em>et al.</em>, 2013</a>]</span>).</p></li>
</ul>
</section>
</div></section>
</section>
<section id="red-de-memoria-a-largo-plazo-lstm">
<h2>Red de memoria a largo plazo (LSTM)<a class="headerlink" href="#red-de-memoria-a-largo-plazo-lstm" title="Link to this heading">#</a></h2>
<div class="proof observation admonition" id="observation_ann10">
<p class="admonition-title"><span class="caption-number">Observation 2 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>La <code class="docutils literal notranslate"><span class="pre">idea</span> <span class="pre">clave</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">red</span> <span class="pre">LSTM</span></code>, propuesta en el artículo seminal <span id="id10">[<a class="reference internal" href="biblio.html#id43" title="Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.">Hochreiter and Schmidhuber, 1997</a>]</span>, es el llamado <code class="docutils literal notranslate"><span class="pre">estado</span> <span class="pre">de</span> <span class="pre">celda</span></code>, que ayuda a <code class="docutils literal notranslate"><span class="pre">superar</span> <span class="pre">los</span> <span class="pre">problemas</span> <span class="pre">asociados</span> <span class="pre">con</span> <span class="pre">los</span> <span class="pre">fenómenos</span> <span class="pre">de</span> <span class="pre">desvanecimiento/explosión</span></code> que son causados por las <code class="docutils literal notranslate"><span class="pre">dependencias</span> <span class="pre">largo</span> <span class="pre">plazo</span></code> dentro de la red. Las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">LSTM</span></code> tienen la capacidad incorporada de <code class="docutils literal notranslate"><span class="pre">controlar</span> <span class="pre">el</span> <span class="pre">flujo</span> <span class="pre">de</span> <span class="pre">información</span> <span class="pre">que</span> <span class="pre">entra</span> <span class="pre">y</span> <span class="pre">sale</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">memoria</span> <span class="pre">del</span> <span class="pre">sistema</span> <span class="pre">mediante</span> <span class="pre">algoritmos</span> <span class="pre">no</span> <span class="pre">lineales</span></code>. Estas puertas se implementan <code class="docutils literal notranslate"><span class="pre">mediante</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">sigmoidea</span> <span class="pre">y</span> <span class="pre">un</span> <span class="pre">multiplicador</span></code>. Desde un punto de vista algorítmico, las puertas equivalen a <code class="docutils literal notranslate"><span class="pre">aplicar</span> <span class="pre">una</span> <span class="pre">ponderación</span> <span class="pre">al</span> <span class="pre">flujo</span> <span class="pre">de</span> <span class="pre">información</span> <span class="pre">correspondiente</span></code>. Los <code class="docutils literal notranslate"><span class="pre">pesos</span></code> se sitúan en el intervalo <span class="math notranslate nohighlight">\([0,1]\)</span> y <code class="docutils literal notranslate"><span class="pre">dependen</span> <span class="pre">de</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">variables</span> <span class="pre">implicadas</span> <span class="pre">que</span> <span class="pre">activan</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">sigmoidea</span></code>.</p></li>
</ul>
</section>
</div><figure class="align-center" id="lstm-arch-rnn-numref">
<a class="reference internal image-reference" href="_images/lstm_arch_rnn.png"><img alt="_images/lstm_arch_rnn.png" src="_images/lstm_arch_rnn.png" style="width: 664.8000000000001px; height: 332.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">Arquitectura de unidad LSTM. Fuente <span id="id11">[<a class="reference internal" href="biblio.html#id31" title="S. Theodoridis. Machine Learning: A Bayesian and Optimization Perspective. Elsevier Science, 2020. ISBN 9780128188040. URL: https://books.google.com.co/books?id=l-nEDwAAQBAJ.">Theodoridis, 2020</a>]</span>.</span><a class="headerlink" href="#lstm-arch-rnn-numref" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En otras palabras, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">ponderación</span> <span class="pre">(control)</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">información</span></code> tiene lugar en el contexto. Según este razonamiento, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">red</span> <span class="pre">tiene</span> <span class="pre">la</span> <span class="pre">agilidad</span> <span class="pre">de</span> <span class="pre">olvidar</span> <span class="pre">la</span> <span class="pre">información</span> <span class="pre">que</span> <span class="pre">ya</span> <span class="pre">ha</span> <span class="pre">sido</span> <span class="pre">utilizada</span> <span class="pre">y</span> <span class="pre">ya</span> <span class="pre">no</span> <span class="pre">es</span> <span class="pre">necesaria</span></code>. La <code class="docutils literal notranslate"><span class="pre">célula/unidad</span> <span class="pre">LSTM</span> <span class="pre">básica</span></code> se
se muestra en la <a class="reference internal" href="#lstm-arch-rnn-numref"><span class="std std-numref">Fig. 16</span></a>. Se construye en torno a <code class="docutils literal notranslate"><span class="pre">dos</span> <span class="pre">conjuntos</span> <span class="pre">de</span> <span class="pre">variables,</span> <span class="pre">apiladas</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">vector</span></code> <span class="math notranslate nohighlight">\(\boldsymbol{s}\)</span>, que se conoce como el <code class="docutils literal notranslate"><span class="pre">estado</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">célula</span> <span class="pre">o</span> <span class="pre">unidad</span></code>, y el vector <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span>, que se conoce como el <code class="docutils literal notranslate"><span class="pre">vector</span> <span class="pre">de</span> <span class="pre">variables</span> <span class="pre">ocultas</span></code>. Una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">LSTM</span></code> se construye a partir de la <code class="docutils literal notranslate"><span class="pre">concatenación</span> <span class="pre">sucesiva</span> <span class="pre">de</span> <span class="pre">esta</span> <span class="pre">unidad</span> <span class="pre">básica</span></code>. La unidad correspondiente al tiempo <span class="math notranslate nohighlight">\(n\)</span>, además del vector de entrada, <span class="math notranslate nohighlight">\(\boldsymbol{x}_{n}\)</span>, recibe <span class="math notranslate nohighlight">\(\boldsymbol{s}_{n-1}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n-1}\)</span> de la etapa anterior y pasa <span class="math notranslate nohighlight">\(\boldsymbol{s}_{n}\)</span> y <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span> a la siguiente.</p></li>
</ul>
<ul>
<li><p>A continuación se resumen las <code class="docutils literal notranslate"><span class="pre">ecuaciones</span> <span class="pre">de</span> <span class="pre">actualización</span> <span class="pre">asociadas</span></code>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{align*}
    \boldsymbol{f}&amp;=\sigma(U^{f}\boldsymbol{x}_{n}+W^{f}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{f}),\\
    \boldsymbol{i}&amp;=\sigma(U^{i}\boldsymbol{x}_{n}+W^{i}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{i}),\\
    \tilde{\boldsymbol{s}}&amp;=\tanh(U^{s}\boldsymbol{x}_{n}+W^{s}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{s}),\\
    \boldsymbol{o}&amp;=\sigma(U^{o}\boldsymbol{x}_{n}+W^{o}\boldsymbol{h}_{n-1}+\boldsymbol{b}^{o}),\\
    \boldsymbol{s}_{n}&amp;=\boldsymbol{s}_{n-1}\circ f+\boldsymbol{i}\circ\tilde{\boldsymbol{s}},\\
    \boldsymbol{h}_{n}&amp;=\boldsymbol{o}\circ\tanh(\boldsymbol{s}_{n}),
    \end{align*}
    \end{split}\]</div>
<p>donde <span class="math notranslate nohighlight">\(\circ\)</span> denota el <code class="docutils literal notranslate"><span class="pre">producto</span> <span class="pre">elemento</span> <span class="pre">a</span> <span class="pre">elemento</span> <span class="pre">entre</span> <span class="pre">vectores</span> <span class="pre">o</span> <span class="pre">matrices</span></code> (producto de <code class="docutils literal notranslate"><span class="pre">Hadamard</span></code>), es decir, <span class="math notranslate nohighlight">\((s\circ f)_{i} = s_{i}f_{i}\)</span>, y <span class="math notranslate nohighlight">\(\sigma\)</span> denota la <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">sigmoidea</span> <span class="pre">logística</span></code>.</p>
</li>
</ul>
<div class="proof observation admonition" id="observation_ann11">
<p class="admonition-title"><span class="caption-number">Observation 3 </span></p>
<section class="observation-content" id="proof-content">
<ul class="simple">
<li><p>Nótese que el <code class="docutils literal notranslate"><span class="pre">estado</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">célula</span></code>, <span class="math notranslate nohighlight">\(\boldsymbol{s}\)</span>, <code class="docutils literal notranslate"><span class="pre">pasa</span> <span class="pre">información</span> <span class="pre">directa</span> <span class="pre">del</span> <span class="pre">instante</span> <span class="pre">anterior</span> <span class="pre">al</span> <span class="pre">siguiente</span></code>. Esta información es <code class="docutils literal notranslate"><span class="pre">controlada</span> <span class="pre">primero</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">primera</span> <span class="pre">puerta</span></code>, según los elementos en <span class="math notranslate nohighlight">\(f\)</span>, que toman valores en el rango <span class="math notranslate nohighlight">\([0, 1]\)</span>, <code class="docutils literal notranslate"><span class="pre">dependiendo</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">entrada</span> <span class="pre">actual</span> <span class="pre">y</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">variables</span> <span class="pre">ocultas</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">reciben</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">etapa</span> <span class="pre">anterior</span></code>. Esto es lo que decíamos antes, es decir, que <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">ponderación</span> <span class="pre">se</span> <span class="pre">ajusta</span> <span class="pre">en</span> <span class="pre">&quot;contexto&quot;</span></code>. A continuación, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">añade</span> <span class="pre">nueva</span> <span class="pre">información</span></code>, es decir, <span class="math notranslate nohighlight">\(\tilde{\boldsymbol{s}}\)</span>, a <span class="math notranslate nohighlight">\(\boldsymbol{s}_{n-1}\)</span>, que también está <code class="docutils literal notranslate"><span class="pre">controlada</span> <span class="pre">por</span> <span class="pre">la</span> <span class="pre">segunda</span> <span class="pre">red</span> <span class="pre">de</span> <span class="pre">puertas</span> <span class="pre">sigmoidales</span></code> (es decir, <span class="math notranslate nohighlight">\(\boldsymbol{i}\)</span>). Así se garantiza que la <code class="docutils literal notranslate"><span class="pre">información</span> <span class="pre">del</span> <span class="pre">pasado</span> <span class="pre">se</span> <span class="pre">transmita</span> <span class="pre">al</span> <span class="pre">futuro</span> <span class="pre">de</span> <span class="pre">forma</span> <span class="pre">directa,</span> <span class="pre">lo</span> <span class="pre">cual,</span> <span class="pre">ayuda</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">red</span> <span class="pre">a</span> <span class="pre">memorizar</span> <span class="pre">información.</span></code>.</p></li>
<li><p>Resulta que este tipo de memoria <code class="docutils literal notranslate"><span class="pre">explota</span> <span class="pre">mejor</span> <span class="pre">las</span> <span class="pre">dependencias</span> <span class="pre">de</span> <span class="pre">largo</span> <span class="pre">alcance</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">datos</span></code>, en comparación con la estructura <code class="docutils literal notranslate"><span class="pre">RNN</span> <span class="pre">básica</span></code>. El vector de variables ocultas <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> <code class="docutils literal notranslate"><span class="pre">está</span> <span class="pre">controlado</span> <span class="pre">tanto</span> <span class="pre">por</span> <span class="pre">el</span> <span class="pre">estado</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">célula</span> <span class="pre">como</span> <span class="pre">por</span> <span class="pre">los</span> <span class="pre">valores</span> <span class="pre">actuales</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">variables</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">y</span> <span class="pre">de</span> <span class="pre">estados</span> <span class="pre">anteriores</span></code>. Todas las <code class="docutils literal notranslate"><span class="pre">matrices</span> <span class="pre">y</span> <span class="pre">vectores</span> <span class="pre">implicados</span> <span class="pre">se</span> <span class="pre">aprenden</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">fase</span> <span class="pre">de</span> <span class="pre">entrenamiento</span></code>. Nótese que hay dos líneas asociadas a <span class="math notranslate nohighlight">\(\boldsymbol{h}_{n}\)</span>. La de <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">derecha</span> <span class="pre">conduce</span> <span class="pre">a</span> <span class="pre">la</span> <span class="pre">siguiente</span> <span class="pre">etapa</span></code> y la de <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">parte</span> <span class="pre">superior</span> <span class="pre">se</span> <span class="pre">utiliza</span> <span class="pre">para</span> <span class="pre">proporcionar</span> <span class="pre">la</span> <span class="pre">salida</span></code>, <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}_{n}\)</span>, en el tiempo <span class="math notranslate nohighlight">\(n\)</span>, a <code class="docutils literal notranslate"><span class="pre">través</span> <span class="pre">de,</span> <span class="pre">digamos,</span> <span class="pre">la</span> <span class="pre">no</span> <span class="pre">linealidad</span> <span class="pre">softmax</span></code>, como en las RNN estándar en Eq. <a class="reference internal" href="#equation-rnn-system-eq">(80)</a>.</p></li>
</ul>
</section>
</div><div class="admonition-variantes-y-aplicaciones admonition">
<p class="admonition-title">Variantes y Aplicaciones</p>
<ul class="simple">
<li><p>Además de la estructura LSTM ya comentada, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">han</span> <span class="pre">propuesto</span> <span class="pre">diversas</span> <span class="pre">variantes</span></code>. Un extenso <code class="docutils literal notranslate"><span class="pre">estudio</span> <span class="pre">comparativo</span> <span class="pre">entre</span> <span class="pre">diferentes</span> <span class="pre">arquitecturas</span> <span class="pre">LSTM</span> <span class="pre">y</span> <span class="pre">RNN</span></code> se puede encontrar en <span id="id12">[<a class="reference internal" href="biblio.html#id44" title="Klaus Greff, Rupesh K Srivastava, Jan Koutník, Bas R Steunebrink, and Jürgen Schmidhuber. Lstm: a search space odyssey. IEEE transactions on neural networks and learning systems, 28(10):2222–2232, 2016.">Greff <em>et al.</em>, 2016</a>, <a class="reference internal" href="biblio.html#id45" title="Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent network architectures. In International conference on machine learning, 2342–2350. PMLR, 2015.">Jozefowicz <em>et al.</em>, 2015</a>]</span>. Las <code class="docutils literal notranslate"><span class="pre">RNNs</span> <span class="pre">y</span> <span class="pre">las</span> <span class="pre">LSTMs</span></code> se han utilizado con éxito en una <code class="docutils literal notranslate"><span class="pre">amplia</span> <span class="pre">gama</span> <span class="pre">de</span> <span class="pre">aplicaciones</span></code>, tales como:</p>
<ul>
<li><p>el <code class="docutils literal notranslate"><span class="pre">modelado</span> <span class="pre">del</span> <span class="pre">lenguaje</span></code> <span id="id13">[<a class="reference internal" href="biblio.html#id38" title="Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11), 1017–1024. 2011.">Sutskever <em>et al.</em>, 2011</a>]</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">traducción</span> <span class="pre">de</span> <span class="pre">máquinas</span></code> <span id="id14">[<a class="reference internal" href="biblio.html#id39" title="Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. A recursive recurrent neural network for statistical machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1491–1500. 2014.">Liu <em>et al.</em>, 2014</a>]</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reconocimiento</span> <span class="pre">del</span> <span class="pre">habla</span></code> <span id="id15">[<a class="reference internal" href="biblio.html#id40" title="Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks. In International conference on machine learning, 1764–1772. PMLR, 2014.">Graves and Jaitly, 2014</a>]</span>,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">visión</span> <span class="pre">artificial</span></code> para la generación de descriptores de imágenes <span id="id16">[<a class="reference internal" href="biblio.html#id46" title="Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, 3128–3137. 2015.">Karpathy and Fei-Fei, 2015</a>]</span>,</p></li>
<li><p>análisis de datos de fMRI para comprender la <code class="docutils literal notranslate"><span class="pre">dinámica</span> <span class="pre">temporal</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">redes</span> <span class="pre">cerebrales</span></code> asociadas <span id="id17">[<a class="reference internal" href="biblio.html#id41" title="Youngjoo Seo, Manuel Morante, Yannis Kopsinis, and Sergios Theodoridis. Unsupervised pre-training of the brain connectivity dynamic using residual d-net. In Neural Information Processing: 26th International Conference, ICONIP 2019, Sydney, NSW, Australia, December 12–15, 2019, Proceedings, Part III 26, 608–620. Springer, 2019.">Seo <em>et al.</em>, 2019</a>]</span>.</p></li>
<li><p>pronostico de <code class="docutils literal notranslate"><span class="pre">volatilidad</span> <span class="pre">de</span> <span class="pre">Bitcoin</span></code> <span id="id18">[<a class="reference internal" href="biblio.html#id42" title="Tiago E Pratas, Filipe R Ramos, and Lihki Rubio. Forecasting bitcoin volatility: exploring the potential of deep learning. Eurasian Economic Review, pages 1–21, 2023.">Pratas <em>et al.</em>, 2023</a>]</span></p></li>
</ul>
</li>
<li><p>Por ejemplo, en el <code class="docutils literal notranslate"><span class="pre">procesamiento</span> <span class="pre">del</span> <span class="pre">lenguaje</span></code> la <code class="docutils literal notranslate"><span class="pre">entrada</span> <span class="pre">suele</span> <span class="pre">ser</span> <span class="pre">una</span> <span class="pre">secuencia</span> <span class="pre">de</span> <span class="pre">palabras,</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">codifican</span> <span class="pre">como</span> <span class="pre">números</span></code> (son punteros al diccionario disponible). La <code class="docutils literal notranslate"><span class="pre">salida</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">secuencia</span> <span class="pre">de</span> <span class="pre">palabras</span> <span class="pre">que</span> <span class="pre">hay</span> <span class="pre">que</span> <span class="pre">predecir</span></code>. Durante el entrenamiento, se establece <span class="math notranslate nohighlight">\(\boldsymbol{y}_{n} = \boldsymbol{x}_{n+1}\)</span>. Es decir, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">red</span> <span class="pre">se</span> <span class="pre">entrena</span> <span class="pre">como</span> <span class="pre">un</span> <span class="pre">predictor</span> <span class="pre">no</span> <span class="pre">lineal</span></code>.</p></li>
</ul>
</div>
</section>
<section id="series-de-tiempo">
<h2>Series de Tiempo<a class="headerlink" href="#series-de-tiempo" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Hasta ahora, en este curso hemos descrito <code class="docutils literal notranslate"><span class="pre">métodos</span> <span class="pre">estadísticos</span></code> tradicionales para el análisis de series temporales. En los capítulos anteriores, hemos analizado varios métodos para predecir la serie en un momento futuro a partir de observaciones tomadas en el pasado. Uno de estos métodos para realizar predicciones es el <code class="docutils literal notranslate"><span class="pre">modelo</span> <span class="pre">autorregresivo</span></code> (<span class="math notranslate nohighlight">\(AR\)</span>), que expresa la serie en el tiempo <span class="math notranslate nohighlight">\(t\)</span> como una <code class="docutils literal notranslate"><span class="pre">regresión</span> <span class="pre">lineal</span></code> de <span class="math notranslate nohighlight">\(p\)</span> observaciones anteriores</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y_{t}=\omega_{0}+\sum_{i=1}^{p}\omega_{i}y_{t-i}+\varepsilon_{t}.
\]</div>
<ul class="simple">
<li><p>Aquí, <span class="math notranslate nohighlight">\(\varepsilon_{t}\)</span> es el término de <code class="docutils literal notranslate"><span class="pre">error</span> <span class="pre">residual</span></code> del modelo <span class="math notranslate nohighlight">\(AR\)</span>. La idea que subyace al modelo lineal puede generalizarse en el sentido de que el objetivo de la predicción de series temporales es <code class="docutils literal notranslate"><span class="pre">desarrollar</span> <span class="pre">una</span> <span class="pre">función</span></code> <span class="math notranslate nohighlight">\(f\)</span> <code class="docutils literal notranslate"><span class="pre">que</span> <span class="pre">prediga</span></code> <span class="math notranslate nohighlight">\(y_{t}\)</span> <code class="docutils literal notranslate"><span class="pre">en</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">observaciones</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\(p\)</span> <code class="docutils literal notranslate"><span class="pre">valores</span> <span class="pre">previos</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">tiempo</span></code></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
y_{t}=f(y_{t-1}, y_{t-2}, \dots, y_{t-p}).
\]</div>
<ul class="simple">
<li><p>En este capítulo, exploraremos tres <code class="docutils literal notranslate"><span class="pre">métodos</span> <span class="pre">basados</span> <span class="pre">en</span> <span class="pre">redes</span> <span class="pre">neuronales</span> <span class="pre">para</span> <span class="pre">desarrollar</span> <span class="pre">la</span> <span class="pre">función</span></code> <span class="math notranslate nohighlight">\(f\)</span>. Cada método incluye la definición de una <code class="docutils literal notranslate"><span class="pre">arquitectura</span> <span class="pre">de</span> <span class="pre">red</span> <span class="pre">neuronal</span></code> (en términos del <code class="docutils literal notranslate"><span class="pre">número</span> <span class="pre">de</span> <span class="pre">capas</span> <span class="pre">ocultas,</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">neuronas</span> <span class="pre">en</span> <span class="pre">cada</span> <span class="pre">capa</span> <span class="pre">oculta</span></code>, etc.) y luego el algoritmo <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code> o su variante adecuada para la arquitectura de red utilizada.</p></li>
</ul>
<ul class="simple">
<li><p>Los ejemplos de este capítulo serán implementados utilizando la <code class="docutils literal notranslate"><span class="pre">API</span> <span class="pre">Keras</span></code> para el aprendizaje profundo. <code class="docutils literal notranslate"><span class="pre">Keras</span></code> es una <code class="docutils literal notranslate"><span class="pre">API</span></code> de alto nivel que permite <code class="docutils literal notranslate"><span class="pre">definir</span> <span class="pre">diferentes</span> <span class="pre">arquitecturas</span> <span class="pre">de</span> <span class="pre">redes</span> <span class="pre">neuronales</span> <span class="pre">y</span> <span class="pre">entrenarlas</span> <span class="pre">utilizando</span> <span class="pre">varios</span> <span class="pre">optimizadores</span> <span class="pre">basados</span> <span class="pre">en</span> <span class="pre">gradientes</span></code>. En el <code class="docutils literal notranslate"><span class="pre">backend,</span> <span class="pre">Keras</span></code> utiliza un marco computacional de bajo nivel implementado en <code class="docutils literal notranslate"><span class="pre">C,</span> <span class="pre">C++</span></code> y <code class="docutils literal notranslate"><span class="pre">FORTRAN</span></code>. Varios de estos entornos de bajo nivel están disponibles en código abierto. <code class="docutils literal notranslate"><span class="pre">Keras</span></code> soporta los tres siguientes entornos: <code class="docutils literal notranslate"><span class="pre">TensorFlow,</span> <span class="pre">que</span> <span class="pre">fue</span> <span class="pre">desarrollado</span> <span class="pre">por</span> <span class="pre">Google</span></code> y es el <code class="docutils literal notranslate"><span class="pre">backend</span></code> por defecto de <code class="docutils literal notranslate"><span class="pre">Keras,</span> <span class="pre">CNTK,</span> <span class="pre">un</span> <span class="pre">marco</span> <span class="pre">de</span> <span class="pre">código</span> <span class="pre">abierto</span> <span class="pre">de</span> <span class="pre">Microsoft</span></code>, y <code class="docutils literal notranslate"><span class="pre">Theano,</span> <span class="pre">desarrollado</span> <span class="pre">originalmente</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">Universidad</span> <span class="pre">de</span> <span class="pre">Montreal,</span> <span class="pre">Canadá</span></code>. Los ejemplos de este libro utilizan <code class="docutils literal notranslate"><span class="pre">TensorFlow</span></code> como backend. Por lo tanto, para ejecutar los ejemplos, necesitarás tanto <code class="docutils literal notranslate"><span class="pre">Keras</span></code> como <code class="docutils literal notranslate"><span class="pre">TensorFlow</span></code> instalados.</p></li>
</ul>
</section>
<section id="perceptrones-multicapa">
<h2>Perceptrones multicapa<a class="headerlink" href="#perceptrones-multicapa" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Los <code class="docutils literal notranslate"><span class="pre">perceptrones</span> <span class="pre">multicapa</span> <span class="pre">(MLP)</span></code> son las formas más básicas de las redes neuronales. Un <code class="docutils literal notranslate"><span class="pre">MLP</span></code> consta de tres componentes: una <code class="docutils literal notranslate"><span class="pre">capa</span> <span class="pre">de</span> <span class="pre">entrada</span></code>, varias <code class="docutils literal notranslate"><span class="pre">capas</span> <span class="pre">ocultas</span></code> y una <code class="docutils literal notranslate"><span class="pre">capa</span> <span class="pre">de</span> <span class="pre">salida</span></code>. Una <code class="docutils literal notranslate"><span class="pre">capa</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">representa</span> <span class="pre">un</span> <span class="pre">vector</span> <span class="pre">de</span> <span class="pre">regresores</span> <span class="pre">o</span> <span class="pre">características</span> <span class="pre">de</span> <span class="pre">entrada</span></code>, por ejemplo, observaciones a partir de <span class="math notranslate nohighlight">\(p\)</span> puntos previos en el tiempo <span class="math notranslate nohighlight">\([y_{t-1}, y_{t-2}, \dots, y_{t-p}]\)</span>. Las <code class="docutils literal notranslate"><span class="pre">características</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">se</span> <span class="pre">introducen</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">capa</span> <span class="pre">oculta</span></code> que tiene <span class="math notranslate nohighlight">\(n\)</span> neuronas, cada una de las cuales aplica una <code class="docutils literal notranslate"><span class="pre">transformación</span> <span class="pre">lineal</span></code> y una <code class="docutils literal notranslate"><span class="pre">activación</span> <span class="pre">no</span> <span class="pre">lineal</span></code> a las características de entrada.</p></li>
</ul>
<ul>
<li><p>La salida de una neurona es <span class="math notranslate nohighlight">\(g_{i} = h(\boldsymbol{w}_{i}x + b_{i})\)</span>, donde <span class="math notranslate nohighlight">\(\boldsymbol{w}_{i}\)</span> y <span class="math notranslate nohighlight">\(b_{i}\)</span> son los <code class="docutils literal notranslate"><span class="pre">pesos</span> <span class="pre">y</span> <span class="pre">el</span> <span class="pre">sesgo</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">transformación</span> <span class="pre">lineal</span></code>, y <span class="math notranslate nohighlight">\(h\)</span> es una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">de</span> <span class="pre">activación</span> <span class="pre">no</span> <span class="pre">lineal</span></code>. La función de activación no lineal permite a la red neuronal <code class="docutils literal notranslate"><span class="pre">modelar</span> <span class="pre">no</span> <span class="pre">linealidades</span> <span class="pre">complejas</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">relaciones</span> <span class="pre">subyacentes</span> <span class="pre">entre</span> <span class="pre">los</span> <span class="pre">regresores</span> <span class="pre">y</span> <span class="pre">la</span> <span class="pre">variable</span> <span class="pre">objetivo</span></code>. Popularmente, <span class="math notranslate nohighlight">\(h\)</span> es la función <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \\[1mm]
    h(z)=\displaystyle{\frac{1}{1-e^{-z}}},
    \end{split}\]</div>
<p>que mapea cualquier número real al intervalo <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p>
</li>
</ul>
<ul>
<li><p>Debido a esta propiedad, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">función</span> <span class="pre">sigmoidea</span> <span class="pre">se</span> <span class="pre">utiliza</span> <span class="pre">para</span> <span class="pre">generar</span> <span class="pre">probabilidades</span> <span class="pre">de</span> <span class="pre">clase</span> <span class="pre">binarias</span> <span class="pre">y,</span> <span class="pre">por</span> <span class="pre">tanto,</span> <span class="pre">es</span> <span class="pre">comunmente</span> <span class="pre">usada</span> <span class="pre">en</span> <span class="pre">los</span> <span class="pre">modelos</span> <span class="pre">de</span> <span class="pre">clasificación</span></code>. Otra opción de función de activación no lineal es la función <code class="docutils literal notranslate"><span class="pre">tanh</span></code></p>
<div class="math notranslate nohighlight">
\[
    h(z)=\displaystyle{\frac{1-e^{-z}}{1+e^{-z}}},
    \]</div>
<p>la cual mapea cualquier número real al intervalo <span class="math notranslate nohighlight">\([-1, 1]\)</span>. En algunos casos la función <span class="math notranslate nohighlight">\(h\)</span> es una <code class="docutils literal notranslate"><span class="pre">función</span> <span class="pre">lineal</span> <span class="pre">o</span> <span class="pre">la</span> <span class="pre">identidad</span></code>.</p>
</li>
</ul>
<ul class="simple">
<li><p>En el caso de una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">neuronal</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">sola</span> <span class="pre">capa</span> <span class="pre">oculta</span></code>, como se muestra en <a class="reference internal" href="#single-hidden-layer"><span class="std std-numref">Fig. 17</span></a>, la salida de cada neurona se pasa a la capa de salida, que aplica una <code class="docutils literal notranslate"><span class="pre">transformación</span> <span class="pre">lineal</span> <span class="pre">y</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">activación</span> <span class="pre">para</span> <span class="pre">generar</span> <span class="pre">la</span> <span class="pre">predicción</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">variable</span> <span class="pre">objetivo</span></code>, que, en el caso de la predicción de series temporales, es el valor predicho de la serie en el tiempo <span class="math notranslate nohighlight">\(t\)</span></p></li>
</ul>
<figure class="align-center" id="single-hidden-layer">
<a class="reference internal image-reference" href="_images/single_hidden_layer.png"><img alt="_images/single_hidden_layer.png" src="_images/single_hidden_layer.png" style="width: 360.0px; height: 320.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Red neuronal con una sola capa oculta.</span><a class="headerlink" href="#single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En un <code class="docutils literal notranslate"><span class="pre">MLP</span></code> (ver <a class="reference internal" href="#multiple-hidden-layer"><span class="std std-numref">Fig. 18</span></a>), se agrupan varias capas ocultas. <code class="docutils literal notranslate"><span class="pre">La</span> <span class="pre">salida</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">neuronas</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">capa</span> <span class="pre">oculta</span> <span class="pre">se</span> <span class="pre">introduce</span> <span class="pre">en</span> <span class="pre">la</span> <span class="pre">siguiente</span> <span class="pre">capa</span> <span class="pre">oculta</span></code>. Las neuronas de esta capa transforman la entrada y la pasan a la siguiente capa oculta. Finalmente, <code class="docutils literal notranslate"><span class="pre">la</span> <span class="pre">última</span> <span class="pre">capa</span> <span class="pre">oculta</span> <span class="pre">alimenta</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">de</span> <span class="pre">salida</span></code></p></li>
</ul>
<figure class="align-center" id="multiple-hidden-layer">
<a class="reference internal image-reference" href="_images/multiple_hidden_layer.png"><img alt="_images/multiple_hidden_layer.png" src="_images/multiple_hidden_layer.png" style="width: 570.0px; height: 335.16px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Red neuronal con múltiples capas ocultas.</span><a class="headerlink" href="#multiple-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Las capas ocultas del <code class="docutils literal notranslate"><span class="pre">MLP</span></code> también se denominan <code class="docutils literal notranslate"><span class="pre">capas</span> <span class="pre">densas</span> <span class="pre">o,</span> <span class="pre">a</span> <span class="pre">veces,</span> <span class="pre">capas</span> <span class="pre">totalmente</span> <span class="pre">conectadas</span></code>. El nombre denso tiene su significado en el hecho de que todas <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">neuronas</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">capa</span> <span class="pre">densa</span> <span class="pre">están</span> <span class="pre">conectadas</span> <span class="pre">a</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">neuronas</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">anterior</span> <span class="pre">y</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">siguiente</span></code>. Si la capa anterior es la capa de entrada, todas las características de entrada alimentan a cada una de las neuronas de la capa oculta. <code class="docutils literal notranslate"><span class="pre">Debido</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">conexiones</span> <span class="pre">múltiples</span> <span class="pre">entre</span> <span class="pre">la</span> <span class="pre">capa</span> <span class="pre">de</span> <span class="pre">entrada</span> <span class="pre">y</span> <span class="pre">la</span> <span class="pre">primera</span> <span class="pre">capa</span> <span class="pre">densa</span> <span class="pre">y</span> <span class="pre">entre</span> <span class="pre">las</span> <span class="pre">capas</span> <span class="pre">densas</span> <span class="pre">intermedias,</span> <span class="pre">un</span> <span class="pre">MLP</span> <span class="pre">tiene</span> <span class="pre">un</span> <span class="pre">número</span> <span class="pre">enorme</span> <span class="pre">de</span> <span class="pre">pesos</span> <span class="pre">entrenables</span></code>. Por ejemplo, si el número de características de entrada es <span class="math notranslate nohighlight">\(p\)</span> y hay tres capas densas que tienen número de neuronas <span class="math notranslate nohighlight">\(n_{1}, n_{2}\)</span> y <span class="math notranslate nohighlight">\(n_{3}\)</span> respectivamente, entonces el número de pesos entrenables es</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p\times n_{1}+n_{1}\times n_{2}+n_{2}\times n_{3}+n_{3}.
\]</div>
<ul class="simple">
<li><p>El último elemento de este cálculo es el número de pesos que conectan la tercera capa oculta y la capa de salida. Los <code class="docutils literal notranslate"><span class="pre">MLP</span> <span class="pre">profundos</span> <span class="pre">tienen</span> <span class="pre">varias</span> <span class="pre">capas</span> <span class="pre">densas</span> <span class="pre">y</span> <span class="pre">cientos,</span> <span class="pre">incluso</span> <span class="pre">miles,</span> <span class="pre">de</span> <span class="pre">neuronas</span> <span class="pre">en</span> <span class="pre">cada</span> <span class="pre">capa</span></code>. Por lo tanto, el número de pesos entrenables en los MLP profundos es muy grande.</p></li>
</ul>
</section>
<section id="entrenamiento-de-mlp">
<h2>Entrenamiento de MLP<a class="headerlink" href="#entrenamiento-de-mlp" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Los pesos <span class="math notranslate nohighlight">\(w\)</span> de una red neuronal se calculan mediante un algoritmo de optimización basado en el gradiente, como el <code class="docutils literal notranslate"><span class="pre">gradiente</span> <span class="pre">decendiente</span> <span class="pre">estocástico,</span> <span class="pre">que</span> <span class="pre">minimiza</span> <span class="pre">iterativamente</span> <span class="pre">la</span> <span class="pre">función</span> <span class="pre">de</span> <span class="pre">pérdida</span> <span class="pre">o</span> <span class="pre">el</span> <span class="pre">error</span></code> (<span class="math notranslate nohighlight">\(L\)</span>) en que incurre la red al hacer predicciones sobre los datos de entrenamiento. El <code class="docutils literal notranslate"><span class="pre">error</span> <span class="pre">cuadrático</span> <span class="pre">medio</span></code> (<span class="math notranslate nohighlight">\(MSE\)</span>) y el <code class="docutils literal notranslate"><span class="pre">error</span> <span class="pre">absoluto</span> <span class="pre">medio</span></code> (<span class="math notranslate nohighlight">\(MAE\)</span>) se utilizan para <code class="docutils literal notranslate"><span class="pre">tareas</span> <span class="pre">de</span> <span class="pre">regresión</span></code>, mientras que la pérdida binaria y categórica logarítmica son funciones de pérdida habituales en los problemas de clasificación.</p></li>
</ul>
<ul class="simple">
<li><p>Para la predicción de series temporales, <span class="math notranslate nohighlight">\(MSE\)</span> y <span class="math notranslate nohighlight">\(MAE\)</span> serían aptos para entrenar los modelos neuronales. <code class="docutils literal notranslate"><span class="pre">Los</span> <span class="pre">algoritmos</span> <span class="pre">gradiente</span> <span class="pre">descendiente</span> <span class="pre">funcionan</span> <span class="pre">moviendo</span> <span class="pre">los</span> <span class="pre">pesos,</span> <span class="pre">en</span> <span class="pre">iteraciones</span></code> <span class="math notranslate nohighlight">\(i\)</span><code class="docutils literal notranslate"><span class="pre">,</span> <span class="pre">a</span> <span class="pre">lo</span> <span class="pre">largo</span> <span class="pre">de</span> <span class="pre">su</span> <span class="pre">trayectoria</span> <span class="pre">de</span> <span class="pre">gradiente</span></code>. El gradiente es la derivada parcial de la función de pérdida <span class="math notranslate nohighlight">\(L\)</span> con respecto al peso. La regla de actualización más sencilla para cambiar un peso <span class="math notranslate nohighlight">\(w\)</span> requiere los valores de los pesos, la derivada parcial de <span class="math notranslate nohighlight">\(L\)</span> con respecto a los pesos, y una tasa de aprendizaje <span class="math notranslate nohighlight">\(\alpha\)</span> que <code class="docutils literal notranslate"><span class="pre">controla</span> <span class="pre">la</span> <span class="pre">rapidez</span> <span class="pre">con</span> <span class="pre">la</span> <span class="pre">que</span> <span class="pre">el</span> <span class="pre">punto</span> <span class="pre">desciende</span> <span class="pre">a</span> <span class="pre">lo</span> <span class="pre">largo</span> <span class="pre">del</span> <span class="pre">gradiente</span></code></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\omega_{i+1}=\omega_{i}-\alpha\left(\frac{\partial L}{\partial\omega}\right)_{\omega = \omega_{i}}
\]</div>
<ul class="simple">
<li><p>Esta regla básica de actualización tiene diversas variantes que influyen en la convergencia del algoritmo. Sin embargo, <code class="docutils literal notranslate"><span class="pre">una</span> <span class="pre">entrada</span> <span class="pre">crucial</span> <span class="pre">para</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">algoritmos</span> <span class="pre">basados</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">gradiente</span> <span class="pre">es</span> <span class="pre">la</span> <span class="pre">derivada</span> <span class="pre">parcial</span> <span class="pre">que</span> <span class="pre">debe</span> <span class="pre">calcularse</span> <span class="pre">para</span> <span class="pre">todos</span> <span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">red</span></code>. En redes neuronales profundas, algunas de las cuales tienen millones de pesos, el cálculo de la derivada puede ser una tarea computacional gigantesca. En esta dirección es exactamente donde entra en juego el famoso <code class="docutils literal notranslate"><span class="pre">algoritmo</span> <span class="pre">backpropagation</span></code> que resuelve este problema de forma eficiente.</p></li>
<li><p>Para entender el algoritmo backpropagation, primero hay que conocer los <code class="docutils literal notranslate"><span class="pre">grafos</span> <span class="pre">computacionales</span> <span class="pre">y</span> <span class="pre">cómo</span> <span class="pre">se</span> <span class="pre">utilizan</span> <span class="pre">para</span> <span class="pre">realizar</span> <span class="pre">cálculos</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">red</span> <span class="pre">neuronal</span></code>. Consideremos una <code class="docutils literal notranslate"><span class="pre">red</span> <span class="pre">neuronal</span> <span class="pre">simple</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">sola</span> <span class="pre">capa</span> <span class="pre">oculta,</span> <span class="pre">que</span> <span class="pre">tiene</span> <span class="pre">dos</span> <span class="pre">unidades</span> <span class="pre">ocultas,</span> <span class="pre">cada</span> <span class="pre">una</span> <span class="pre">con</span> <span class="pre">una</span> <span class="pre">activación</span> <span class="pre">sigmoidea</span></code>. La unidad de salida es una transformación lineal de sus entradas. La red se alimenta con dos variables de entrada, <span class="math notranslate nohighlight">\([y_{1},y_{2}]\)</span>. Los pesos se muestran a lo largo de los bordes de la red</p></li>
</ul>
<figure class="align-center" id="twoinput-single-hidden-layer">
<a class="reference internal image-reference" href="_images/twoinput_single_hidden_layer.png"><img alt="_images/twoinput_single_hidden_layer.png" src="_images/twoinput_single_hidden_layer.png" style="width: 300.0px; height: 207.60000000000002px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Red neuronal con una sola capa oculta y dos entradas.</span><a class="headerlink" href="#twoinput-single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>La red realiza una serie de <code class="docutils literal notranslate"><span class="pre">sumas,</span> <span class="pre">multiplicaciones</span> <span class="pre">y</span> <span class="pre">un</span> <span class="pre">par</span> <span class="pre">evaluaciones</span> <span class="pre">de</span> <span class="pre">funciones</span> <span class="pre">sigmoidales</span> <span class="pre">para</span> <span class="pre">transformar</span> <span class="pre">la</span> <span class="pre">entrada</span> <span class="pre">en</span> <span class="pre">una</span> <span class="pre">predicción</span></code>. La transformación de la entrada en una predicción se denomina paso hacia delante de la red neuronal. La <a class="reference internal" href="#forwardprop-single-hidden-layer"><span class="std std-numref">Fig. 20</span></a> muestra cómo se consigue un pase hacia adelante mediante un grafo computacional para un par de entrada <span class="math notranslate nohighlight">\([-1, 2]\)</span>. Cada cálculo da como resultado una salida intermedia <span class="math notranslate nohighlight">\(p_{i}\)</span>. <code class="docutils literal notranslate"><span class="pre">Los</span> <span class="pre">resultados</span> <span class="pre">intermedios</span></code> <span class="math notranslate nohighlight">\(p_{7}\)</span> <code class="docutils literal notranslate"><span class="pre">y</span></code> <span class="math notranslate nohighlight">\(p_{8}\)</span> <code class="docutils literal notranslate"><span class="pre">son</span> <span class="pre">la</span> <span class="pre">salida</span> <span class="pre">de</span> <span class="pre">las</span> <span class="pre">neuronas</span> <span class="pre">ocultas</span></code> <span class="math notranslate nohighlight">\(g_{1}\)</span> y <span class="math notranslate nohighlight">\(g_{2}\)</span>. Durante el entrenamiento, la función de pérdida <span class="math notranslate nohighlight">\(L\)</span> también se calcula en el paso hacia delante</p></li>
</ul>
<figure class="align-center" id="forwardprop-single-hidden-layer">
<img alt="_images/forwardprop_single_hidden_layer.png" src="_images/forwardprop_single_hidden_layer.png" />
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">Gráfico computacional de un perceptrón con una capa y dos neuronas ocultas.</span><a class="headerlink" href="#forwardprop-single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>En este punto, <code class="docutils literal notranslate"><span class="pre">se</span> <span class="pre">aplica</span> <span class="pre">el</span> <span class="pre">algoritmo</span> <span class="pre">de</span> <span class="pre">backpropagation</span> <span class="pre">para</span> <span class="pre">calcular</span> <span class="pre">las</span> <span class="pre">derivadas</span> <span class="pre">parciales</span> <span class="pre">entre</span> <span class="pre">dos</span> <span class="pre">nodos</span> <span class="pre">conectados</span> <span class="pre">por</span> <span class="pre">una</span> <span class="pre">arista</span></code>. El recorrido hacia atrás en el grafo para calcular la derivada parcial también se conoce como paso hacia atrás (<code class="docutils literal notranslate"><span class="pre">backward</span> <span class="pre">pass</span></code>). <code class="docutils literal notranslate"><span class="pre">El</span> <span class="pre">operador</span> <span class="pre">de</span> <span class="pre">diferenciación</span> <span class="pre">parcial</span> <span class="pre">se</span> <span class="pre">aplica</span> <span class="pre">en</span> <span class="pre">cada</span> <span class="pre">nodo</span> <span class="pre">y</span> <span class="pre">las</span> <span class="pre">derivadas</span> <span class="pre">parciales</span> <span class="pre">se</span> <span class="pre">asignan</span> <span class="pre">a</span> <span class="pre">las</span> <span class="pre">respectivas</span> <span class="pre">aristas</span> <span class="pre">que</span> <span class="pre">conectan</span> <span class="pre">el</span> <span class="pre">nodo</span> <span class="pre">descendente</span> <span class="pre">a</span> <span class="pre">lo</span> <span class="pre">largo</span> <span class="pre">del</span> <span class="pre">grafo</span> <span class="pre">computacional</span></code>.</p></li>
<li><p>Siguiendo la regla de la cadena la derivada parcial <span class="math notranslate nohighlight">\(\partial_{\omega} L\)</span> se calcula <code class="docutils literal notranslate"><span class="pre">multiplicando</span> <span class="pre">las</span> <span class="pre">derivadas</span> <span class="pre">parciales</span> <span class="pre">en</span> <span class="pre">todas</span> <span class="pre">las</span> <span class="pre">aristas</span> <span class="pre">que</span> <span class="pre">conectan</span> <span class="pre">el</span> <span class="pre">nodo</span> <span class="pre">de</span> <span class="pre">peso</span> <span class="pre">y</span> <span class="pre">el</span> <span class="pre">nodo</span> <span class="pre">de</span> <span class="pre">pérdida</span></code>. Si existen varios caminos entre un nodo de peso y el nodo de pérdida, las derivadas parciales a lo largo de cada camino se suman para obtener la <code class="docutils literal notranslate"><span class="pre">derivada</span> <span class="pre">parcial</span> <span class="pre">total</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">pérdida</span> <span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">al</span> <span class="pre">peso</span></code>. Esta técnica gráfica de implementar los pasos hacia delante y hacia atrás es la técnica computacional subyacente utilizada en potentes bibliotecas de aprendizaje profundo. El paso hacia atrás se ilustra en <a class="reference internal" href="#partialderivates-single-hidden-layer"><span class="std std-numref">Fig. 21</span></a></p></li>
</ul>
<figure class="align-center" id="partialderivates-single-hidden-layer">
<img alt="_images/partialderivates_single_hidden_layer.png" src="_images/partialderivates_single_hidden_layer.png" />
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">Cálculo de derivadas parciales en un grafo computacional.</span><a class="headerlink" href="#partialderivates-single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Las derivadas parciales de la función de pérdida con respecto a los pesos se obtienen aplicando la <code class="docutils literal notranslate"><span class="pre">regla</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">cadena</span></code>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\omega_{5}} &amp;= -2(y-\hat{y})\times 1\times p_{7}\\
\frac{\partial L}{\partial\omega_{6}} &amp;= -2(y-\hat{y})\times 1\times p_{8}\\
\frac{\partial L}{\partial\omega_{1}} = \frac{\partial L}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial p_{9}}\frac{\partial p_{9}}{\partial p_{7}}\frac{\partial p_{7}}{\partial p_{5}}\frac{\partial p_{5}}{\partial p_{1}}\frac{\partial p_{1}}{\partial\omega_{1}} &amp;= -2(y-\hat{y})\times 1\times\omega_{5}\times p_{7}^{2}e^{-p_{5}}\times 1\times -1\\
\frac{\partial L}{\partial\omega_{2}} = \frac{\partial L}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial p_{9}}\frac{\partial p_{9}}{\partial p_{7}}\frac{\partial p_{7}}{\partial p_{5}}\frac{\partial p_{5}}{\partial p_{2}}\frac{\partial p_{2}}{\partial\omega_{2}} &amp;= -2(y-\hat{y})\times 1\times\omega_{5}\times p_{7}^{2}e^{-p_{5}}\times 1\times 2\\
\frac{\partial L}{\partial\omega_{3}} = \frac{\partial L}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial p_{10}}\frac{\partial p_{10}}{\partial p_{8}}\frac{\partial p_{8}}{\partial p_{6}}\frac{\partial p_{6}}{\partial p_{3}}\frac{\partial p_{3}}{\partial\omega_{3}} &amp;= -2(y-\hat{y})\times 1\times\omega_{6}\times p_{8}^{2}e^{-p_{6}}\times 1\times -1\\
\frac{\partial L}{\partial\omega_{4}} = \frac{\partial L}{\partial\hat{y}}\frac{\partial\hat{y}}{\partial p_{10}}\frac{\partial p_{10}}{\partial p_{8}}\frac{\partial p_{8}}{\partial p_{6}}\frac{\partial p_{6}}{\partial p_{4}}\frac{\partial p_{4}}{\partial\omega_{4}} &amp;= -2(y-\hat{y})\times 1\times\omega_{6}\times p_{8}^{2}e^{-p_{6}}\times 1\times 2.
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>Durante el entrenamiento, <code class="docutils literal notranslate"><span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">se</span> <span class="pre">inicializan</span> <span class="pre">con</span> <span class="pre">números</span> <span class="pre">aleatorios</span> <span class="pre">comúnmente</span> <span class="pre">muestreados</span> <span class="pre">a</span> <span class="pre">partir</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">distribución</span> <span class="pre">uniforme</span> <span class="pre">con</span> <span class="pre">límites</span> <span class="pre">superior</span> <span class="pre">e</span> <span class="pre">inferior</span> <span class="pre">en</span></code> <span class="math notranslate nohighlight">\([-1, 1]\)</span> o una <code class="docutils literal notranslate"><span class="pre">distribución</span> <span class="pre">normal</span> <span class="pre">que</span> <span class="pre">tiene</span> <span class="pre">media</span> <span class="pre">cero</span> <span class="pre">y</span> <span class="pre">varianza</span> <span class="pre">unitaria</span></code>. Estos esquemas de inicialización aleatoria tienen algunas variantes que mejoran la convergencia de la optimización. En este caso, vamos a suponer que los pesos son inicializados a partir de una distribución aleatoria uniforme y, por tanto, <span class="math notranslate nohighlight">\(w_{1} = -0.33, w_{2} = -0.33, w_{3} = 0.57, w_{4} = -0.01, w_{5}=0.07\)</span>, y <span class="math notranslate nohighlight">\(w_{6} = 0.82\)</span>.</p></li>
<li><p>Con estos valores, vamos a realizar pasos hacia adelante y hacia atrás sobre el grafo computacional. Actualizamos la figura anterior con los valores calculados durante la <code class="docutils literal notranslate"><span class="pre">pasada</span> <span class="pre">hacia</span> <span class="pre">adelante</span> <span class="pre">en</span> <span class="pre">azul</span> <span class="pre">y</span> <span class="pre">los</span> <span class="pre">gradientes</span> <span class="pre">calculados</span> <span class="pre">durante</span> <span class="pre">la</span> <span class="pre">pasada</span> <span class="pre">hacia</span> <span class="pre">atrás</span> <span class="pre">en</span> <span class="pre">rojo</span></code>. Para este ejemplo, fijamos el <code class="docutils literal notranslate"><span class="pre">valor</span> <span class="pre">real</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">variable</span> <span class="pre">objetivo</span> <span class="pre">como</span></code> <span class="math notranslate nohighlight">\(y = 1\)</span></p></li>
</ul>
<figure class="align-center" id="forward-backward-single-hidden-layer">
<img alt="_images/forward_backward_single_hidden_layer.png" src="_images/forward_backward_single_hidden_layer.png" />
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">Pasos hacia delante (en azul) y hacia atrás (en rojo) sobre un grafo computacional.</span><a class="headerlink" href="#forward-backward-single-hidden-layer" title="Link to this image">#</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Una vez calculados los gradientes a lo largo de las aristas, <code class="docutils literal notranslate"><span class="pre">las</span> <span class="pre">derivadas</span> <span class="pre">parciales</span> <span class="pre">con</span> <span class="pre">respecto</span> <span class="pre">a</span> <span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">no</span> <span class="pre">son</span> <span class="pre">más</span> <span class="pre">que</span> <span class="pre">una</span> <span class="pre">aplicación</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">regla</span> <span class="pre">de</span> <span class="pre">la</span> <span class="pre">cadena</span></code>, de la que ya hemos hablado. Los valores de las derivadas parciales son los siguientes:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial L}{\partial\omega_{5}} &amp;= -0.919\times 1\times 0.418 = -0.384\\
\frac{\partial L}{\partial\omega_{6}} &amp;= -0.919\times 1\times 0.357 = -0.328\\
\frac{\partial L}{\partial\omega_{1}} &amp;= -0.919\times 1\times 0.07\times 0.243\times 1\times -1 = 0.016\\
\frac{\partial L}{\partial\omega_{2}} &amp;= -0.919\times 1\times 0.07\times 0.243\times 1\times 2 = -0.032\\
\frac{\partial L}{\partial\omega_{3}} &amp;= -0.919\times 1\times 0.82\times 0.229\times 1\times -1 = 0.173\\
\frac{\partial L}{\partial\omega_{4}} &amp;= -0.919\times 1\times 0.82\times 0.229\times 1\times 2 = -0.346\\
\end{align*}
\end{split}\]</div>
<ul class="simple">
<li><p>El siguiente paso consiste en <code class="docutils literal notranslate"><span class="pre">actualizar</span> <span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">mediante</span> <span class="pre">el</span> <span class="pre">algoritmo</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">descendiente</span></code>. Así, con una <code class="docutils literal notranslate"><span class="pre">tasa</span> <span class="pre">de</span> <span class="pre">aprendizaje</span> <span class="pre">de</span></code> <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>, el nuevo valor de</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
w_{5} = 0.07 - 0.01\times -0.384 = 0.0738.
\]</div>
<ul class="simple">
<li><p>El resto de pesos también pueden actualizarse utilizando una regla de actualización similar. El proceso de actualización iterativa de los pesos se repite varias veces. <code class="docutils literal notranslate"><span class="pre">El</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">veces</span> <span class="pre">que</span> <span class="pre">se</span> <span class="pre">actualizan</span> <span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">se</span> <span class="pre">conoce</span> <span class="pre">como</span> <span class="pre">número</span> <span class="pre">de</span> <span class="pre">épocas</span></code> o pasadas sobre los datos de entrenamiento. Normalmente, un criterio de tolerancia sobre el cambio de la función de pérdida en comparación con la época anterior controla el número de épocas.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Para</span> <span class="pre">determinar</span> <span class="pre">los</span> <span class="pre">pesos</span> <span class="pre">de</span> <span class="pre">una</span> <span class="pre">red</span> <span class="pre">neuronal</span> <span class="pre">se</span> <span class="pre">utiliza</span> <span class="pre">el</span> <span class="pre">algoritmo</span> <span class="pre">backpropagation</span> <span class="pre">junto</span> <span class="pre">con</span> <span class="pre">un</span> <span class="pre">optimizador</span> <span class="pre">basado</span> <span class="pre">en</span> <span class="pre">el</span> <span class="pre">gradiente</span></code>. Afortunadamente, existen potentes bibliotecas de aprendizaje profundo, como <code class="docutils literal notranslate"><span class="pre">Tensorflow,</span> <span class="pre">Theano</span></code> y <code class="docutils literal notranslate"><span class="pre">CNTK</span></code> que implementan gráficos computacionales para entrenar redes neuronales de cualquier arquitectura y complejidad. Estas bibliotecas vienen con soporte para ejecutar los cálculos como operaciones matemáticas en matrices multidimensionales y también pueden aprovechar las <code class="docutils literal notranslate"><span class="pre">GPU</span></code> para realizar cálculos más rápidos.</p></li>
</ul>
</section>
<section id="mlp-para-la-prediccion-de-series-temporales">
<h2>MLP para la predicción de series temporales<a class="headerlink" href="#mlp-para-la-prediccion-de-series-temporales" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>En esta sección, utilizaremos <code class="docutils literal notranslate"><span class="pre">MLP</span></code> para desarrollar <em><strong>modelos de predicción de series temporales</strong></em>. El conjunto de datos utilizado para estos ejemplos es sobre la <em><strong>contaminación atmosférica medida por la concentración de partículas (PM) de diámetro inferior o igual a 2.5 micrómetros</strong></em>.</p></li>
<li><p>Hay otras variables, como la <em><strong>presión atmosférica, temperatura del aire, punto de rocío, etc</strong></em>. Se han desarrollado un par de modelos de series temporales, uno sobre la <em><strong>presión atmosférica</strong></em> <code class="docutils literal notranslate"><span class="pre">PRES</span></code> y otro sobre <code class="docutils literal notranslate"><span class="pre">pm</span> <span class="pre">2.5</span></code>. El conjunto de datos se ha descargado del repositorio de aprendizaje automático de la <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data">UCI</a>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;text.usetex&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;darkgrid&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;datasets/PRSA_data_2010.1.1-2014.12.31.csv&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="k">lambda</span> <span class="n">column</span><span class="p">:</span> <span class="n">column</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;DEWP&#39;</span><span class="p">,</span> <span class="s1">&#39;TEMP&#39;</span><span class="p">,</span> <span class="s1">&#39;cbwd&#39;</span><span class="p">,</span> <span class="s1">&#39;Iws&#39;</span><span class="p">,</span>	<span class="s1">&#39;Is&#39;</span><span class="p">,</span> <span class="s1">&#39;Ir&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of the dataframe:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of the dataframe: (43824, 7)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>1021.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>1020.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>NaN</td>
      <td>1019.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>NaN</td>
      <td>1019.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>NaN</td>
      <td>1018.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Para asegurarse de que las filas están en el orden correcto de fecha y hora de las observaciones, <em><strong>se crea una nueva columna datetime a partir de las columnas relacionadas con la fecha y la hora del DataFrame</strong></em>. La nueva columna se compone de objetos <code class="docutils literal notranslate"><span class="pre">datetime.datetime</span> <span class="pre">de</span> <span class="pre">Python</span></code>. El DataFrame se ordena en orden ascendente sobre esta columna</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;month&#39;</span><span class="p">,</span> <span class="s1">&#39;day&#39;</span><span class="p">,</span> <span class="s1">&#39;hour&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">month</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;month&#39;</span><span class="p">],</span> <span class="n">day</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;day&#39;</span><span class="p">],</span>
                                                                                          <span class="n">hour</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;hour&#39;</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;datetime&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>1021.0</td>
      <td>2010-01-01 00:00:00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>1020.0</td>
      <td>2010-01-01 01:00:00</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>NaN</td>
      <td>1019.0</td>
      <td>2010-01-01 02:00:00</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>NaN</td>
      <td>1019.0</td>
      <td>2010-01-01 03:00:00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>NaN</td>
      <td>1018.0</td>
      <td>2010-01-01 04:00:00</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Dibujamos un diagrama de cajas para visualizar la <em><strong>tendencia central y la dispersión</strong></em> de por ejemplo la columna <code class="docutils literal notranslate"><span class="pre">PRES</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_fontsize</span> <span class="o">=</span> <span class="mi">18</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">],</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Set2&quot;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Box plot of Air Pressure&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Air Pressure readings in hPa&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> 
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">],</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Set2&quot;</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Box plot of PM2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;PM2.5 readings in µg/m³&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/9375994072b95a60d4a5fdfbc168c0daf18896601b7ae5404493caacd2a21b94.png" src="_images/9375994072b95a60d4a5fdfbc168c0daf18896601b7ae5404493caacd2a21b94.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">])</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of Air Pressure&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Air Pressure readings in hPa&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">])</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of PM2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;PM2.5 readings in µg/m³&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/109d4cabf2e4035792ecaea5dd3f3cc8223f8b2399b3c1b86838b716f154aef7.png" src="_images/109d4cabf2e4035792ecaea5dd3f3cc8223f8b2399b3c1b86838b716f154aef7.png" />
</div>
</div>
<ul class="simple">
<li><p><em><strong>Los algoritmos de gradiente descendiente funcionan mejor (por ejemplo, convergen más rápido) si las variables están dentro del intervalo</strong></em> <span class="math notranslate nohighlight">\([-1, 1]\)</span>. Muchas fuentes relajan el límite hasta <span class="math notranslate nohighlight">\([-3, 3]\)</span>. La variable <code class="docutils literal notranslate"><span class="pre">PRES</span></code> es escalada con <code class="docutils literal notranslate"><span class="pre">minmax</span></code> para limitar la variable transformada dentro de <span class="math notranslate nohighlight">\([0,1]\)</span>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Antes de entrenar el modelo, el conjunto de datos se divide en dos partes: el conjunto de <em><strong>entrenamiento</strong></em> y el conjunto de <em><strong>validación</strong></em>. La red neuronal se entrena en el conjunto de entrenamiento. Esto significa que <em><strong>el cálculo de la función de pérdida, la propagación hacia atrás y los pesos actualizados mediante un algoritmo de gradiente descendiente se realizan en el conjunto de entrenamiento</strong></em>.</p></li>
<li><p><em><strong>El conjunto de validación se utiliza para evaluar el modelo y determinar el número de épocas en su entrenamiento</strong></em>. Aumentar el número de épocas reducirá aún más la función de pérdida en el conjunto de entrenamiento, pero no necesariamente tendrá el mismo efecto en el conjunto de validación debido al sobreajuste en el conjunto de entrenamiento, por lo que <em><strong>el número de épocas se controla manteniendo una evaluación y verificación sobre la función de pérdida calculada para el conjunto de validación</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>Utilizamos <code class="docutils literal notranslate"><span class="pre">Keras</span></code> con el backend <code class="docutils literal notranslate"><span class="pre">Tensorflow</span></code> para definir y entrenar el modelo. Todos los pasos implicados en el entrenamiento y validación del modelo se realizan llamando a las funciones apropiadas de la <code class="docutils literal notranslate"><span class="pre">API</span></code> de <code class="docutils literal notranslate"><span class="pre">Keras</span></code>.</p></li>
</ul>
<ul class="simple">
<li><p>Los cuatro primeros años, <em><strong>de 2010 a 2013, se utilizan como entrenamiento y
2014 se utiliza para la validación</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="mi">2014</span><span class="p">,</span> <span class="n">month</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">day</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hour</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&lt;</span><span class="n">split_date</span><span class="p">]</span>
<span class="n">df_val</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">split_date</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of train:&#39;</span><span class="p">,</span> <span class="n">df_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of test:&#39;</span><span class="p">,</span> <span class="n">df_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of train: (35064, 9)
Shape of test: (8760, 9)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>1021.0</td>
      <td>2010-01-01 00:00:00</td>
      <td>0.545455</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>1020.0</td>
      <td>2010-01-01 01:00:00</td>
      <td>0.527273</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>NaN</td>
      <td>1019.0</td>
      <td>2010-01-01 02:00:00</td>
      <td>0.509091</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>NaN</td>
      <td>1019.0</td>
      <td>2010-01-01 03:00:00</td>
      <td>0.509091</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>NaN</td>
      <td>1018.0</td>
      <td>2010-01-01 04:00:00</td>
      <td>0.490909</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>35064</th>
      <td>35065</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>24.0</td>
      <td>1014.0</td>
      <td>2014-01-01 00:00:00</td>
      <td>0.418182</td>
    </tr>
    <tr>
      <th>35065</th>
      <td>35066</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>53.0</td>
      <td>1013.0</td>
      <td>2014-01-01 01:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>35066</th>
      <td>35067</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>65.0</td>
      <td>1013.0</td>
      <td>2014-01-01 02:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>35067</th>
      <td>35068</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>70.0</td>
      <td>1013.0</td>
      <td>2014-01-01 03:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>35068</th>
      <td>35069</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>79.0</td>
      <td>1012.0</td>
      <td>2014-01-01 04:00:00</td>
      <td>0.381818</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Restablecemos los <em><strong>índices del conjunto de validación</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>35065</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>24.0</td>
      <td>1014.0</td>
      <td>2014-01-01 00:00:00</td>
      <td>0.418182</td>
    </tr>
    <tr>
      <th>1</th>
      <td>35066</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>53.0</td>
      <td>1013.0</td>
      <td>2014-01-01 01:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>35067</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>65.0</td>
      <td>1013.0</td>
      <td>2014-01-01 02:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35068</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>70.0</td>
      <td>1013.0</td>
      <td>2014-01-01 03:00:00</td>
      <td>0.400000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35069</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>79.0</td>
      <td>1012.0</td>
      <td>2014-01-01 04:00:00</td>
      <td>0.381818</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>También <em><strong>se grafican las series temporales de entrenamiento y validación normalizadas para PRES</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of scaled Air Pressure in train set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Scaled Air Pressure readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of scaled Air Pressure in validation set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Scaled Air Pressure readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/52fed10808f1e3a9ad6e32c0bf50d7240b62329d897f926d2335931e57331e38.png" src="_images/52fed10808f1e3a9ad6e32c0bf50d7240b62329d897f926d2335931e57331e38.png" />
</div>
</div>
<div class="tip admonition">
<p class="admonition-title">Regresores y Variable Objetivo</p>
<ul class="simple">
<li><p>Ahora necesitamos generar los <em><strong>regresores</strong></em> (<span class="math notranslate nohighlight">\(X\)</span>) y la <em><strong>variable objetivo</strong></em> (<span class="math notranslate nohighlight">\(y\)</span>) para el entrenamiento y la validación. <em><strong>La matriz bidimensional de regresores y la matriz unidimensional objetivo se crean a partir de la matriz unidimensional original de la columna standardized_PRES</strong></em> en el <em>DataFrame</em>.</p></li>
<li><p>Para el modelo de predicción de series temporales, de este ejemplo, <em><strong>se utilizan las observaciones de los últimos siete días para predecir el día siguiente</strong></em>. Esto equivale a un modelo <span class="math notranslate nohighlight">\(AR(7)\)</span>. Definimos una función que toma la serie temporal original y el número de pasos temporales en regresores como entrada para generar las matrices <span class="math notranslate nohighlight">\(X\)</span> e <span class="math notranslate nohighlight">\(y\)</span></p></li>
</ul>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">makeXy</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">nb_timesteps</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Input: </span>
<span class="sd">           ts: original time series</span>
<span class="sd">           nb_timesteps: number of time steps in the regressors</span>
<span class="sd">    Output: </span>
<span class="sd">           X: 2-D array of regressors</span>
<span class="sd">           y: 1-D array of target </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nb_timesteps</span><span class="p">,</span> <span class="n">ts</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">i</span><span class="o">-</span><span class="n">nb_timesteps</span> <span class="o">&lt;=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">nb_timesteps</span><span class="p">,</span> <span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="n">nb_timesteps</span><span class="p">:</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="c1">#Regressors</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ts</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="c1">#Target</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">makeXy</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">],</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 6 7
1 7 8
2 8 9
3 9 10
4 10 11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of train arrays:&#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of train arrays: (35057, 7) (35057,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">makeXy</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;scaled_PRES&#39;</span><span class="p">],</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 6 7
1 7 8
2 8 9
3 9 10
4 10 11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of validation arrays:&#39;</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of validation arrays: (8753, 7) (8753,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Ahora definimos la red <code class="docutils literal notranslate"><span class="pre">MLP</span></code> utilizando la <code class="docutils literal notranslate"><span class="pre">API</span></code> funcional de <code class="docutils literal notranslate"><span class="pre">Keras</span></code>. En este enfoque <em><strong>una capa puede ser declarada como la entrada de la siguiente capa en el momento de definir la siguiente</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>En este caso, <code class="docutils literal notranslate"><span class="pre">Input</span></code> es una función que se utiliza para <em><strong>crear una capa de entrada en un modelo de red neuronal</strong></em>. <code class="docutils literal notranslate"><span class="pre">shape=(7,)</span></code> específica la <em><strong>forma de los datos de entrada</strong></em>. En este caso, <em>significa que los datos de entrada tendrán 7 dimensiones</em>. <code class="docutils literal notranslate"><span class="pre">dtype='float32'</span></code> especifica el *<strong>tipo de datos de los elementos de la capa de entrada</strong>. En este caso, son números de punto <em>flotante de 32 bits</em>.</p></li>
</ul>
<ul class="simple">
<li><p>Las capas densas las definimos en esta caso con <em><strong>activación lineal</strong></em>. Puede utilizar un <code class="docutils literal notranslate"><span class="pre">GridSearch</span></code> tal como se hizo en el curso de <em><strong>Machine Learning</strong></em> para encontrar <em><strong>hiperparámetros adecuados minimizando las métricas de regresión</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)(</span><span class="n">dense1</span><span class="p">)</span>
<span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)(</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="tip admonition">
<p class="admonition-title"><strong>Dense</strong> e <strong>input_layer</strong></p>
<ul class="simple">
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">Dense</span></code></strong></em>: Correspone a una <em><strong>capa totalmente conectada</strong></em> (fully connected).</p></li>
<li><p><em><strong>Unidades (Neurons)</strong></em>: 32 neuronas.</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">dense1</span></code></strong></em>: Esta capa <em><strong>toma como entrada</strong></em> <code class="docutils literal notranslate"><span class="pre">input_layer</span></code>, que puede ser la <em><strong>capa de entrada del modelo u otra capa anterior</strong></em>.</p></li>
<li><p><em><strong><code class="docutils literal notranslate"><span class="pre">dense2</span></code></strong></em>: Esta capa <em><strong>toma como entrada la salida de</strong></em> <code class="docutils literal notranslate"><span class="pre">dense1</span></code>. Esto significa que <em><strong>los 32 valores de salida de</strong></em> <code class="docutils literal notranslate"><span class="pre">dense1</span></code> <em><strong>se usan como entrada para</strong></em> <code class="docutils literal notranslate"><span class="pre">dense2</span></code>. Similarmente, ocurre con <code class="docutils literal notranslate"><span class="pre">dense3</span></code></p></li>
</ul>
</div>
<div class="admonition-observacion admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>Las <em><strong>múltiples capas ocultas y el gran número de neuronas en cada capa oculta</strong></em> le dan a las redes neuronales la <em><strong>capacidad de modelar la compleja no linealidad de las relaciones subyacentes entre los regresores y el objetivo</strong></em>. Sin embargo, las redes neuronales profundas también <em><strong>pueden sobreajustar los datos de entrenamiento</strong></em> y dar malos resultados en el conjunto de validación o prueba. La función <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> se ha utilizado eficazmente para <em><strong>regularizar las redes neuronales profundas</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>En este ejemplo, se añade una capa <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> antes de la capa de salida. <em><strong>Dropout aleatoriamente establece</strong></em> <span class="math notranslate nohighlight">\(p\)</span> <em><strong>fracción de neuronas de entrada a cero antes de pasar a la siguiente capa</strong></em>. La eliminación aleatoria de entradas actúa esencialmente como un tipo de ensamblaje de modelos de agregación <code class="docutils literal notranslate"><span class="pre">bootstrap</span></code>.</p></li>
<li><p>Por ejemplo, <em><strong>el bosque aleatorio utiliza el ensamblaje mediante la construcción de árboles en subconjuntos aleatorios de características de entrada</strong></em>. Utilizamos <span class="math notranslate nohighlight">\(p=0.2\)</span> para <em>descartar el 20% de las características de entrada seleccionadas aleatoriamente</em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">dense3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Por último, <em><strong>la capa de salida predice la presión atmosférica del día siguiente</strong></em></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)(</span><span class="n">dropout_layer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Las capas de entrada, densa y de salida se empaquetarán ahora dentro de un modelo, que es una clase envolvente para entrenar y hacer predicciones. <em><strong>Como función de pérdida se utiliza el error cuadrático medio</strong></em> (MSE). Los pesos de la red se optimizan mediante el algoritmo <code class="docutils literal notranslate"><span class="pre">Adam</span></code>. <code class="docutils literal notranslate"><span class="pre">Adam</span></code> significa <em><strong>Estimación Adaptativa de Momentos</strong></em> y ha sido una opción popular para el entrenamiento de redes neuronales profundas.</p></li>
<li><p>A diferencia del <em><strong>Gradiente Descendiente Estocástico, Adam utiliza diferentes tasas de aprendizaje para cada peso y las actualiza por separado a medida que avanza el entrenamiento</strong></em>. La tasa de aprendizaje de un peso se actualiza basándose en medias móviles ponderadas exponencialmente de los gradientes del peso y los gradientes al cuadrado.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ts_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_1"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)             │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>)                   │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)                  │             <span style="color: #00af00; text-decoration-color: #00af00">256</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)                  │             <span style="color: #00af00; text-decoration-color: #00af00">528</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)                  │             <span style="color: #00af00; text-decoration-color: #00af00">272</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)                    │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)                  │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_3 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">17</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,073</span> (4.19 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,073</span> (4.19 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div></div>
</div>
<div class="tip admonition">
<p class="admonition-title">Observación</p>
<ul class="simple">
<li><p>En este caso, <code class="docutils literal notranslate"><span class="pre">Params</span> <span class="pre">#</span></code> es calculado mediante la fórmula: <em><strong><code class="docutils literal notranslate"><span class="pre">Param</span> <span class="pre">#</span> <span class="pre">=</span> <span class="pre">#Entradas</span> <span class="pre">x</span> <span class="pre">#Neuronas</span> <span class="pre">+</span> <span class="pre">#Neuronas</span></code></strong></em>. Esto incluye los <em><strong>pesos de cada conexión entre las entradas y las neuronas</strong></em> y un <em><strong>bias para cada neurona</strong></em>. En este caso <code class="docutils literal notranslate"><span class="pre">Params</span> <span class="pre">#</span> <span class="pre">=</span> <span class="pre">7x32</span> <span class="pre">+</span> <span class="pre">32</span> <span class="pre">=</span> <span class="pre">256</span></code>.</p></li>
<li><p>El modelo se entrena llamando a la función <code class="docutils literal notranslate"><span class="pre">fit()</span></code> en el objeto modelo y pasándole <code class="docutils literal notranslate"><span class="pre">X_train</span></code> y <code class="docutils literal notranslate"><span class="pre">y_train</span></code>. El entrenamiento se realiza para un <em><strong>número predefinido de épocas</strong></em>. Además, <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> define el <em><strong>número de muestras del conjunto de entrenamiento que se utilizarán para una instancia de backpropagation</strong></em>.</p></li>
</ul>
</div>
<ul class="simple">
<li><p>El conjunto de datos de validación también se pasa para evaluar el modelo después de cada <code class="docutils literal notranslate"><span class="pre">epoch</span></code> completa. Un objeto <em><strong><code class="docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> rastrea la función de pérdida en el conjunto de validación y guarda el modelo para la época en la que la función de pérdida ha sido mínima</strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">save_weights_at</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;keras_models&#39;</span><span class="p">,</span> <span class="s1">&#39;PRSA_data_Air_Pressure_MLP_weights.</span><span class="si">{epoch:02d}</span><span class="s1">-</span><span class="si">{val_loss:.4f}</span><span class="s1">.keras&#39;</span><span class="p">)</span>
<span class="n">save_best</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">save_weights_at</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">save_freq</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Aquí <code class="docutils literal notranslate"><span class="pre">val_loss</span></code> es el <em><strong>valor de la función de coste para los datos de validación cruzada</strong></em> y <code class="docutils literal notranslate"><span class="pre">loss</span></code> es el <em><strong>valor de la función de coste para los datos de entrenamiento</strong></em>. Con <code class="docutils literal notranslate"><span class="pre">verbose=0</span></code>, no se imprime ningún mensaje en la consola durante el proceso de guardado del modelo. <code class="docutils literal notranslate"><span class="pre">period=1</span></code> indica que <em><strong>el modelo se evaluará y potencialmente se guardará después de cada época</strong></em>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ts_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
             <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">save_best</span><span class="p">],</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
             <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/airpresure_mlp_epochs.png"><img alt="_images/airpresure_mlp_epochs.png" src="_images/airpresure_mlp_epochs.png" style="width: 484.0px; height: 753.0px;" /></a>
</figure>
<ul class="simple">
<li><p>En este caso, el modo <code class="docutils literal notranslate"><span class="pre">verbose=2</span></code> muestra una <em><strong>barra de progreso por cada época</strong></em>. Los modos posibles son: <code class="docutils literal notranslate"><span class="pre">0</span></code> para no mostrar nada, <code class="docutils literal notranslate"><span class="pre">1</span></code> para mostrar la barra de progreso, <code class="docutils literal notranslate"><span class="pre">2</span></code> para mostrar una línea por época. Las <em><strong>muestras se mezclan aleatoriamente antes de cada época</strong></em> (<code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>).</p></li>
</ul>
<ul class="simple">
<li><p>Se hacen predicciones para la presión atmosférica a partir del mejor modelo guardado. <em><strong>Las predicciones del modelo sobre la presión atmosférica escalada, se transforman inversamente para obtener predicciones sobre la presión atmosférica original</strong></em>. También se calcula la bondad de ajuste o <em><strong><code class="docutils literal notranslate"><span class="pre">R</span> <span class="pre">cuadrado</span></code></strong></em>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;keras_models&#39;</span><span class="p">,</span> <span class="s1">&#39;PRSA_data_Air_Pressure_MLP_weights.05-0.0001.keras&#39;</span><span class="p">),</span> <span class="nb">compile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">pred_PRES</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
<span class="n">pred_PRES</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">pred_PRES</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">  1/274</span> <span class=" -Color -Color-White">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">12s</span> 47ms/step
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">131/274</span> <span class=" -Color -Color-Green">━━━━━━━━━</span><span class=" -Color -Color-White">━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 389us/step
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">274/274</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 415us/step
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">274/274</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 418us/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">r2</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:],</span> <span class="n">pred_PRES</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R-squared for the validation set:&#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">r2</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>R-squared for the validation set: 0.9955
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;PRES&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:</span><span class="mi">56</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">pred_PRES</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Actual&#39;</span><span class="p">,</span><span class="s1">&#39;Predicted&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual vs Predicted Air Pressure&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Air Pressure&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f69141173a42e6618059571280d3d4214f47f24b8f9bf476e013385c7c128520.png" src="_images/f69141173a42e6618059571280d3d4214f47f24b8f9bf476e013385c7c128520.png" />
</div>
</div>
<ul class="simple">
<li><p>Para <em><strong>predecir la variable</strong></em> <code class="docutils literal notranslate"><span class="pre">pm2.5</span></code> usando <code class="docutils literal notranslate"><span class="pre">MLP</span></code> usamos la implementación presentada a continuación</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;datasets/PRSA_data_2010.1.1-2014.12.31.csv&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="k">lambda</span> <span class="n">column</span><span class="p">:</span> <span class="n">column</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;DEWP&#39;</span><span class="p">,</span> <span class="s1">&#39;TEMP&#39;</span><span class="p">,</span> <span class="s1">&#39;cbwd&#39;</span><span class="p">,</span> <span class="s1">&#39;Iws&#39;</span><span class="p">,</span>	<span class="s1">&#39;Is&#39;</span><span class="p">,</span> <span class="s1">&#39;Ir&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of the dataframe:&#39;</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of the dataframe: (43824, 7)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>NaN</td>
      <td>1021.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>NaN</td>
      <td>1020.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>NaN</td>
      <td>1019.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>NaN</td>
      <td>1019.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>2010</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>NaN</td>
      <td>1018.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Rows having NaN values in column pm2.5 are dropped.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;month&#39;</span><span class="p">,</span> <span class="s1">&#39;day&#39;</span><span class="p">,</span> <span class="s1">&#39;hour&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">month</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;month&#39;</span><span class="p">],</span> <span class="n">day</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;day&#39;</span><span class="p">],</span>
                                                                                          <span class="n">hour</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;hour&#39;</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;datetime&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> 
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">],</span> <span class="n">orient</span><span class="o">=</span><span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Set2&quot;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Box plot of pm2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">yticklabels</span><span class="o">=</span><span class="p">[])</span>
<span class="n">g1</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">])</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of pm2.5&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5 readings&#39;</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7b455a4d3c546c2789d26c120b7f28170858583ffa7e24742a67918e0997f5de.png" src="_images/7b455a4d3c546c2789d26c120b7f28170858583ffa7e24742a67918e0997f5de.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&lt;=</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span><span class="n">month</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span><span class="n">day</span><span class="o">=</span><span class="mi">30</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;pm2.5 during 2010&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5 readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&lt;=</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="mi">2010</span><span class="p">,</span><span class="n">month</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">day</span><span class="o">=</span><span class="mi">31</span><span class="p">)],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Zoom in on one month: pm2.5 during Jan 2010&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5 readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/421eba76e525cfc286c11027fe72b291445d468f252b44ce80853961d36baaab.png" src="_images/421eba76e525cfc286c11027fe72b291445d468f252b44ce80853961d36baaab.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">split_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="mi">2014</span><span class="p">,</span> <span class="n">month</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">day</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hour</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df_train</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&lt;</span><span class="n">split_date</span><span class="p">]</span>
<span class="n">df_val</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;datetime&#39;</span><span class="p">]</span><span class="o">&gt;=</span><span class="n">split_date</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of train:&#39;</span><span class="p">,</span> <span class="n">df_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of test:&#39;</span><span class="p">,</span> <span class="n">df_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of train: (33096, 9)
Shape of test: (8661, 9)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_pm2.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>129.0</td>
      <td>1020.0</td>
      <td>2010-01-02 00:00:00</td>
      <td>0.129779</td>
    </tr>
    <tr>
      <th>1</th>
      <td>26</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>148.0</td>
      <td>1020.0</td>
      <td>2010-01-02 01:00:00</td>
      <td>0.148893</td>
    </tr>
    <tr>
      <th>2</th>
      <td>27</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>159.0</td>
      <td>1021.0</td>
      <td>2010-01-02 02:00:00</td>
      <td>0.159960</td>
    </tr>
    <tr>
      <th>3</th>
      <td>28</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>181.0</td>
      <td>1022.0</td>
      <td>2010-01-02 03:00:00</td>
      <td>0.182093</td>
    </tr>
    <tr>
      <th>4</th>
      <td>29</td>
      <td>2010</td>
      <td>1</td>
      <td>2</td>
      <td>4</td>
      <td>138.0</td>
      <td>1022.0</td>
      <td>2010-01-02 04:00:00</td>
      <td>0.138833</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>No</th>
      <th>year</th>
      <th>month</th>
      <th>day</th>
      <th>hour</th>
      <th>pm2.5</th>
      <th>PRES</th>
      <th>datetime</th>
      <th>scaled_pm2.5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>33096</th>
      <td>35065</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>24.0</td>
      <td>1014.0</td>
      <td>2014-01-01 00:00:00</td>
      <td>0.024145</td>
    </tr>
    <tr>
      <th>33097</th>
      <td>35066</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>53.0</td>
      <td>1013.0</td>
      <td>2014-01-01 01:00:00</td>
      <td>0.053320</td>
    </tr>
    <tr>
      <th>33098</th>
      <td>35067</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>65.0</td>
      <td>1013.0</td>
      <td>2014-01-01 02:00:00</td>
      <td>0.065392</td>
    </tr>
    <tr>
      <th>33099</th>
      <td>35068</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>70.0</td>
      <td>1013.0</td>
      <td>2014-01-01 03:00:00</td>
      <td>0.070423</td>
    </tr>
    <tr>
      <th>33100</th>
      <td>35069</td>
      <td>2014</td>
      <td>1</td>
      <td>1</td>
      <td>4</td>
      <td>79.0</td>
      <td>1012.0</td>
      <td>2014-01-01 04:00:00</td>
      <td>0.079477</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_val</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">g1</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of scaled pm2.5 in train set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Scaled pm2.5 readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g2</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Time series of scaled pm2.5 in validation set&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">)</span>
<span class="n">g2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Scaled pm2.5 readings&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">plot_fontsize</span><span class="p">);</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/925b5c6eda295e2ea40312c770bcc89c44ba2dcada7ec6e9b7c64d1510736b72.png" src="_images/925b5c6eda295e2ea40312c770bcc89c44ba2dcada7ec6e9b7c64d1510736b72.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">makeXy</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">],</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 6 7
1 7 8
2 8 9
3 9 10
4 10 11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of train arrays:&#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of train arrays: (33089, 7) (33089,)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">makeXy</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;scaled_pm2.5&#39;</span><span class="p">],</span> <span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0 6 7
1 7 8
2 8 9
3 9 10
4 10 11
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of validation arrays:&#39;</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of validation arrays: (8654, 7) (8654,)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Definimos la <em><strong>capa de entrada</strong></em> que tiene forma <code class="docutils literal notranslate"><span class="pre">(None,</span> <span class="pre">7)</span></code> y de tipo <code class="docutils literal notranslate"><span class="pre">float32</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Dense layers are defined with linear activation</span>
<span class="n">dense1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">dense2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">)(</span><span class="n">dense1</span><span class="p">)</span>
<span class="n">dense3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">)(</span><span class="n">dense2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">dense3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Finally the output layer gives prediction for the next day&#39;s air pressure.</span>
<span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)(</span><span class="n">dropout_layer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ts_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_absolute_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_3"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>)                   │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_4 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)                  │             <span style="color: #00af00; text-decoration-color: #00af00">256</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_5 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)                  │             <span style="color: #00af00; text-decoration-color: #00af00">528</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_6 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)                  │             <span style="color: #00af00; text-decoration-color: #00af00">272</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)                  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">16</span>)                  │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_7 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">17</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,073</span> (4.19 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">1,073</span> (4.19 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">save_weights_at</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;keras_models&#39;</span><span class="p">,</span> <span class="s1">&#39;PRSA_data_PM2.5_MLP_weights.</span><span class="si">{epoch:02d}</span><span class="s1">-</span><span class="si">{val_loss:.4f}</span><span class="s1">.keras&#39;</span><span class="p">)</span>
<span class="n">save_best</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">save_weights_at</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">save_freq</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ts_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
             <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">save_best</span><span class="p">],</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
             <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/mlp_pm25_epochs.png"><img alt="_images/mlp_pm25_epochs.png" src="_images/mlp_pm25_epochs.png" style="width: 561.0px; height: 688.0px;" /></a>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;keras_models&#39;</span><span class="p">,</span> <span class="s1">&#39;PRSA_data_PM2.5_MLP_weights.19-0.0116.keras&#39;</span><span class="p">),</span> <span class="nb">compile</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">pred_pm25</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
<span class="n">pred_pm25</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">pred_pm25</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">  1/271</span> <span class=" -Color -Color-White">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">33s</span> 123ms/step
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold"> 66/271</span> <span class=" -Color -Color-Green">━━━━</span><span class=" -Color -Color-White">━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 770us/step 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">138/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━</span><span class=" -Color -Color-White">━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 733us/step
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">213/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━</span><span class=" -Color -Color-White">━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 712us/step
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">271/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 1ms/step  
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">271/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 1ms/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:],</span> <span class="n">pred_pm25</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE for the validation set:&#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">mae</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MAE for the validation set: 11.5359
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Trazamos los 50 primeros valores reales y predichos de <code class="docutils literal notranslate"><span class="pre">pm2.5</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:</span><span class="mi">56</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">pred_pm25</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Actual&#39;</span><span class="p">,</span><span class="s1">&#39;Predicted&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual vs Predicted pm2.5&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6b68983f1bce3ac670d40f9152290bbf3b95c901eebf2b3baa8143c6f9052271.png" src="_images/6b68983f1bce3ac670d40f9152290bbf3b95c901eebf2b3baa8143c6f9052271.png" />
</div>
</div>
</section>
<section id="lstm-para-la-prediccion-de-series-de-tiempo">
<h2>LSTM para la predicción de series de tiempo<a class="headerlink" href="#lstm-para-la-prediccion-de-series-de-tiempo" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X_val</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Shape of 3D arrays:&#39;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Shape of 3D arrays: (33089, 7, 1) (8654, 7, 1)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lstm_layer1</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">lstm_layer2</span> <span class="o">=</span> <span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">64</span><span class="p">),</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">False</span><span class="p">)(</span><span class="n">lstm_layer1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">lstm_layer2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)(</span><span class="n">dropout_layer</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ts_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_absolute_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span><span class="c1">#SGD(lr=0.001, decay=1e-5))</span>
<span class="n">ts_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold">Model: "functional_5"</span>
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃<span style="font-weight: bold"> Layer (type)                         </span>┃<span style="font-weight: bold"> Output Shape                </span>┃<span style="font-weight: bold">         Param # </span>┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">InputLayer</span>)           │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lstm (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                          │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">7</span>, <span style="color: #00af00; text-decoration-color: #00af00">64</span>)               │          <span style="color: #00af00; text-decoration-color: #00af00">16,896</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lstm_1 (<span style="color: #0087ff; text-decoration-color: #0087ff">LSTM</span>)                        │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)                  │          <span style="color: #00af00; text-decoration-color: #00af00">12,416</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_2 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dropout</span>)                  │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">32</span>)                  │               <span style="color: #00af00; text-decoration-color: #00af00">0</span> │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_8 (<span style="color: #0087ff; text-decoration-color: #0087ff">Dense</span>)                      │ (<span style="color: #00d7ff; text-decoration-color: #00d7ff">None</span>, <span style="color: #00af00; text-decoration-color: #00af00">1</span>)                   │              <span style="color: #00af00; text-decoration-color: #00af00">33</span> │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Total params: </span><span style="color: #00af00; text-decoration-color: #00af00">29,345</span> (114.63 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">29,345</span> (114.63 KB)
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="font-weight: bold"> Non-trainable params: </span><span style="color: #00af00; text-decoration-color: #00af00">0</span> (0.00 B)
</pre>
</div></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">save_weights_at</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;keras_models&#39;</span><span class="p">,</span> <span class="s1">&#39;PRSA_data_PM2.5_LSTM_weights.</span><span class="si">{epoch:02d}</span><span class="s1">-</span><span class="si">{val_loss:.4f}</span><span class="s1">.keras&#39;</span><span class="p">)</span>
<span class="n">save_best</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">save_weights_at</span><span class="p">,</span> <span class="n">monitor</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">save_freq</span><span class="o">=</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ts_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
             <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">save_best</span><span class="p">],</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">),</span>
             <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/lstm_epochs.png"><img alt="_images/lstm_epochs.png" src="_images/lstm_epochs.png" style="width: 563.0px; height: 713.0px;" /></a>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">best_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;keras_models&#39;</span><span class="p">,</span> <span class="s1">&#39;PRSA_data_PM2.5_LSTM_weights.20-0.0116.keras&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">preds</span> <span class="o">=</span> <span class="n">best_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">pred_pm25</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">preds</span><span class="p">)</span>
<span class="n">pred_pm25</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">pred_pm25</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold">  1/271</span> <span class=" -Color -Color-White">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">31s</span> 116ms/step
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold"> 66/271</span> <span class=" -Color -Color-Green">━━━━</span><span class=" -Color -Color-White">━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 771us/step 
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">140/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━</span><span class=" -Color -Color-White">━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 728us/step
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">214/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━</span><span class=" -Color -Color-White">━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 711us/step
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">271/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 1ms/step  
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
<span class=" -Color -Color-Bold">271/271</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">0s</span> 1ms/step
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mae</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:],</span> <span class="n">pred_pm25</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MAE for the validation set:&#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">mae</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MAE for the validation set: 11.5265
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">df_val</span><span class="p">[</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">7</span><span class="p">:</span><span class="mi">56</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="n">pred_pm25</span><span class="p">[:</span><span class="mi">50</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Actual&#39;</span><span class="p">,</span><span class="s1">&#39;Predicted&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Actual vs Predicted pm2.5&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;pm2.5&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Index&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f087f0b47f75f138a3eb45b24387df0f8379810aebcc1b95792886a02b8d119d.png" src="_images/f087f0b47f75f138a3eb45b24387df0f8379810aebcc1b95792886a02b8d119d.png" />
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "tensorflow"
        },
        kernelOptions: {
            name: "tensorflow",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'tensorflow'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="arima_model.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Modelos autorregresivos integrados de media móvil</p>
      </div>
    </a>
    <a class="right-next"
       href="biblio.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bibliografía</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales">Redes neuronales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradiente-descendiente">Gradiente descendiente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-perceptron">El perceptrón</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-totalmente-conectadas">Redes Totalmente Conectadas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-algoritmo-de-backpropagation">El Algoritmo De Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-esquema-de-backpropagation-para-gradiente-descendiente">El Esquema De Backpropagation Para Gradiente Descendiente</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-gradientes">Cálculo de gradientes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-de-delta-nj-r">Cálculo de <span class="math notranslate nohighlight">\(\delta_{nj}^{r}\)</span></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#las-capas-ocultas">Las capas ocultas</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-neuronales-recurrentes">Redes Neuronales Recurrentes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-en-tiempo">Backpropagation en tiempo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#desvanecimiento-y-explosion-de-gradientes">Desvanecimiento y explosión de gradientes</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#red-de-memoria-a-largo-plazo-lstm">Red de memoria a largo plazo (LSTM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#series-de-tiempo">Series de Tiempo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptrones-multicapa">Perceptrones multicapa</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entrenamiento-de-mlp">Entrenamiento de MLP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mlp-para-la-prediccion-de-series-temporales">MLP para la predicción de series temporales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm-para-la-prediccion-de-series-de-tiempo">LSTM para la predicción de series de tiempo</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Lihki Rubio PhD in Mathematical Engineering
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright Lihki Rubio PhD.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>